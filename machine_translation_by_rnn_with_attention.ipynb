{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_rnn_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAzMVzsKlJlV",
        "outputId": "4d0dbda0-d462-4aa8-aa7a-e86dc4046ecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a7df43643d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dKvwvE3qUB3q"
      },
      "outputs": [],
      "source": [
        "hidden_size = 100\n",
        "embedding_dim = 30\n",
        "learning_rate = 0.001\n",
        "batch_size = 50\n",
        "beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pgaJy39hUt2C"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(2000,3000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append(item[0] + '>')\n",
        "  fr_lines.append(item[1] + '>')\n",
        "\n",
        "max_len_line_en = max([len(l) for l in en_lines])\n",
        "max_len_line_fr = max([len(l) for l in fr_lines])\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) < max_len_line_en):\n",
        "    en_lines[i] = en_lines[i].ljust(max_len_line_en, padding_character)\n",
        "  if (len(fr_lines[i]) < max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i].ljust(max_len_line_fr, padding_character)\n",
        "\n",
        "\n",
        "source_vocab = sorted(set(''.join(en_lines)))\n",
        "target_vocab = sorted(set(''.join(fr_lines)))\n",
        "\n",
        "target_vocab.append('<')\n",
        "\n",
        "source_vocab_size = len(set(''.join(source_vocab)))\n",
        "target_vocab_size = len(set(''.join(target_vocab)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}\n",
        "\n",
        "padding_token_index = target_char_to_ix[padding_character]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GUfLKgxoU7j3"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([source_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "def target_line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([target_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "en_data = []\n",
        "fr_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_aiwNJdMpOvb"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    seq_len = encoder_outputs.size(1)\n",
        "    # Repeat the hidden state for all timesteps\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "    # Concatenate the hidden state and encoder outputs\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=-1)))\n",
        "    # Calculate attention scores\n",
        "    attention_scores = torch.matmul(energy, self.v)\n",
        "    # Convert attention scores to probabilities\n",
        "    attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "    # Calculate the attention-weighted sum of encoder outputs\n",
        "    context_vector = torch.sum(attention_weights.unsqueeze(2) * encoder_outputs, dim=1)\n",
        "    return context_vector\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    h_list = []\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      h_list.append(h)\n",
        "    return torch.stack(h_list, dim=0)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "    self.i2h = nn.Linear(self.embedding_dim, self.hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "    self.h2o = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.e2d = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, self.hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "    self.att = Attention(hidden_size)\n",
        "\n",
        "  def init_state(self, encode_state):\n",
        "    T,B,C = encode_state.shape\n",
        "    self.encode_state = encode_state.reshape(B,T,C)\n",
        "\n",
        "\n",
        "  def forward(self, batch_size):\n",
        "\n",
        "    # if x is None:\n",
        "    h = torch.zeros(batch_size, self.hidden_size)\n",
        "    x = torch.tensor([target_char_to_ix[start_character] for _ in range(batch_size)],dtype=torch.long)\n",
        "    output = []\n",
        "    for i in range(max_len_line_fr):\n",
        "      t = self.embedding(x)\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      context_state = self.att(h, self.encode_state)\n",
        "      y = self.e2d(context_state) + self.h2o(h) + self.ob\n",
        "      p = nn.functional.softmax(y, dim=1)\n",
        "      ix = torch.argmax(p, dim=-1)\n",
        "      x = ix\n",
        "      output.append(y)\n",
        "    return torch.stack(output, dim=0).permute(1, 0, 2)\n",
        "\n",
        "  def beam_search(self):\n",
        "    \"\"\"\n",
        "    Perform beam search to generate sequences.\n",
        "    \"\"\"\n",
        "    beams = [(torch.tensor([target_char_to_ix[start_character]], dtype=torch.long), 1.0)]\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "\n",
        "    for i in range(max_len_line_fr):\n",
        "      new_beams = []\n",
        "\n",
        "      for seq, score in beams:\n",
        "        x = seq[-1].view(1, -1)  # Take the last predicted token\n",
        "\n",
        "        t = self.embedding(x)\n",
        "        h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "        context_state = self.att(h.reshape(-1, self.hidden_size), self.encode_state)\n",
        "        y = self.e2d(context_state) + self.h2o(h) + self.ob\n",
        "        p = F.softmax(y, dim=-1)\n",
        "        top_probs, top_ix = torch.topk(p, beam_width, dim=-1)\n",
        "\n",
        "        for prob, token_ix in zip(top_probs[0][0], top_ix[0][0]):\n",
        "          new_seq = torch.cat((seq, torch.tensor([token_ix], dtype=torch.long)), dim=0)\n",
        "          new_beams.append((new_seq, score * prob.item()))\n",
        "\n",
        "      beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return beams\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = Encoder(source_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "    self.decoder = Decoder(target_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "  def forward(self, source, batch_size):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    output = self.decoder(batch_size)\n",
        "    return output\n",
        "\n",
        "  def translate(self, source):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    beams = self.decoder.beam_search()\n",
        "    return beams\n",
        "\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "model = Seq2Seq(source_vocab_size, target_vocab_size, embedding_dim, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HwqI27qRs9_"
      },
      "outputs": [],
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(0,len(en_data) - batch_size - 1,batch_size):\n",
        "\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # encoder = Encoder(source_vocab_size, embedding_dim, hidden_size)\n",
        "    # output = encoder(source_batch)\n",
        "    output = model(source_batch, batch_size)\n",
        "\n",
        "    output = output.reshape(-1, target_vocab_size)\n",
        "    # remove the padding tokens when calculate the loss\n",
        "    # Create a mask to ignore padding tokens\n",
        "    padding_mask = (target_batch != padding_token_index).float()\n",
        "\n",
        "    # Compute the loss with the padding mask\n",
        "    loss = criterion(output, target_batch.view(-1))\n",
        "    loss = (loss * padding_mask.view(-1)).sum() / padding_mask.sum()\n",
        "\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    if p%500 == 0:\n",
        "      # Print or log the training loss for each epoch\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVVtitSdL6t",
        "outputId": "fabfc627-2c32-419c-c26c-5f58b505519b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Noe  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "<Noes&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "<Noe s&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n"
          ]
        }
      ],
      "source": [
        "test_line = \"We're armed>&\"\n",
        "\n",
        "input = line_to_tensor(test_line)\n",
        "\n",
        "outputs = model.translate(input)\n",
        "for tensor,p in outputs:\n",
        "  result = [target_ix_to_char[j.item()] for j in tensor]\n",
        "  print(''.join(result))\n",
        "\n",
        "# outputs = model(input,1)\n",
        "# result = []\n",
        "# for i in range(outputs.shape[0]):\n",
        "\n",
        "#   p = nn.functional.softmax(outputs[i], dim=-1).detach().numpy().ravel()\n",
        "#   ix = np.random.choice(range(target_vocab_size), p=p)\n",
        "\n",
        "#   result.append(target_ix_to_char[ix])\n",
        "\n",
        "# print(''.join(result))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_lines"
      ],
      "metadata": {
        "id": "6hhu-SBOUE-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Acq_F0bUlua"
      },
      "source": [
        "I trained all day, but the performance is still poor. I'll figure out why later. Maybe the RNN is too simple? Let's see.\n",
        "\n",
        "The problem is that in order to train the model by minibatch, I have to add a lot of padding tokens into the training set. This hurts the performance of the model. As seen in the above example, when I tried to translate the English sentence 'closer look' into French, there are a lot of padding tokens (&&&), which is annoying.\n",
        "\n",
        "I think if I train the model with individual examples, then the problem could be relieved. However, the downside is that it could be time-consuming.\n",
        "\n",
        "Anyway, I will try to update another file that will use attention mechanism.\n",
        "\n",
        "See you in another Colab file\n",
        "\n",
        "see you in the next week?\n",
        "\n",
        "I have to take care the new baby in my family. so in the weekend, I didnot have the time to adjust my code, and train the model. I will try to do that in the next week. see you then.\n",
        "\n",
        "I know, this is a small project, I just want to write those projects to be more familiar with the NLP, which is helpful for me in my future phd study.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeg_uyoLvunJ"
      },
      "source": [
        "I finally found the where the problem.\n",
        "\n",
        "first I should not only use the final hidden state of the encoder. I should concate all the hidden state, and convert them into decoder.\n",
        "\n",
        "second for the decoder, for the prediction, the predicted character should be conditioned by the previous characters, the current state, and the state of the encoder.\n",
        "\n",
        "third, it is for the loss function. when I calcualte the loss, if encounter the padding, I should stop, because the error in the loss could cause the model to go at the wrong direction.\n",
        "\n",
        "okay, I will ask for more advices from chatGPT, continue this project.\n",
        "\n",
        "one more thing to mention is that, by doing the second step, I could easily add the attention machaism between the encoder and the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, after some bug fixes, I finnaly found the model wes able to translate the first few characters of English into France, but the overall performance is still not good.\n",
        "but that is okay, Because I only use the vanilla RNN.\n",
        "there are two things, I want to mension\n",
        "1: if the I use the word level token, the the seq of the source and target will become much smaller\n",
        "2: if I use more complicated structure of the rnn like GRU or LSTM, or deep rnn, maybe the model could capture more infos in the language.\n",
        "\n",
        "Will do both in the future"
      ],
      "metadata": {
        "id": "UXGJIZXlAaMM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJGIYbZj2oJoUe5r9m98YM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}