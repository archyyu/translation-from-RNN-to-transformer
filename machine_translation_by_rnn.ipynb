{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAzMVzsKlJlV",
        "outputId": "540a753e-4b25-419d-b144-54a4c1292969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d3989a8c3b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "dKvwvE3qUB3q"
      },
      "outputs": [],
      "source": [
        "hidden_size = 100\n",
        "embedding_dim = 30\n",
        "learning_rate = 1e-1 * 0.1\n",
        "batch_size = 50\n",
        "beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "pgaJy39hUt2C"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(1000,2000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append(item[0] + '>')\n",
        "  fr_lines.append('<' + item[1] + '>')\n",
        "\n",
        "max_len_line_en = min([len(l) for l in en_lines])\n",
        "max_len_line_fr = min([len(l) for l in fr_lines])\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) > max_len_line_en):\n",
        "    en_lines[i] = en_lines[i][0:max_len_line_en]\n",
        "  if (len(fr_lines[i]) > max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i][0:max_len_line_fr]\n",
        "\n",
        "\n",
        "source_vocab = sorted(set(''.join(en_lines)))\n",
        "target_vocab = sorted(set(''.join(fr_lines)))\n",
        "\n",
        "source_vocab_size = len(set(''.join(source_vocab)))\n",
        "target_vocab_size = len(set(''.join(target_vocab)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}\n",
        "\n",
        "# padding_token_index = target_char_to_ix[padding_character]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "GUfLKgxoU7j3"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([source_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "def target_line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([target_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "en_data = []\n",
        "fr_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([target_char_to_ix[end_character] for _ in range(batch_size)],dtype=torch.long)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv--WQUPOkmX",
        "outputId": "af19d707-533f-4215-addc-c07714192060"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
              "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
              "        6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "_aiwNJdMpOvb"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "    return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "    self.i2h = nn.Linear(self.embedding_dim, self.hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "    self.h2o = nn.Linear(self.hidden_size * 2, vocab_size, bias=False)\n",
        "\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, self.hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "  def init_state(self, encode_state):\n",
        "    self.encode_state = encode_state\n",
        "\n",
        "\n",
        "  # def forward(self, target):\n",
        "\n",
        "  #   # if x is None:\n",
        "  #   h = torch.zeros(1, self.hidden_size)\n",
        "  #   output = []\n",
        "  #   for i in range(max_len_line_fr):\n",
        "  #     x = target[:,i]\n",
        "  #     t = self.embedding(x)\n",
        "  #     h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "  #     y = self.h2o(torch.cat((self.encode_state, h), dim=-1)) + self.ob\n",
        "  #     output.append(y)\n",
        "  #   return torch.stack(output, dim=0)\n",
        "\n",
        "  def forward(self, batch_size):\n",
        "\n",
        "    # if x is None:\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    x = torch.tensor([target_char_to_ix[start_character] for _ in range(batch_size)],dtype=torch.long)\n",
        "    output = []\n",
        "    for i in range(max_len_line_fr):\n",
        "      t = self.embedding(x)\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      y = self.h2o(torch.cat((self.encode_state, h), dim=-1)) + self.ob\n",
        "      p = nn.functional.softmax(y, dim=1)\n",
        "      ix = torch.argmax(p, dim=-1)\n",
        "      x = ix\n",
        "      output.append(y)\n",
        "    return torch.stack(output, dim=0).permute(1, 0, 2)\n",
        "\n",
        "  def beam_search(self):\n",
        "    \"\"\"\n",
        "    Perform beam search to generate sequences.\n",
        "    \"\"\"\n",
        "    beams = [(torch.tensor([target_char_to_ix[start_character]], dtype=torch.long), 1.0)]\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "\n",
        "    for i in range(max_len_line_fr):\n",
        "      new_beams = []\n",
        "\n",
        "      for seq, score in beams:\n",
        "        x = seq[-1].view(1, -1)  # Take the last predicted token\n",
        "\n",
        "        t = self.embedding(x)\n",
        "        h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "        y = self.h2o(torch.cat((self.encode_state.unsqueeze(1), h), dim=-1)) + self.ob\n",
        "        p = F.softmax(y, dim=-1)\n",
        "        top_probs, top_ix = torch.topk(p, beam_width, dim=-1)\n",
        "\n",
        "        for prob, token_ix in zip(top_probs[0][0], top_ix[0][0]):\n",
        "          new_seq = torch.cat((seq, torch.tensor([token_ix], dtype=torch.long)), dim=0)\n",
        "          new_beams.append((new_seq, score * prob.item()))\n",
        "\n",
        "      beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return beams\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = Encoder(source_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "    self.decoder = Decoder(target_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "  def forward(self, source, batch_size):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    output = self.decoder(batch_size)\n",
        "    return output\n",
        "\n",
        "  def translate(self, source):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    beams = self.decoder.beam_search()\n",
        "    return beams\n",
        "\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "model = Seq2Seq(source_vocab_size, target_vocab_size, embedding_dim, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "1HwqI27qRs9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abf6405-7593-47df-df13-6bf35d27def3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p 0, Loss: 3.3112354278564453\n",
            "p 100, Loss: 0.5506493449211121\n",
            "p 200, Loss: 1.775083303451538\n",
            "p 300, Loss: 1.8876794576644897\n",
            "p 400, Loss: 1.5526385307312012\n",
            "p 500, Loss: 0.8385313153266907\n",
            "p 600, Loss: 1.703081488609314\n",
            "p 700, Loss: 1.0264885425567627\n",
            "p 800, Loss: 1.3980627059936523\n",
            "p 900, Loss: 1.3405945301055908\n",
            "p 0, Loss: 3.2073705196380615\n",
            "p 100, Loss: 0.4596770107746124\n",
            "p 200, Loss: 1.5942093133926392\n",
            "p 300, Loss: 1.8573014736175537\n",
            "p 400, Loss: 1.5101113319396973\n",
            "p 500, Loss: 0.8334330916404724\n",
            "p 600, Loss: 1.624329686164856\n",
            "p 700, Loss: 1.097843050956726\n",
            "p 800, Loss: 1.3430861234664917\n",
            "p 900, Loss: 1.212517499923706\n",
            "p 0, Loss: 4.454158782958984\n",
            "p 100, Loss: 0.453940749168396\n",
            "p 200, Loss: 1.5648692846298218\n",
            "p 300, Loss: 1.8089659214019775\n",
            "p 400, Loss: 1.424806833267212\n",
            "p 500, Loss: 0.8540245294570923\n",
            "p 600, Loss: 1.7454708814620972\n",
            "p 700, Loss: 1.0906648635864258\n",
            "p 800, Loss: 1.282371997833252\n",
            "p 900, Loss: 1.1412523984909058\n",
            "p 0, Loss: 4.678920269012451\n",
            "p 100, Loss: 0.5008078217506409\n",
            "p 200, Loss: 1.61273193359375\n",
            "p 300, Loss: 1.7331944704055786\n",
            "p 400, Loss: 1.431137204170227\n",
            "p 500, Loss: 0.8458341956138611\n",
            "p 600, Loss: 1.593345284461975\n",
            "p 700, Loss: 1.0162651538848877\n",
            "p 800, Loss: 1.2941384315490723\n",
            "p 900, Loss: 1.1840001344680786\n",
            "p 0, Loss: 5.01571798324585\n",
            "p 100, Loss: 0.41735175251960754\n",
            "p 200, Loss: 1.4428482055664062\n",
            "p 300, Loss: 1.7942872047424316\n",
            "p 400, Loss: 1.6551839113235474\n",
            "p 500, Loss: 0.8096128702163696\n",
            "p 600, Loss: 1.6138898134231567\n",
            "p 700, Loss: 0.950813889503479\n",
            "p 800, Loss: 1.3068537712097168\n",
            "p 900, Loss: 1.168959140777588\n",
            "p 0, Loss: 3.3899929523468018\n",
            "p 100, Loss: 0.5147445201873779\n",
            "p 200, Loss: 1.5123062133789062\n",
            "p 300, Loss: 1.8067251443862915\n",
            "p 400, Loss: 1.3343863487243652\n",
            "p 500, Loss: 0.7923672199249268\n",
            "p 600, Loss: 1.5534100532531738\n",
            "p 700, Loss: 1.041265606880188\n",
            "p 800, Loss: 1.2978920936584473\n",
            "p 900, Loss: 1.1336839199066162\n",
            "p 0, Loss: 4.455434322357178\n",
            "p 100, Loss: 0.44192275404930115\n",
            "p 200, Loss: 1.5548678636550903\n",
            "p 300, Loss: 1.7648687362670898\n",
            "p 400, Loss: 1.3568365573883057\n",
            "p 500, Loss: 0.7868609428405762\n",
            "p 600, Loss: 1.5874172449111938\n",
            "p 700, Loss: 0.9852882623672485\n",
            "p 800, Loss: 1.3396366834640503\n",
            "p 900, Loss: 1.1875141859054565\n",
            "p 0, Loss: 4.662542819976807\n",
            "p 100, Loss: 0.44564536213874817\n",
            "p 200, Loss: 1.6189972162246704\n",
            "p 300, Loss: 1.7996472120285034\n",
            "p 400, Loss: 1.4118319749832153\n",
            "p 500, Loss: 0.7564208507537842\n",
            "p 600, Loss: 1.7029460668563843\n",
            "p 700, Loss: 1.0361430644989014\n",
            "p 800, Loss: 1.3029152154922485\n",
            "p 900, Loss: 1.2156106233596802\n",
            "p 0, Loss: 4.671741485595703\n",
            "p 100, Loss: 0.391150563955307\n",
            "p 200, Loss: 1.679032325744629\n",
            "p 300, Loss: 1.787163496017456\n",
            "p 400, Loss: 1.3849127292633057\n",
            "p 500, Loss: 0.7560065388679504\n",
            "p 600, Loss: 1.6433899402618408\n",
            "p 700, Loss: 1.0069687366485596\n",
            "p 800, Loss: 1.2913429737091064\n",
            "p 900, Loss: 1.1539537906646729\n",
            "p 0, Loss: 5.312049388885498\n",
            "p 100, Loss: 0.4839705526828766\n",
            "p 200, Loss: 1.4351146221160889\n",
            "p 300, Loss: 1.7884069681167603\n",
            "p 400, Loss: 1.3572328090667725\n",
            "p 500, Loss: 0.8463205695152283\n",
            "p 600, Loss: 1.6599637269973755\n",
            "p 700, Loss: 1.017904281616211\n",
            "p 800, Loss: 1.3169087171554565\n",
            "p 900, Loss: 1.2506908178329468\n",
            "p 0, Loss: 3.854127883911133\n",
            "p 100, Loss: 0.44964301586151123\n",
            "p 200, Loss: 1.7104120254516602\n",
            "p 300, Loss: 1.8130511045455933\n",
            "p 400, Loss: 1.479660153388977\n",
            "p 500, Loss: 0.7962395548820496\n",
            "p 600, Loss: 1.777514934539795\n",
            "p 700, Loss: 1.083484172821045\n",
            "p 800, Loss: 1.3294824361801147\n",
            "p 900, Loss: 1.2527296543121338\n",
            "p 0, Loss: 4.376798152923584\n",
            "p 100, Loss: 0.4977777898311615\n",
            "p 200, Loss: 1.816227674484253\n",
            "p 300, Loss: 1.909281611442566\n",
            "p 400, Loss: 1.5271114110946655\n",
            "p 500, Loss: 0.8602493405342102\n",
            "p 600, Loss: 1.6090551614761353\n",
            "p 700, Loss: 1.0561819076538086\n",
            "p 800, Loss: 1.3969073295593262\n",
            "p 900, Loss: 1.2063307762145996\n",
            "p 0, Loss: 5.753072738647461\n",
            "p 100, Loss: 0.4258820414543152\n",
            "p 200, Loss: 1.7935786247253418\n",
            "p 300, Loss: 1.8355010747909546\n",
            "p 400, Loss: 1.4331332445144653\n",
            "p 500, Loss: 0.8303422331809998\n",
            "p 600, Loss: 1.5785949230194092\n",
            "p 700, Loss: 1.0686306953430176\n",
            "p 800, Loss: 1.3646883964538574\n",
            "p 900, Loss: 1.2576584815979004\n",
            "p 0, Loss: 5.583622932434082\n",
            "p 100, Loss: 0.5086967945098877\n",
            "p 200, Loss: 1.534662127494812\n",
            "p 300, Loss: 1.9115326404571533\n",
            "p 400, Loss: 1.570928931236267\n",
            "p 500, Loss: 0.829623281955719\n",
            "p 600, Loss: 1.7392712831497192\n",
            "p 700, Loss: 1.160246729850769\n",
            "p 800, Loss: 1.3871783018112183\n",
            "p 900, Loss: 1.3191981315612793\n",
            "p 0, Loss: 4.325753688812256\n",
            "p 100, Loss: 0.4696838855743408\n",
            "p 200, Loss: 1.736303687095642\n",
            "p 300, Loss: 1.8755230903625488\n",
            "p 400, Loss: 1.4715576171875\n",
            "p 500, Loss: 0.9005885124206543\n",
            "p 600, Loss: 1.7752307653427124\n",
            "p 700, Loss: 1.0671769380569458\n",
            "p 800, Loss: 1.3891265392303467\n",
            "p 900, Loss: 1.1864774227142334\n",
            "p 0, Loss: 4.74992561340332\n",
            "p 100, Loss: 0.5092765092849731\n",
            "p 200, Loss: 1.721319556236267\n",
            "p 300, Loss: 2.0500612258911133\n",
            "p 400, Loss: 1.667934536933899\n",
            "p 500, Loss: 0.8698756694793701\n",
            "p 600, Loss: 1.822455883026123\n",
            "p 700, Loss: 1.0255697965621948\n",
            "p 800, Loss: 1.4205231666564941\n",
            "p 900, Loss: 1.2481271028518677\n",
            "p 0, Loss: 3.884392023086548\n",
            "p 100, Loss: 0.5418261885643005\n",
            "p 200, Loss: 1.8289531469345093\n",
            "p 300, Loss: 1.778609037399292\n",
            "p 400, Loss: 1.3700445890426636\n",
            "p 500, Loss: 0.7865269184112549\n",
            "p 600, Loss: 1.6917535066604614\n",
            "p 700, Loss: 1.0455747842788696\n",
            "p 800, Loss: 1.407751202583313\n",
            "p 900, Loss: 1.2958892583847046\n",
            "p 0, Loss: 4.5160932540893555\n",
            "p 100, Loss: 0.4951340854167938\n",
            "p 200, Loss: 1.5860997438430786\n",
            "p 300, Loss: 1.6557984352111816\n",
            "p 400, Loss: 1.5722442865371704\n",
            "p 500, Loss: 0.8645542860031128\n",
            "p 600, Loss: 1.6683058738708496\n",
            "p 700, Loss: 0.9876693487167358\n",
            "p 800, Loss: 1.3139055967330933\n",
            "p 900, Loss: 1.1002334356307983\n",
            "p 0, Loss: 3.4058837890625\n",
            "p 100, Loss: 0.39525672793388367\n",
            "p 200, Loss: 1.4887524843215942\n",
            "p 300, Loss: 1.816643238067627\n",
            "p 400, Loss: 1.5432543754577637\n",
            "p 500, Loss: 0.7956333160400391\n",
            "p 600, Loss: 1.6315172910690308\n",
            "p 700, Loss: 1.0300233364105225\n",
            "p 800, Loss: 1.3742282390594482\n",
            "p 900, Loss: 1.167904019355774\n",
            "p 0, Loss: 4.5718674659729\n",
            "p 100, Loss: 0.48673874139785767\n",
            "p 200, Loss: 1.6577202081680298\n",
            "p 300, Loss: 1.8953323364257812\n",
            "p 400, Loss: 1.5142791271209717\n",
            "p 500, Loss: 0.8661444187164307\n",
            "p 600, Loss: 1.7009377479553223\n",
            "p 700, Loss: 0.9925485253334045\n",
            "p 800, Loss: 1.3796077966690063\n",
            "p 900, Loss: 1.3179692029953003\n",
            "p 0, Loss: 4.553781509399414\n",
            "p 100, Loss: 0.473862886428833\n",
            "p 200, Loss: 1.704178810119629\n",
            "p 300, Loss: 1.859910249710083\n",
            "p 400, Loss: 1.445372462272644\n",
            "p 500, Loss: 0.812180757522583\n",
            "p 600, Loss: 1.6881392002105713\n",
            "p 700, Loss: 1.0062620639801025\n",
            "p 800, Loss: 1.4704514741897583\n",
            "p 900, Loss: 1.1974595785140991\n",
            "p 0, Loss: 3.897049903869629\n",
            "p 100, Loss: 0.5045672059059143\n",
            "p 200, Loss: 1.5424163341522217\n",
            "p 300, Loss: 1.8641228675842285\n",
            "p 400, Loss: 1.5050288438796997\n",
            "p 500, Loss: 0.8114495873451233\n",
            "p 600, Loss: 1.598575234413147\n",
            "p 700, Loss: 1.082115650177002\n",
            "p 800, Loss: 1.4368726015090942\n",
            "p 900, Loss: 1.2351973056793213\n",
            "p 0, Loss: 4.279582500457764\n",
            "p 100, Loss: 0.5139490365982056\n",
            "p 200, Loss: 1.6022825241088867\n",
            "p 300, Loss: 1.9104223251342773\n",
            "p 400, Loss: 1.5458662509918213\n",
            "p 500, Loss: 0.9092625975608826\n",
            "p 600, Loss: 1.8376410007476807\n",
            "p 700, Loss: 1.103858232498169\n",
            "p 800, Loss: 1.377349853515625\n",
            "p 900, Loss: 1.300830364227295\n",
            "p 0, Loss: 4.694893836975098\n",
            "p 100, Loss: 0.4697313606739044\n",
            "p 200, Loss: 1.7842406034469604\n",
            "p 300, Loss: 1.932871699333191\n",
            "p 400, Loss: 1.411598563194275\n",
            "p 500, Loss: 0.7670408487319946\n",
            "p 600, Loss: 1.4753060340881348\n",
            "p 700, Loss: 0.9660300612449646\n",
            "p 800, Loss: 1.2504310607910156\n",
            "p 900, Loss: 1.1513028144836426\n",
            "p 0, Loss: 4.061707019805908\n",
            "p 100, Loss: 0.35849055647850037\n",
            "p 200, Loss: 1.5861406326293945\n",
            "p 300, Loss: 1.6851767301559448\n",
            "p 400, Loss: 1.4311423301696777\n",
            "p 500, Loss: 0.7168889045715332\n",
            "p 600, Loss: 1.452306866645813\n",
            "p 700, Loss: 0.8982916474342346\n",
            "p 800, Loss: 1.2601186037063599\n",
            "p 900, Loss: 1.1327579021453857\n",
            "p 0, Loss: 4.002687454223633\n",
            "p 100, Loss: 0.4351027309894562\n",
            "p 200, Loss: 1.5009510517120361\n",
            "p 300, Loss: 1.8717966079711914\n",
            "p 400, Loss: 1.4680047035217285\n",
            "p 500, Loss: 0.7543651461601257\n",
            "p 600, Loss: 1.5404181480407715\n",
            "p 700, Loss: 1.0134148597717285\n",
            "p 800, Loss: 1.34710693359375\n",
            "p 900, Loss: 1.3197219371795654\n",
            "p 0, Loss: 4.244594573974609\n",
            "p 100, Loss: 0.5558497309684753\n",
            "p 200, Loss: 1.7039920091629028\n",
            "p 300, Loss: 1.9094479084014893\n",
            "p 400, Loss: 1.512959599494934\n",
            "p 500, Loss: 0.8919709920883179\n",
            "p 600, Loss: 1.6359120607376099\n",
            "p 700, Loss: 1.0939586162567139\n",
            "p 800, Loss: 1.3881638050079346\n",
            "p 900, Loss: 1.2553654909133911\n",
            "p 0, Loss: 4.458247661590576\n",
            "p 100, Loss: 0.6134697198867798\n",
            "p 200, Loss: 1.7130886316299438\n",
            "p 300, Loss: 1.9049956798553467\n",
            "p 400, Loss: 1.4257514476776123\n",
            "p 500, Loss: 0.8180500864982605\n",
            "p 600, Loss: 1.5956496000289917\n",
            "p 700, Loss: 1.048352837562561\n",
            "p 800, Loss: 1.4978610277175903\n",
            "p 900, Loss: 1.3677362203598022\n",
            "p 0, Loss: 3.883136510848999\n",
            "p 100, Loss: 0.5818470120429993\n",
            "p 200, Loss: 1.4696277379989624\n",
            "p 300, Loss: 1.8645893335342407\n",
            "p 400, Loss: 1.5245270729064941\n",
            "p 500, Loss: 0.983113706111908\n",
            "p 600, Loss: 1.7840244770050049\n",
            "p 700, Loss: 1.0541791915893555\n",
            "p 800, Loss: 1.5047274827957153\n",
            "p 900, Loss: 1.3346338272094727\n",
            "p 0, Loss: 4.140500545501709\n",
            "p 100, Loss: 0.4736747443675995\n",
            "p 200, Loss: 1.4268039464950562\n",
            "p 300, Loss: 1.7627750635147095\n",
            "p 400, Loss: 1.6841692924499512\n",
            "p 500, Loss: 0.8969013690948486\n",
            "p 600, Loss: 1.8623443841934204\n",
            "p 700, Loss: 1.063880443572998\n",
            "p 800, Loss: 1.4545998573303223\n",
            "p 900, Loss: 1.282090663909912\n"
          ]
        }
      ],
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(len(en_data) - batch_size - 1):\n",
        "\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(source_batch, batch_size)\n",
        "\n",
        "    # remove the padding tokens when calculate the loss\n",
        "    # Create a mask to ignore padding tokens\n",
        "    # padding_mask = (target_batch != padding_token_index).float()\n",
        "    output = output.reshape(-1, target_vocab_size)\n",
        "\n",
        "    # Compute the loss with the padding mask\n",
        "    loss = criterion(output, target_batch.view(-1))\n",
        "    # loss = (loss * padding_mask.view(-1)).sum() / padding_mask.sum()\n",
        "\n",
        "    # for i in range(target_batch.shape[0]):\n",
        "    #   line = []\n",
        "    #   for j in range(len(target_batch[i])):\n",
        "    #     line.append(target_ix_to_char[target_batch[i][j].item()])\n",
        "    #   print(''.join(line))\n",
        "\n",
        "    # print(\"---\")\n",
        "    # probability_np = output.detach().numpy()\n",
        "    # Find the index of the maximum probability along the last dimension\n",
        "    # max_index_np = np.argmax(probability_np, axis=-1)\n",
        "    # Convert the resulting NumPy array back to a PyTorch tensor\n",
        "    # output = torch.tensor(max_index_np).reshape(target_batch.shape)\n",
        "    # for i in range(output.shape[0]):\n",
        "    #   line = []\n",
        "    #   for j in range(len(output[i])):\n",
        "    #     line.append(target_ix_to_char[output[i][j].item()])\n",
        "    #   print(''.join(line))\n",
        "    # print(loss.item())\n",
        "    # print(target_batch.shape)\n",
        "    # print(output.shape)\n",
        "    # break\n",
        "\n",
        "    # loss = criterion(output.view(-1, target_vocab_size), target_batch.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    if p%100 == 0:\n",
        "      # Print or log the training loss for each epoch\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n",
        "    p += batch_size\n",
        "    # break\n",
        "  # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVVtitSdL6t",
        "outputId": "32a0d901-dabe-4c47-dfad-b61a0e647b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<J'rrrrr\n",
            "<<J'rrrrt\n",
            "<<J'rrrtt\n"
          ]
        }
      ],
      "source": [
        "test_line = \"will \"\n",
        "\n",
        "input = line_to_tensor(test_line)\n",
        "\n",
        "\n",
        "outputs = model.translate(input)\n",
        "for tensor,p in outputs:\n",
        "  result = [target_ix_to_char[j.item()] for j in tensor]\n",
        "  print(''.join(result))\n",
        "\n",
        "# outputs = model(input,1)\n",
        "# result = []\n",
        "# for i in range(outputs.shape[0]):\n",
        "\n",
        "#   p = nn.functional.softmax(outputs[i], dim=-1).detach().numpy().ravel()\n",
        "#   ix = np.random.choice(range(target_vocab_size), p=p)\n",
        "\n",
        "#   result.append(target_ix_to_char[ix])\n",
        "\n",
        "# print(''.join(result))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJVXgOsNVeWF",
        "outputId": "11231eab-354f-4196-8563-31bec8cb79db"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I guess so.>',\n",
              " 'I guess so.>',\n",
              " 'I had help.>',\n",
              " 'I hate you.>',\n",
              " 'I hate you.>',\n",
              " 'I have one.>',\n",
              " 'I have one.>',\n",
              " 'I have won.>',\n",
              " 'I have won.>',\n",
              " 'I help him.>',\n",
              " 'I hope not.>',\n",
              " 'I hope not.>',\n",
              " 'I know CPR.>',\n",
              " 'I know her.>',\n",
              " 'I know him.>',\n",
              " 'I like art.>',\n",
              " 'I like him.>',\n",
              " 'I like him.>',\n",
              " 'I like tea.>',\n",
              " 'I like you.>',\n",
              " 'I like you.>',\n",
              " 'I like you.>',\n",
              " 'I liked it.>',\n",
              " 'I liked it.>',\n",
              " 'I love Tom.>',\n",
              " 'I love tea.>',\n",
              " 'I love you.>',\n",
              " 'I love you.>',\n",
              " 'I loved it.>',\n",
              " 'I made tea.>',\n",
              " 'I made two.>',\n",
              " 'I made two.>',\n",
              " 'I met them.>',\n",
              " 'I met them.>',\n",
              " 'I met them.>',\n",
              " 'I must run.>',\n",
              " 'I must run.>',\n",
              " 'I need air.>',\n",
              " 'I need air.>',\n",
              " 'I need ice.>',\n",
              " 'I need you.>',\n",
              " 'I need you.>',\n",
              " 'I panicked.>',\n",
              " 'I promised.>',\n",
              " 'I ran away.>',\n",
              " 'I ran home.>',\n",
              " 'I remember.>',\n",
              " 'I remember.>',\n",
              " 'I remember.>',\n",
              " 'I said yes.>',\n",
              " 'I sat down.>',\n",
              " 'I saw that.>',\n",
              " 'I saw them.>',\n",
              " 'I saw them.>',\n",
              " 'I saw them.>',\n",
              " 'I screamed.>',\n",
              " 'I see them.>',\n",
              " 'I survived.>',\n",
              " 'I threw up.>',\n",
              " 'I tried it.>',\n",
              " 'I tried it.>',\n",
              " 'I use this.>',\n",
              " 'I want one!>',\n",
              " 'I want one!>',\n",
              " 'I want one.>',\n",
              " 'I want one.>',\n",
              " 'I want you.>',\n",
              " 'I want you.>',\n",
              " 'I want you.>',\n",
              " 'I was away.>',\n",
              " 'I was busy.>',\n",
              " 'I was good.>',\n",
              " 'I was good.>',\n",
              " 'I was good.>',\n",
              " 'I was good.>',\n",
              " 'I was late.>',\n",
              " 'I was late.>',\n",
              " 'I was lost.>',\n",
              " 'I was lost.>',\n",
              " 'I was sick.>',\n",
              " 'I was sick.>',\n",
              " 'I will try.>',\n",
              " 'I work out.>',\n",
              " 'I wrote it.>',\n",
              " 'I wrote it.>',\n",
              " \"I'd accept.>\",\n",
              " \"I'll check.>\",\n",
              " \"I'll do it.>\",\n",
              " \"I'll do it.>\",\n",
              " \"I'll drive.>\",\n",
              " \"I'll go in.>\",\n",
              " \"I'll hurry.>\",\n",
              " \"I'll leave.>\",\n",
              " \"I'll shoot.>\",\n",
              " \"I'll stand.>\",\n",
              " \"I'll start.>\",\n",
              " \"I'm French.>\",\n",
              " \"I'm Korean.>\",\n",
              " \"I'm a hero.>\",\n",
              " \"I'm a liar.>\",\n",
              " \"I'm baking!>\",\n",
              " \"I'm better.>\",\n",
              " \"I'm buying.>\",\n",
              " \"I'm buying.>\",\n",
              " \"I'm chubby.>\",\n",
              " \"I'm chubby.>\",\n",
              " \"I'm eating.>\",\n",
              " \"I'm famous.>\",\n",
              " \"I'm famous.>\",\n",
              " \"I'm faster.>\",\n",
              " \"I'm flabby.>\",\n",
              " \"I'm greedy.>\",\n",
              " \"I'm greedy.>\",\n",
              " \"I'm greedy.>\",\n",
              " \"I'm hiding.>\",\n",
              " \"I'm honest.>\",\n",
              " \"I'm humble.>\",\n",
              " \"I'm hungry!>\",\n",
              " \"I'm hungry.>\",\n",
              " \"I'm immune.>\",\n",
              " \"I'm immune.>\",\n",
              " \"I'm in bed.>\",\n",
              " \"I'm in bed.>\",\n",
              " \"I'm in bed.>\",\n",
              " \"I'm joking.>\",\n",
              " \"I'm loaded.>\",\n",
              " \"I'm lonely.>\",\n",
              " \"I'm lonely.>\",\n",
              " \"I'm losing.>\",\n",
              " \"I'm moving.>\",\n",
              " \"I'm normal.>\",\n",
              " \"I'm normal.>\",\n",
              " \"I'm paying.>\",\n",
              " \"I'm paying.>\",\n",
              " \"I'm pooped.>\",\n",
              " \"I'm rested.>\",\n",
              " \"I'm rested.>\",\n",
              " \"I'm ruined.>\",\n",
              " \"I'm ruined.>\",\n",
              " \"I'm shaken.>\",\n",
              " \"I'm shaken.>\",\n",
              " \"I'm single.>\",\n",
              " \"I'm skinny.>\",\n",
              " \"I'm skinny.>\",\n",
              " \"I'm sleepy!>\",\n",
              " \"I'm sleepy!>\",\n",
              " \"I'm sneaky.>\",\n",
              " \"I'm sneaky.>\",\n",
              " \"I'm strict.>\",\n",
              " \"I'm strict.>\",\n",
              " \"I'm strong.>\",\n",
              " \"I'm strong.>\",\n",
              " \"I'm thirty.>\",\n",
              " \"I'm wasted.>\",\n",
              " \"I've tried.>\",\n",
              " \"I've tried.>\",\n",
              " 'Ignore Tom.>',\n",
              " 'Ignore Tom.>',\n",
              " 'Ignore him.>',\n",
              " 'Is he busy?>',\n",
              " 'Is he dead?>',\n",
              " 'Is he dead?>',\n",
              " 'Is he tall?>',\n",
              " 'Is it done?>',\n",
              " 'Is it free?>',\n",
              " 'Is it hard?>',\n",
              " 'Is it love?>',\n",
              " 'Is it love?>',\n",
              " 'Is it nice?>',\n",
              " 'Is it safe?>',\n",
              " 'Is it safe?>',\n",
              " 'Is it safe?>',\n",
              " 'Is it true?>',\n",
              " 'Is that it?>',\n",
              " 'Is that so?>',\n",
              " 'Is that so?>',\n",
              " 'Is that so?>',\n",
              " 'It is cold.>',\n",
              " 'It matters.>',\n",
              " 'It matters.>',\n",
              " 'It matters.>',\n",
              " 'It went OK.>',\n",
              " \"It'll work.>\",\n",
              " \"It'll work.>\",\n",
              " \"It's a fad.>\",\n",
              " \"It's a fad.>\",\n",
              " \"It's a fad.>\",\n",
              " \"It's awful.>\",\n",
              " \"It's awful.>\",\n",
              " \"It's bogus.>\",\n",
              " \"It's bulky.>\",\n",
              " \"It's cheap.>\",\n",
              " \"It's clean.>\",\n",
              " \"It's early.>\",\n",
              " \"It's funny.>\",\n",
              " \"It's funny.>\",\n",
              " \"It's funny.>\",\n",
              " \"It's green.>\",\n",
              " \"It's my CD.>\",\n",
              " \"It's night.>\",\n",
              " \"It's on me.>\",\n",
              " \"It's phony.>\",\n",
              " \"It's ready.>\",\n",
              " \"It's right.>\",\n",
              " \"It's safer.>\",\n",
              " \"It's sweet.>\",\n",
              " \"It's sweet.>\",\n",
              " \"It's weird.>\",\n",
              " \"It's wrong.>\",\n",
              " \"It's yours.>\",\n",
              " \"It's yours.>\",\n",
              " \"It's yours.>\",\n",
              " \"It's yours.>\",\n",
              " 'Just relax.>',\n",
              " 'Just relax.>',\n",
              " 'Keep quiet!>',\n",
              " 'Keep quiet!>',\n",
              " 'Keep quiet.>',\n",
              " 'Keep quiet.>',\n",
              " 'Keep quiet.>',\n",
              " 'Let him go!>',\n",
              " 'Let him go!>',\n",
              " 'Let him go!>',\n",
              " 'Let him go!>',\n",
              " 'Let him go.>',\n",
              " 'Let him go.>',\n",
              " 'Let him go.>',\n",
              " 'Let him go.>',\n",
              " 'Let him in.>',\n",
              " 'Let him in.>',\n",
              " 'Let him in.>',\n",
              " 'Let him in.>',\n",
              " 'Let me die.>',\n",
              " 'Let me die.>',\n",
              " 'Let me out!>',\n",
              " 'Let me out!>',\n",
              " 'Let me pay.>',\n",
              " 'Let me pay.>',\n",
              " 'Let me see.>',\n",
              " 'Let me try.>',\n",
              " 'Let me try.>',\n",
              " 'Let us out.>',\n",
              " 'Let us out.>',\n",
              " \"Let's chat.>\",\n",
              " \"Let's kiss.>\",\n",
              " \"Let's pray.>\",\n",
              " \"Let's talk.>\",\n",
              " \"Let's work.>\",\n",
              " 'Lighten up.>',\n",
              " 'Lighten up.>',\n",
              " 'Look at it.>',\n",
              " 'Look at it.>',\n",
              " 'Look at me.>',\n",
              " 'Look it up.>',\n",
              " 'Love hurts.>',\n",
              " 'Love hurts.>',\n",
              " 'Mama cried.>',\n",
              " 'Mama cried.>',\n",
              " 'Never mind!>',\n",
              " 'Never mind!>',\n",
              " 'No comment.>',\n",
              " 'No kidding?>',\n",
              " 'No kidding?>',\n",
              " 'No problem!>',\n",
              " 'No problem!>',\n",
              " 'No problem.>',\n",
              " 'No problem.>',\n",
              " 'No problem.>',\n",
              " 'No problem.>',\n",
              " 'Oh, really?>',\n",
              " 'Once again.>',\n",
              " 'Pick it up.>',\n",
              " 'Please sit.>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Say please.>',\n",
              " 'Say please.>',\n",
              " 'Shadow him.>',\n",
              " 'She is old.>',\n",
              " 'She smiled.>',\n",
              " \"She's busy.>\",\n",
              " \"She's nice.>\",\n",
              " 'Stand back!>',\n",
              " 'Stand back!>',\n",
              " 'Stand back!>',\n",
              " 'Stand back!>',\n",
              " 'Stay awake.>',\n",
              " 'Stay awake.>',\n",
              " 'Stay sharp.>',\n",
              " 'Step aside.>',\n",
              " 'Step aside.>',\n",
              " 'Stop lying.>',\n",
              " 'Stop lying.>',\n",
              " 'Study hard.>',\n",
              " 'Study hard.>',\n",
              " 'Take a bus.>',\n",
              " 'Take a nap.>',\n",
              " 'Talk to me!>',\n",
              " 'Talk to me!>',\n",
              " 'Talk to me!>',\n",
              " 'Talk to me.>',\n",
              " 'Talk to me.>',\n",
              " 'That a boy!>',\n",
              " 'That a boy!>',\n",
              " 'That hurts.>',\n",
              " 'That works.>',\n",
              " \"That's all.>\",\n",
              " \"That's fun.>\",\n",
              " \"That's fun.>\",\n",
              " \"That's her.>\",\n",
              " \"That's his.>\",\n",
              " \"That's his.>\",\n",
              " \"That's his.>\",\n",
              " \"That's odd.>\",\n",
              " 'They agree.>',\n",
              " 'They agree.>',\n",
              " 'They cheat.>',\n",
              " 'They cheat.>',\n",
              " 'They voted.>',\n",
              " 'They voted.>',\n",
              " 'This is it.>',\n",
              " 'This works.>',\n",
              " 'Time flies.>',\n",
              " 'Time flies.>',\n",
              " 'Time is up.>',\n",
              " 'Time is up.>',\n",
              " 'Tom agrees.>',\n",
              " 'Tom cheats.>',\n",
              " 'Tom drinks.>',\n",
              " 'Tom drives.>',\n",
              " 'Tom forgot.>',\n",
              " 'Tom forgot.>',\n",
              " 'Tom helped.>',\n",
              " 'Tom helped.>',\n",
              " 'Tom jumped.>',\n",
              " 'Tom looked.>',\n",
              " 'Tom looked.>',\n",
              " 'Tom nodded.>',\n",
              " 'Tom sighed.>',\n",
              " 'Tom snores.>',\n",
              " 'Tom yawned.>',\n",
              " \"Tom's dead.>\",\n",
              " \"Tom's deaf.>\",\n",
              " \"Tom's died.>\",\n",
              " \"Tom's fast.>\",\n",
              " \"Tom's free.>\",\n",
              " \"Tom's glad.>\",\n",
              " \"Tom's glad.>\",\n",
              " \"Tom's glad.>\",\n",
              " \"Tom's here.>\",\n",
              " \"Tom's here.>\",\n",
              " \"Tom's home.>\",\n",
              " \"Tom's home.>\",\n",
              " \"Tom's left.>\",\n",
              " \"Tom's left.>\",\n",
              " \"Tom's well.>\",\n",
              " 'Tough luck!>',\n",
              " 'Turn it up.>',\n",
              " 'Turn it up.>',\n",
              " 'Turn it up.>',\n",
              " 'Turn it up.>',\n",
              " 'Wait a bit.>',\n",
              " 'Wait a bit.>',\n",
              " 'Wait a sec.>',\n",
              " 'Wait a sec.>',\n",
              " 'We all die.>',\n",
              " 'We all die.>',\n",
              " 'We are men.>',\n",
              " 'We buy CDs.>',\n",
              " 'We can pay.>',\n",
              " 'We can try.>',\n",
              " 'We can try.>',\n",
              " 'We can win.>',\n",
              " 'We can win.>',\n",
              " 'We like it.>',\n",
              " 'We made it.>',\n",
              " 'We made it.>',\n",
              " 'We made it.>',\n",
              " 'We must go.>',\n",
              " 'We must go.>',\n",
              " 'We must go.>',\n",
              " 'We must go.>',\n",
              " 'We need it.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We want it.>',\n",
              " \"We'll cook.>\",\n",
              " \"We'll fail.>\",\n",
              " \"We'll help.>\",\n",
              " \"We'll obey.>\",\n",
              " \"We'll pass.>\",\n",
              " \"We'll swim.>\",\n",
              " \"We'll wait.>\",\n",
              " \"We'll walk.>\",\n",
              " \"We'll work.>\",\n",
              " \"We're back.>\",\n",
              " \"We're busy.>\",\n",
              " \"We're busy.>\",\n",
              " \"We're done.>\",\n",
              " \"We're done.>\",\n",
              " \"We're done.>\",\n",
              " \"We're done.>\",\n",
              " \"We're even.>\",\n",
              " \"We're fine.>\",\n",
              " \"We're here.>\",\n",
              " \"We're here.>\",\n",
              " \"We're home.>\",\n",
              " \"We're late.>\",\n",
              " \"We're lost.>\",\n",
              " \"We're lost.>\",\n",
              " \"We're rich.>\",\n",
              " \"We're safe.>\",\n",
              " \"We're sunk.>\",\n",
              " \"We're sunk.>\",\n",
              " \"We're weak.>\",\n",
              " 'What a day!>',\n",
              " 'What is it?>',\n",
              " 'What is it?>',\n",
              " \"What's new?>\",\n",
              " 'Where am I?>',\n",
              " 'Who are we?>',\n",
              " 'Who did it?>',\n",
              " 'Who did it?>',\n",
              " 'Who saw me?>',\n",
              " 'Who talked?>',\n",
              " 'Who yelled?>',\n",
              " 'Why bother?>',\n",
              " 'You decide.>',\n",
              " 'You decide.>',\n",
              " 'You did it!>',\n",
              " 'You did it!>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " \"You're bad.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're fun.>\",\n",
              " \"You're fun.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're sad.>\",\n",
              " \"You're sad.>\",\n",
              " \"You're shy.>\",\n",
              " \"You're shy.>\",\n",
              " \"You've won!>\",\n",
              " \"You've won!>\",\n",
              " 'All is well.',\n",
              " 'Am I hungry!',\n",
              " 'Am I stupid?',\n",
              " 'Am I stupid?',\n",
              " 'Anyone home?',\n",
              " 'Anyone home?',\n",
              " 'Anyone home?',\n",
              " 'Anyone hurt?',\n",
              " 'Anyone hurt?',\n",
              " 'Are we done?',\n",
              " 'Are we done?',\n",
              " 'Are we lost?',\n",
              " 'Are we lost?',\n",
              " 'Are we safe?',\n",
              " 'Are you Tom?',\n",
              " 'Are you Tom?',\n",
              " 'Are you mad?',\n",
              " 'Are you mad?',\n",
              " 'Are you mad?',\n",
              " 'Are you new?',\n",
              " 'Are you sad?',\n",
              " 'As you like.',\n",
              " 'As you like.',\n",
              " 'Ask anybody.',\n",
              " 'Ask anybody.',\n",
              " 'Ask anybody.',\n",
              " 'Ask anybody.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be ruthless.',\n",
              " 'Be ruthless.',\n",
              " 'Be ruthless.',\n",
              " 'Be sensible.',\n",
              " 'Be sensible.',\n",
              " 'Be sensible.',\n",
              " 'Be yourself.',\n",
              " 'Be yourself.',\n",
              " 'Break it up!',\n",
              " 'Can it wait?',\n",
              " 'Can we come?',\n",
              " 'Can we help?',\n",
              " 'Can we stop?',\n",
              " 'Can we talk?',\n",
              " 'Can you see?',\n",
              " 'Can you see?',\n",
              " 'Can you ski?',\n",
              " 'Can you ski?',\n",
              " 'Can you try?',\n",
              " 'Clean it up.',\n",
              " 'Clean it up.',\n",
              " 'Come get it.',\n",
              " 'Come get me.',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it.',\n",
              " 'Come off it.',\n",
              " 'Cook for me.',\n",
              " 'Cook for me.',\n",
              " 'Count me in.',\n",
              " 'Cover it up.',\n",
              " 'Cover it up.',\n",
              " 'Deal me out.',\n",
              " 'Did it hurt?',\n",
              " 'Did you win?',\n",
              " 'Did you win?',\n",
              " 'Did you win?',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it right.',\n",
              " 'Do it right.',\n",
              " 'Do your job.',\n",
              " \"Don't do it!\",\n",
              " \"Don't do it!\",\n",
              " \"Don't do it.\",\n",
              " \"Don't do it.\",\n",
              " \"Don't gloat.\",\n",
              " \"Don't gloat.\",\n",
              " \"Don't laugh.\",\n",
              " \"Don't laugh.\",\n",
              " \"Don't leave!\",\n",
              " \"Don't leave!\",\n",
              " \"Don't leave.\",\n",
              " \"Don't leave.\",\n",
              " \"Don't shoot!\",\n",
              " \"Don't shoot!\",\n",
              " \"Don't shoot.\",\n",
              " \"Don't shoot.\",\n",
              " \"Don't shout.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " 'Flip a coin.',\n",
              " 'Get serious.',\n",
              " 'Get the box.',\n",
              " 'Get the box.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go on ahead.',\n",
              " 'Go to sleep.',\n",
              " 'Grab a seat.',\n",
              " 'Have a beer.',\n",
              " 'Have a look.',\n",
              " 'Have a look.',\n",
              " 'Have a look.',\n",
              " 'Have a look.',\n",
              " 'Have a seat.',\n",
              " 'Have a seat.',\n",
              " 'Have we met?',\n",
              " 'He can come.',\n",
              " 'He can read.',\n",
              " 'He can swim.',\n",
              " 'He chuckled.',\n",
              " 'He chuckled.',\n",
              " 'He found it.',\n",
              " 'He found it.',\n",
              " 'He got away.',\n",
              " 'He grew old.',\n",
              " 'He grew old.',\n",
              " 'He has come!',\n",
              " 'He has guts.',\n",
              " 'He has wine.',\n",
              " 'He helps us.',\n",
              " 'He is drunk.',\n",
              " 'He is drunk.',\n",
              " 'He is eight.',\n",
              " 'He is hated.',\n",
              " 'He is nasty.',\n",
              " 'He is smart.',\n",
              " 'He is young.',\n",
              " 'He likes me.',\n",
              " 'He likes me.',\n",
              " 'He needs it.',\n",
              " 'He resigned.',\n",
              " 'He squinted.',\n",
              " 'He squinted.',\n",
              " 'He stood up.',\n",
              " 'He stood up.',\n",
              " 'He stood up.',\n",
              " 'He was busy.',\n",
              " \"He's a hunk.\",\n",
              " \"He's a hunk.\",\n",
              " \"He's a jerk.\",\n",
              " \"He's a liar.\",\n",
              " \"He's a nerd.\",\n",
              " \"He's a slob.\",\n",
              " \"He's asleep.\",\n",
              " \"He's coming.\",\n",
              " \"He's coming.\",\n",
              " \"He's crying.\",\n",
              " \"He's faking.\",\n",
              " \"He's loaded.\",\n",
              " \"He's loaded.\",\n",
              " \"He's loaded.\",\n",
              " \"He's my age.\",\n",
              " \"He's not in.\",\n",
              " \"He's not in.\",\n",
              " \"He's not in.\",\n",
              " 'Help me out.',\n",
              " 'Help me out.',\n",
              " 'Here I come.',\n",
              " 'Here I come.',\n",
              " 'Here she is!',\n",
              " 'Here we are!',\n",
              " 'Here we are!',\n",
              " 'Here we are!',\n",
              " 'Here we are.',\n",
              " 'Here we are.',\n",
              " 'Here we are.',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How curious!',\n",
              " 'How is life?',\n",
              " 'I also went.',\n",
              " 'I am French.',\n",
              " 'I am Korean.',\n",
              " 'I am a cook.',\n",
              " 'I am a monk.',\n",
              " 'I am better.',\n",
              " 'I am better.',\n",
              " 'I am coming.',\n",
              " 'I am hungry.',\n",
              " 'I am joking.',\n",
              " 'I am single.',\n",
              " 'I am taller.',\n",
              " 'I apologize.',\n",
              " 'I asked Tom.',\n",
              " 'I assume so.',\n",
              " 'I bought it.',\n",
              " 'I buried it.',\n",
              " 'I buried it.',\n",
              " 'I burned it.',\n",
              " 'I burned it.',\n",
              " 'I came back.',\n",
              " 'I can dance.',\n",
              " 'I can do it.',\n",
              " 'I can do it.',\n",
              " 'I can do it.',\n",
              " 'I can do it.',\n",
              " \"I can't eat.\",\n",
              " \"I can't say.\",\n",
              " \"I can't say.\",\n",
              " \"I can't see.\",\n",
              " \"I can't see.\",\n",
              " \"I can't see.\",\n",
              " \"I can't win.\",\n",
              " \"I can't win.\",\n",
              " \"I can't win.\",\n",
              " 'I caught it.',\n",
              " 'I caught it.',\n",
              " 'I confessed.',\n",
              " 'I confessed.',\n",
              " 'I could try.',\n",
              " 'I cut class.',\n",
              " \"I didn't go.\",\n",
              " \"I didn't go.\",\n",
              " \"I didn't go.\",\n",
              " \"I didn't go.\",\n",
              " 'I eat alone.',\n",
              " 'I eat alone.',\n",
              " 'I eat bread.',\n",
              " 'I eat fruit.',\n",
              " 'I exercised.',\n",
              " 'I exercised.',\n",
              " 'I feel blue.',\n",
              " 'I feel blue.',\n",
              " 'I feel cold.',\n",
              " 'I feel fine.',\n",
              " 'I feel good.',\n",
              " 'I feel lost.',\n",
              " 'I feel lost.',\n",
              " 'I feel safe.',\n",
              " 'I feel sick.',\n",
              " 'I feel weak.',\n",
              " 'I feel well.',\n",
              " 'I felt dumb.',\n",
              " 'I felt dumb.',\n",
              " 'I felt fear.',\n",
              " 'I felt good.',\n",
              " 'I felt safe.',\n",
              " 'I felt safe.',\n",
              " 'I got bored.',\n",
              " 'I got dizzy.',\n",
              " 'I got dizzy.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got lucky.',\n",
              " 'I got lucky.',\n",
              " 'I got upset.',\n",
              " 'I got upset.',\n",
              " 'I got upset.',\n",
              " 'I got upset.',\n",
              " 'I guess not.',\n",
              " 'I guess not.',\n",
              " 'I had to go.',\n",
              " 'I hate dogs.',\n",
              " 'I hate dogs.',\n",
              " 'I hate fish.',\n",
              " 'I hate golf.',\n",
              " 'I hate milk.',\n",
              " 'I hate sand.',\n",
              " 'I hate sand.',\n",
              " 'I hate that.',\n",
              " 'I hate them.',\n",
              " 'I hate this.',\n",
              " 'I hate work.',\n",
              " 'I have cash.',\n",
              " 'I have cash.',\n",
              " 'I have eyes.',\n",
              " 'I have food.',\n",
              " 'I have food.',\n",
              " 'I have time.',\n",
              " 'I have time.',\n",
              " 'I have wine.',\n",
              " 'I know that.',\n",
              " 'I know them.',\n",
              " 'I know this.',\n",
              " 'I know this.',\n",
              " 'I like blue.',\n",
              " 'I like blue.',\n",
              " 'I like both.',\n",
              " 'I like both.',\n",
              " 'I like cake.',\n",
              " 'I like cats.',\n",
              " 'I like cats.',\n",
              " 'I like dogs.',\n",
              " 'I like jazz.',\n",
              " 'I like math.',\n",
              " 'I like milk.',\n",
              " 'I like rice.',\n",
              " 'I like rock.',\n",
              " 'I like that.',\n",
              " 'I like that.',\n",
              " 'I like them.',\n",
              " 'I like this.',\n",
              " 'I like this.',\n",
              " 'I live here.',\n",
              " 'I love Mary.',\n",
              " 'I love kids.',\n",
              " 'I love life.',\n",
              " 'I loved you.',\n",
              " 'I loved you.',\n",
              " 'I loved you.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I must obey.',\n",
              " 'I must obey.',\n",
              " 'I nailed it.',\n",
              " 'I nailed it.',\n",
              " 'I nailed it.',\n",
              " 'I need food.',\n",
              " 'I need glue.',\n",
              " 'I need more.',\n",
              " 'I need more.',\n",
              " 'I need some.',\n",
              " 'I need that.',\n",
              " 'I need that.',\n",
              " 'I need that.',\n",
              " 'I need time.',\n",
              " 'I need time.',\n",
              " 'I never bet.',\n",
              " 'I never bet.',\n",
              " 'I never win.',\n",
              " 'I never win.',\n",
              " 'I often ski.',\n",
              " 'I oppose it.',\n",
              " 'I oppose it.',\n",
              " 'I pay taxes.',\n",
              " 'I pay taxes.',\n",
              " 'I recovered.',\n",
              " 'I recovered.',\n",
              " 'I recovered.',\n",
              " 'I recovered.',\n",
              " 'I said stop.',\n",
              " 'I said that.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saw a dog.',\n",
              " 'I sell cars.',\n",
              " 'I should go.',\n",
              " 'I should go.',\n",
              " 'I smell gas.',\n",
              " 'I surrender.',\n",
              " 'I thank you.',\n",
              " 'I trust her.',\n",
              " 'I trust him.',\n",
              " 'I trust him.',\n",
              " 'I trust you.',\n",
              " 'I trust you.',\n",
              " 'I waited up.',\n",
              " 'I want Mary.',\n",
              " 'I want cash.',\n",
              " 'I want eggs.',\n",
              " 'I want kids.',\n",
              " 'I want mine.',\n",
              " 'I want mine.',\n",
              " 'I want more.',\n",
              " 'I want more.',\n",
              " 'I want soup.',\n",
              " 'I want that.',\n",
              " 'I want that.',\n",
              " 'I want them.',\n",
              " 'I want this.',\n",
              " 'I want time.',\n",
              " 'I was alone.',\n",
              " 'I was alone.',\n",
              " 'I was alone.',\n",
              " 'I was bored.',\n",
              " 'I was broke.',\n",
              " 'I was dizzy.',\n",
              " 'I was dizzy.',\n",
              " 'I was drunk.',\n",
              " 'I was fired.',\n",
              " 'I was fired.',\n",
              " 'I was fired.',\n",
              " 'I was hired.',\n",
              " 'I was lucky.',\n",
              " 'I was moved.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was ready.',\n",
              " 'I was ready.',\n",
              " 'I was ready.',\n",
              " 'I was sober.',\n",
              " 'I was sorry.',\n",
              " 'I was sorry.',\n",
              " 'I was there.',\n",
              " 'I was there.',\n",
              " 'I was there.',\n",
              " 'I was there.',\n",
              " 'I was tired.',\n",
              " 'I was tired.',\n",
              " 'I was wrong.',\n",
              " 'I was wrong.',\n",
              " 'I was wrong.',\n",
              " 'I washed it.',\n",
              " 'I washed it.',\n",
              " 'I went home.',\n",
              " 'I went, too.',\n",
              " 'I went, too.',\n",
              " 'I went, too.',\n",
              " 'I will come.',\n",
              " 'I will help.',\n",
              " 'I will obey.',\n",
              " 'I will wait.',\n",
              " 'I will walk.',\n",
              " 'I will work.',\n",
              " 'I will work.',\n",
              " 'I won again.',\n",
              " \"I won't cry.\",\n",
              " \"I won't cry.\",\n",
              " 'I work here.',\n",
              " \"I'll attend.\",\n",
              " \"I'll attend.\",\n",
              " \"I'll buy it.\",\n",
              " \"I'll cancel.\",\n",
              " \"I'll change.\",\n",
              " \"I'll decide.\",\n",
              " \"I'll decide.\",\n",
              " \"I'll get in.\",\n",
              " \"I'll get it.\",\n",
              " \"I'll get it.\",\n",
              " \"I'll get up.\",\n",
              " \"I'll go now.\",\n",
              " \"I'll go see.\",\n",
              " \"I'll manage.\",\n",
              " \"I'll scream.\",\n",
              " \"I'll try it.\",\n",
              " \"I'll try it.\",\n",
              " \"I'm 17, too.\",\n",
              " \"I'm Finnish.\",\n",
              " \"I'm Finnish.\",\n",
              " \"I'm Italian.\",\n",
              " \"I'm a baker.\",\n",
              " \"I'm a baker.\",\n",
              " \"I'm all set.\",\n",
              " \"I'm all set.\",\n",
              " \"I'm ashamed.\",\n",
              " \"I'm at home.\",\n",
              " \"I'm at home.\",\n",
              " \"I'm baffled.\",\n",
              " \"I'm blessed.\",\n",
              " \"I'm blessed.\",\n",
              " \"I'm careful.\",\n",
              " \"I'm careful.\",\n",
              " \"I'm certain.\",\n",
              " \"I'm certain.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Acq_F0bUlua"
      },
      "source": [
        "I trained all day, but the performance is still poor. I'll figure out why later. Maybe the RNN is too simple? Let's see.\n",
        "\n",
        "The problem is that in order to train the model by minibatch, I have to add a lot of padding tokens into the training set. This hurts the performance of the model. As seen in the above example, when I tried to translate the English sentence 'closer look' into French, there are a lot of padding tokens (&&&), which is annoying.\n",
        "\n",
        "I think if I train the model with individual examples, then the problem could be relieved. However, the downside is that it could be time-consuming.\n",
        "\n",
        "Anyway, I will try to update another file that will use attention mechanism.\n",
        "\n",
        "See you in another Colab file\n",
        "\n",
        "see you in the next week?\n",
        "\n",
        "I have to take care the new baby in my family. so in the weekend, I didnot have the time to adjust my code, and train the model. I will try to do that in the next week. see you then.\n",
        "\n",
        "I know, this is a small project, I just want to write those projects to be more familiar with the NLP, which is helpful for me in my future phd study.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I finally found the where the problem.\n",
        "\n",
        "first I should not only use the final hidden state of the encoder. I should concate all the hidden state, and convert them into decoder.\n",
        "\n",
        "second for the decoder, for the prediction, the predicted character should be conditioned by the previous characters, the current state, and the state of the encoder.\n",
        "\n",
        "third, it is for the loss function. when I calcualte the loss, if encounter the padding, I should stop, because the error in the loss could cause the model to go at the wrong direction.\n",
        "\n",
        "okay, I will ask for more advices from chatGPT, continue this project.\n",
        "\n",
        "one more thing to mention is that, by doing the second step, I could easily add the attention machaism between the encoder and the decoder."
      ],
      "metadata": {
        "id": "xeg_uyoLvunJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now\n",
        "I changed some code in here, but the performance is still poor.\n",
        "what I change:\n",
        "1: use the final hidden state of the encoder, when predict the next token.\n",
        "2: when calculating the loss function, remove the padding tokens.\n",
        "\n",
        "\n",
        "okay, I think I could pause now,\n",
        "I will create another file to run the seq-2seq with attention, and check its performance. then use the transformers.\n",
        "\n",
        "if the performance is still poor, then I will use the word-level token not the character level.\n",
        "\n",
        "see you"
      ],
      "metadata": {
        "id": "zVpu55Scla2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have changed the approach in which the hidden_state of the encoder influlence the decoder prediction, but the performance is still poor.\n",
        "before using the word-level token, I will debug the training.\n",
        "let see."
      ],
      "metadata": {
        "id": "wpwPgUBBn_O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after debugging, I finnaly find the clue why the perforamcne is poor\n",
        "after each training step. the return result of the forward function is [T,B,C].\n",
        "when I use view(-1,C) to feed into the criterion. but then the order of the tokens is wrong. So I use permute to reshape the result.\n",
        "\n",
        "the result is still not perfect, but get better know.\n"
      ],
      "metadata": {
        "id": "oVlI9ILPRP_W"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ/dU6kic4UfQT/4plwl1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}