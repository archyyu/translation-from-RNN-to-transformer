{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAzMVzsKlJlV",
        "outputId": "75b735a8-878c-4cbe-de10-732036f4cd8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d12903743d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dKvwvE3qUB3q"
      },
      "outputs": [],
      "source": [
        "hidden_size = 100\n",
        "embedding_dim = 30\n",
        "learning_rate = 1e-1 * 0.1\n",
        "batch_size = 50\n",
        "beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pgaJy39hUt2C"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(1000,2000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append(item[0] + '>')\n",
        "  fr_lines.append(item[1] + '>')\n",
        "\n",
        "max_len_line_en = min([len(l) for l in en_lines])\n",
        "max_len_line_fr = min([len(l) for l in fr_lines])\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) > max_len_line_en):\n",
        "    en_lines[i] = en_lines[i][0:max_len_line_en]\n",
        "  if (len(fr_lines[i]) > max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i][0:max_len_line_fr]\n",
        "\n",
        "\n",
        "source_vocab = sorted(set(''.join(en_lines)))\n",
        "target_vocab = sorted(set(''.join(fr_lines)))\n",
        "target_vocab.append('<')\n",
        "\n",
        "source_vocab_size = len(set(''.join(source_vocab)))\n",
        "target_vocab_size = len(set(''.join(target_vocab)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}\n",
        "\n",
        "# padding_token_index = target_char_to_ix[padding_character]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GUfLKgxoU7j3"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([source_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "def target_line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([target_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "en_data = []\n",
        "fr_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_aiwNJdMpOvb"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "    return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "    self.i2h = nn.Linear(self.embedding_dim, self.hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "    self.h2o = nn.Linear(self.hidden_size * 2, vocab_size, bias=False)\n",
        "\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, self.hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "  def init_state(self, encode_state):\n",
        "    self.encode_state = encode_state\n",
        "\n",
        "\n",
        "  # def forward(self, target):\n",
        "\n",
        "  #   # if x is None:\n",
        "  #   h = torch.zeros(1, self.hidden_size)\n",
        "  #   output = []\n",
        "  #   for i in range(max_len_line_fr):\n",
        "  #     x = target[:,i]\n",
        "  #     t = self.embedding(x)\n",
        "  #     h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "  #     y = self.h2o(torch.cat((self.encode_state, h), dim=-1)) + self.ob\n",
        "  #     output.append(y)\n",
        "  #   return torch.stack(output, dim=0)\n",
        "\n",
        "  def forward(self, batch_size):\n",
        "\n",
        "    # if x is None:\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    x = torch.tensor([target_char_to_ix[start_character] for _ in range(batch_size)],dtype=torch.long)\n",
        "    output = []\n",
        "    for i in range(max_len_line_fr):\n",
        "      t = self.embedding(x)\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      y = self.h2o(torch.cat((self.encode_state, h), dim=-1)) + self.ob\n",
        "      p = nn.functional.softmax(y, dim=1)\n",
        "      ix = torch.argmax(p, dim=-1)\n",
        "      x = ix\n",
        "      output.append(y)\n",
        "    return torch.stack(output, dim=0).permute(1, 0, 2)\n",
        "\n",
        "  def beam_search(self):\n",
        "    \"\"\"\n",
        "    Perform beam search to generate sequences.\n",
        "    \"\"\"\n",
        "    beams = [(torch.tensor([target_char_to_ix[start_character]], dtype=torch.long), 1.0)]\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "\n",
        "    for i in range(max_len_line_fr):\n",
        "      new_beams = []\n",
        "\n",
        "      for seq, score in beams:\n",
        "        x = seq[-1].view(1, -1)  # Take the last predicted token\n",
        "\n",
        "        t = self.embedding(x)\n",
        "        h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "        y = self.h2o(torch.cat((self.encode_state.unsqueeze(1), h), dim=-1)) + self.ob\n",
        "        p = F.softmax(y, dim=-1)\n",
        "        top_probs, top_ix = torch.topk(p, beam_width, dim=-1)\n",
        "\n",
        "        for prob, token_ix in zip(top_probs[0][0], top_ix[0][0]):\n",
        "          new_seq = torch.cat((seq, torch.tensor([token_ix], dtype=torch.long)), dim=0)\n",
        "          new_beams.append((new_seq, score * prob.item()))\n",
        "\n",
        "      beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return beams\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = Encoder(source_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "    self.decoder = Decoder(target_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "  def forward(self, source, batch_size):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    output = self.decoder(batch_size)\n",
        "    return output\n",
        "\n",
        "  def translate(self, source):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    beams = self.decoder.beam_search()\n",
        "    return beams\n",
        "\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "model = Seq2Seq(source_vocab_size, target_vocab_size, embedding_dim, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "1HwqI27qRs9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c34f7db-9860-4917-ad3e-f0565bdb8334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p 0, Loss: 2.0768420696258545\n",
            "p 300, Loss: 2.582000494003296\n",
            "p 600, Loss: 2.5315101146698\n",
            "p 900, Loss: 2.0100529193878174\n",
            "p 0, Loss: 2.0279698371887207\n",
            "p 300, Loss: 2.4813873767852783\n",
            "p 600, Loss: 2.4581587314605713\n",
            "p 900, Loss: 1.7988905906677246\n",
            "p 0, Loss: 2.1648037433624268\n",
            "p 300, Loss: 2.6059515476226807\n",
            "p 600, Loss: 2.6002471446990967\n",
            "p 900, Loss: 1.9809889793395996\n",
            "p 0, Loss: 2.0555474758148193\n",
            "p 300, Loss: 2.4510719776153564\n",
            "p 600, Loss: 2.4972212314605713\n",
            "p 900, Loss: 1.905595302581787\n",
            "p 0, Loss: 2.194617748260498\n",
            "p 300, Loss: 2.6806352138519287\n",
            "p 600, Loss: 2.361604928970337\n",
            "p 900, Loss: 2.4408352375030518\n",
            "p 0, Loss: 1.993909239768982\n",
            "p 300, Loss: 2.4522523880004883\n",
            "p 600, Loss: 2.6946771144866943\n",
            "p 900, Loss: 2.058340549468994\n",
            "p 0, Loss: 2.3536906242370605\n",
            "p 300, Loss: 2.731320858001709\n",
            "p 600, Loss: 2.4768028259277344\n",
            "p 900, Loss: 2.1593918800354004\n",
            "p 0, Loss: 2.3043787479400635\n",
            "p 300, Loss: 2.5196640491485596\n",
            "p 600, Loss: 2.6683099269866943\n",
            "p 900, Loss: 1.9398373365402222\n",
            "p 0, Loss: 2.118190288543701\n",
            "p 300, Loss: 2.6127467155456543\n",
            "p 600, Loss: 2.423929214477539\n",
            "p 900, Loss: 2.1124608516693115\n",
            "p 0, Loss: 2.14020037651062\n",
            "p 300, Loss: 2.5293710231781006\n",
            "p 600, Loss: 2.5771379470825195\n",
            "p 900, Loss: 1.9086359739303589\n",
            "p 0, Loss: 2.083481788635254\n",
            "p 300, Loss: 2.523954153060913\n",
            "p 600, Loss: 2.429251194000244\n",
            "p 900, Loss: 2.062652349472046\n",
            "p 0, Loss: 2.1546552181243896\n",
            "p 300, Loss: 2.4779469966888428\n",
            "p 600, Loss: 2.5829222202301025\n",
            "p 900, Loss: 2.0196094512939453\n",
            "p 0, Loss: 1.9623967409133911\n",
            "p 300, Loss: 2.749263286590576\n",
            "p 600, Loss: 2.537163496017456\n",
            "p 900, Loss: 2.381270170211792\n",
            "p 0, Loss: 2.3158743381500244\n",
            "p 300, Loss: 2.5811879634857178\n",
            "p 600, Loss: 2.5849924087524414\n",
            "p 900, Loss: 1.94051194190979\n",
            "p 0, Loss: 2.0615875720977783\n",
            "p 300, Loss: 2.571937322616577\n",
            "p 600, Loss: 2.5067074298858643\n",
            "p 900, Loss: 2.1536128520965576\n",
            "p 0, Loss: 2.0873818397521973\n",
            "p 300, Loss: 2.5596425533294678\n",
            "p 600, Loss: 2.5444209575653076\n",
            "p 900, Loss: 2.024331569671631\n",
            "p 0, Loss: 1.9983407258987427\n",
            "p 300, Loss: 2.542120933532715\n",
            "p 600, Loss: 2.434135675430298\n",
            "p 900, Loss: 2.3220133781433105\n",
            "p 0, Loss: 2.189828872680664\n",
            "p 300, Loss: 2.7374136447906494\n",
            "p 600, Loss: 2.62269926071167\n",
            "p 900, Loss: 1.9012609720230103\n",
            "p 0, Loss: 2.0147063732147217\n",
            "p 300, Loss: 2.5174784660339355\n",
            "p 600, Loss: 2.587909698486328\n",
            "p 900, Loss: 2.1192944049835205\n",
            "p 0, Loss: 2.0793778896331787\n",
            "p 300, Loss: 2.3748321533203125\n",
            "p 600, Loss: 2.717376232147217\n",
            "p 900, Loss: 1.862567663192749\n",
            "p 0, Loss: 2.0373265743255615\n",
            "p 300, Loss: 2.4818413257598877\n",
            "p 600, Loss: 2.4808473587036133\n",
            "p 900, Loss: 2.2711899280548096\n",
            "p 0, Loss: 2.280792713165283\n",
            "p 300, Loss: 2.35915207862854\n",
            "p 600, Loss: 2.5254337787628174\n",
            "p 900, Loss: 1.8085576295852661\n",
            "p 0, Loss: 1.9185876846313477\n",
            "p 300, Loss: 2.491919994354248\n",
            "p 600, Loss: 2.365356683731079\n",
            "p 900, Loss: 2.304882526397705\n",
            "p 0, Loss: 2.0630130767822266\n",
            "p 300, Loss: 2.238274335861206\n",
            "p 600, Loss: 2.5788512229919434\n",
            "p 900, Loss: 1.86472487449646\n",
            "p 0, Loss: 2.1090188026428223\n",
            "p 300, Loss: 2.529844045639038\n",
            "p 600, Loss: 2.396280527114868\n",
            "p 900, Loss: 2.435161590576172\n",
            "p 0, Loss: 2.3792495727539062\n",
            "p 300, Loss: 2.3278143405914307\n",
            "p 600, Loss: 2.4804952144622803\n",
            "p 900, Loss: 1.921303153038025\n",
            "p 0, Loss: 1.9398250579833984\n",
            "p 300, Loss: 2.6606879234313965\n",
            "p 600, Loss: 2.445833683013916\n",
            "p 900, Loss: 2.0709054470062256\n",
            "p 0, Loss: 2.196723699569702\n",
            "p 300, Loss: 2.4688398838043213\n",
            "p 600, Loss: 2.492710590362549\n",
            "p 900, Loss: 2.031935214996338\n",
            "p 0, Loss: 1.9652903079986572\n",
            "p 300, Loss: 2.9701876640319824\n",
            "p 600, Loss: 2.5120339393615723\n",
            "p 900, Loss: 2.13285231590271\n",
            "p 0, Loss: 2.206057071685791\n",
            "p 300, Loss: 2.4618632793426514\n",
            "p 600, Loss: 2.5553078651428223\n",
            "p 900, Loss: 1.8630784749984741\n",
            "p 0, Loss: 1.8934472799301147\n",
            "p 300, Loss: 2.9005954265594482\n",
            "p 600, Loss: 2.196293830871582\n",
            "p 900, Loss: 2.3013510704040527\n",
            "p 0, Loss: 2.2477478981018066\n",
            "p 300, Loss: 2.4096457958221436\n",
            "p 600, Loss: 2.5081372261047363\n",
            "p 900, Loss: 1.9369852542877197\n",
            "p 0, Loss: 2.0190269947052\n",
            "p 300, Loss: 2.700256109237671\n",
            "p 600, Loss: 2.2483813762664795\n",
            "p 900, Loss: 2.1285817623138428\n",
            "p 0, Loss: 2.2385458946228027\n",
            "p 300, Loss: 2.4360296726226807\n",
            "p 600, Loss: 2.5682544708251953\n",
            "p 900, Loss: 1.990782618522644\n",
            "p 0, Loss: 2.2216451168060303\n",
            "p 300, Loss: 2.8564441204071045\n",
            "p 600, Loss: 2.4036998748779297\n",
            "p 900, Loss: 2.2243635654449463\n",
            "p 0, Loss: 2.321925640106201\n",
            "p 300, Loss: 2.6525278091430664\n",
            "p 600, Loss: 2.5231099128723145\n",
            "p 900, Loss: 1.9287958145141602\n",
            "p 0, Loss: 2.0117907524108887\n",
            "p 300, Loss: 2.750272274017334\n",
            "p 600, Loss: 2.6749484539031982\n",
            "p 900, Loss: 2.2228262424468994\n",
            "p 0, Loss: 2.2951087951660156\n",
            "p 300, Loss: 2.5848844051361084\n",
            "p 600, Loss: 2.5426392555236816\n",
            "p 900, Loss: 1.935441493988037\n",
            "p 0, Loss: 2.001492977142334\n",
            "p 300, Loss: 3.1063008308410645\n",
            "p 600, Loss: 2.53752064704895\n",
            "p 900, Loss: 2.0009703636169434\n",
            "p 0, Loss: 2.1864986419677734\n",
            "p 300, Loss: 2.684093713760376\n",
            "p 600, Loss: 2.577281951904297\n",
            "p 900, Loss: 2.0097763538360596\n",
            "p 0, Loss: 1.8174529075622559\n",
            "p 300, Loss: 2.7855284214019775\n",
            "p 600, Loss: 2.56123948097229\n",
            "p 900, Loss: 2.0601675510406494\n",
            "p 0, Loss: 2.1019375324249268\n",
            "p 300, Loss: 2.750817060470581\n",
            "p 600, Loss: 2.5871195793151855\n",
            "p 900, Loss: 1.812199354171753\n",
            "p 0, Loss: 1.9545087814331055\n",
            "p 300, Loss: 2.701531410217285\n",
            "p 600, Loss: 2.4355380535125732\n",
            "p 900, Loss: 2.0197339057922363\n",
            "p 0, Loss: 2.2124462127685547\n",
            "p 300, Loss: 2.665752410888672\n",
            "p 600, Loss: 2.507176160812378\n",
            "p 900, Loss: 1.8719779253005981\n",
            "p 0, Loss: 2.025486946105957\n",
            "p 300, Loss: 2.8216969966888428\n",
            "p 600, Loss: 2.7608044147491455\n",
            "p 900, Loss: 1.9444109201431274\n",
            "p 0, Loss: 2.0585594177246094\n",
            "p 300, Loss: 2.734527349472046\n",
            "p 600, Loss: 2.519047737121582\n",
            "p 900, Loss: 1.9457519054412842\n",
            "p 0, Loss: 1.808371901512146\n",
            "p 300, Loss: 2.6704602241516113\n",
            "p 600, Loss: 2.437790870666504\n",
            "p 900, Loss: 1.9877077341079712\n",
            "p 0, Loss: 2.064652919769287\n",
            "p 300, Loss: 2.632774591445923\n",
            "p 600, Loss: 2.468446969985962\n",
            "p 900, Loss: 1.8692822456359863\n",
            "p 0, Loss: 1.8424347639083862\n",
            "p 300, Loss: 2.7161478996276855\n",
            "p 600, Loss: 2.417208194732666\n",
            "p 900, Loss: 1.986393690109253\n",
            "p 0, Loss: 2.066162347793579\n",
            "p 300, Loss: 2.6320152282714844\n",
            "p 600, Loss: 2.4463212490081787\n",
            "p 900, Loss: 1.90998375415802\n",
            "p 0, Loss: 1.878015160560608\n",
            "p 300, Loss: 2.7297723293304443\n",
            "p 600, Loss: 2.4424147605895996\n",
            "p 900, Loss: 1.827238917350769\n",
            "p 0, Loss: 1.892898678779602\n",
            "p 300, Loss: 2.364248514175415\n",
            "p 600, Loss: 2.3518662452697754\n",
            "p 900, Loss: 1.779843807220459\n",
            "p 0, Loss: 2.0383217334747314\n",
            "p 300, Loss: 2.5441038608551025\n",
            "p 600, Loss: 2.6859631538391113\n",
            "p 900, Loss: 1.86204993724823\n",
            "p 0, Loss: 1.989559292793274\n",
            "p 300, Loss: 2.358025074005127\n",
            "p 600, Loss: 2.599350690841675\n",
            "p 900, Loss: 1.8038545846939087\n",
            "p 0, Loss: 1.8401142358779907\n",
            "p 300, Loss: 2.382504463195801\n",
            "p 600, Loss: 2.5082523822784424\n",
            "p 900, Loss: 1.849281907081604\n",
            "p 0, Loss: 2.02877140045166\n",
            "p 300, Loss: 2.5292787551879883\n",
            "p 600, Loss: 2.4042129516601562\n",
            "p 900, Loss: 1.8956300020217896\n",
            "p 0, Loss: 1.966354489326477\n",
            "p 300, Loss: 2.506901502609253\n",
            "p 600, Loss: 2.379084587097168\n",
            "p 900, Loss: 1.9156888723373413\n",
            "p 0, Loss: 1.9427040815353394\n",
            "p 300, Loss: 2.533231258392334\n",
            "p 600, Loss: 2.7033791542053223\n",
            "p 900, Loss: 1.8685011863708496\n",
            "p 0, Loss: 2.0746593475341797\n",
            "p 300, Loss: 2.499901294708252\n",
            "p 600, Loss: 2.4795382022857666\n",
            "p 900, Loss: 1.9027819633483887\n",
            "p 0, Loss: 1.9102059602737427\n",
            "p 300, Loss: 2.593553304672241\n",
            "p 600, Loss: 2.626800298690796\n",
            "p 900, Loss: 1.966148018836975\n",
            "p 0, Loss: 2.060300588607788\n",
            "p 300, Loss: 2.778756856918335\n",
            "p 600, Loss: 2.5860424041748047\n",
            "p 900, Loss: 1.9551794528961182\n",
            "p 0, Loss: 2.0088555812835693\n",
            "p 300, Loss: 2.775211811065674\n",
            "p 600, Loss: 2.6273863315582275\n",
            "p 900, Loss: 1.9751663208007812\n",
            "p 0, Loss: 2.1813182830810547\n",
            "p 300, Loss: 2.6473138332366943\n",
            "p 600, Loss: 2.503185987472534\n",
            "p 900, Loss: 1.9149850606918335\n",
            "p 0, Loss: 1.9463551044464111\n",
            "p 300, Loss: 2.463848352432251\n",
            "p 600, Loss: 2.3811607360839844\n",
            "p 900, Loss: 1.9628522396087646\n",
            "p 0, Loss: 2.101627826690674\n",
            "p 300, Loss: 2.538647413253784\n",
            "p 600, Loss: 2.482670545578003\n",
            "p 900, Loss: 1.915143370628357\n",
            "p 0, Loss: 2.1050572395324707\n",
            "p 300, Loss: 2.5590829849243164\n",
            "p 600, Loss: 2.3126680850982666\n",
            "p 900, Loss: 1.7999187707901\n",
            "p 0, Loss: 1.9800329208374023\n",
            "p 300, Loss: 2.5813393592834473\n",
            "p 600, Loss: 2.395176887512207\n",
            "p 900, Loss: 1.7510322332382202\n",
            "p 0, Loss: 2.037001848220825\n",
            "p 300, Loss: 2.5966928005218506\n",
            "p 600, Loss: 2.3333308696746826\n",
            "p 900, Loss: 1.7557727098464966\n",
            "p 0, Loss: 2.1251044273376465\n",
            "p 300, Loss: 2.6483521461486816\n",
            "p 600, Loss: 2.4119038581848145\n",
            "p 900, Loss: 1.8945128917694092\n",
            "p 0, Loss: 2.044846773147583\n",
            "p 300, Loss: 2.9713926315307617\n",
            "p 600, Loss: 2.494208574295044\n",
            "p 900, Loss: 1.8679386377334595\n",
            "p 0, Loss: 2.0613467693328857\n",
            "p 300, Loss: 2.784046173095703\n",
            "p 600, Loss: 2.423734426498413\n",
            "p 900, Loss: 1.8552544116973877\n",
            "p 0, Loss: 1.9057079553604126\n",
            "p 300, Loss: 2.561323642730713\n",
            "p 600, Loss: 2.2280828952789307\n",
            "p 900, Loss: 1.7949031591415405\n",
            "p 0, Loss: 2.0688798427581787\n",
            "p 300, Loss: 2.8387250900268555\n",
            "p 600, Loss: 2.635016918182373\n",
            "p 900, Loss: 2.0146870613098145\n",
            "p 0, Loss: 1.8958096504211426\n",
            "p 300, Loss: 2.4659407138824463\n",
            "p 600, Loss: 2.30959153175354\n",
            "p 900, Loss: 1.836974024772644\n",
            "p 0, Loss: 2.0739476680755615\n",
            "p 300, Loss: 2.770927667617798\n",
            "p 600, Loss: 2.3603146076202393\n",
            "p 900, Loss: 2.1073718070983887\n",
            "p 0, Loss: 2.0112411975860596\n",
            "p 300, Loss: 2.6939854621887207\n",
            "p 600, Loss: 2.3429465293884277\n",
            "p 900, Loss: 1.9386614561080933\n",
            "p 0, Loss: 2.045621395111084\n",
            "p 300, Loss: 2.678250312805176\n",
            "p 600, Loss: 2.5812180042266846\n",
            "p 900, Loss: 2.2312309741973877\n",
            "p 0, Loss: 1.9823634624481201\n",
            "p 300, Loss: 2.745820999145508\n",
            "p 600, Loss: 2.3974714279174805\n",
            "p 900, Loss: 1.9545588493347168\n",
            "p 0, Loss: 2.0787084102630615\n",
            "p 300, Loss: 2.7851598262786865\n",
            "p 600, Loss: 2.6029605865478516\n",
            "p 900, Loss: 2.233818292617798\n",
            "p 0, Loss: 2.1618049144744873\n",
            "p 300, Loss: 2.7882959842681885\n",
            "p 600, Loss: 2.9250640869140625\n",
            "p 900, Loss: 2.3376243114471436\n",
            "p 0, Loss: 2.668490171432495\n",
            "p 300, Loss: 2.984004497528076\n",
            "p 600, Loss: 2.8174731731414795\n",
            "p 900, Loss: 2.2766571044921875\n",
            "p 0, Loss: 2.2835144996643066\n",
            "p 300, Loss: 2.84415864944458\n",
            "p 600, Loss: 2.61045503616333\n",
            "p 900, Loss: 2.161607265472412\n",
            "p 0, Loss: 2.1864829063415527\n",
            "p 300, Loss: 2.666830062866211\n",
            "p 600, Loss: 2.728447675704956\n",
            "p 900, Loss: 2.0722053050994873\n",
            "p 0, Loss: 2.100303888320923\n",
            "p 300, Loss: 2.488609552383423\n",
            "p 600, Loss: 2.573117971420288\n",
            "p 900, Loss: 1.9540534019470215\n",
            "p 0, Loss: 2.0956618785858154\n",
            "p 300, Loss: 2.560899019241333\n",
            "p 600, Loss: 2.761051654815674\n",
            "p 900, Loss: 2.0483815670013428\n",
            "p 0, Loss: 2.042686700820923\n",
            "p 300, Loss: 2.408013343811035\n",
            "p 600, Loss: 2.5286641120910645\n",
            "p 900, Loss: 1.8492134809494019\n",
            "p 0, Loss: 2.0479512214660645\n",
            "p 300, Loss: 2.5083730220794678\n",
            "p 600, Loss: 2.608159303665161\n",
            "p 900, Loss: 1.9884554147720337\n",
            "p 0, Loss: 1.9981811046600342\n",
            "p 300, Loss: 2.715944528579712\n",
            "p 600, Loss: 2.46235990524292\n",
            "p 900, Loss: 1.9117755889892578\n",
            "p 0, Loss: 2.0701169967651367\n",
            "p 300, Loss: 2.9179282188415527\n",
            "p 600, Loss: 2.5870704650878906\n",
            "p 900, Loss: 2.2506065368652344\n",
            "p 0, Loss: 2.1641993522644043\n",
            "p 300, Loss: 3.0013887882232666\n",
            "p 600, Loss: 2.4754600524902344\n",
            "p 900, Loss: 2.0543999671936035\n",
            "p 0, Loss: 2.0442984104156494\n",
            "p 300, Loss: 2.8189992904663086\n",
            "p 600, Loss: 2.5298004150390625\n",
            "p 900, Loss: 2.018686294555664\n",
            "p 0, Loss: 2.112333297729492\n",
            "p 300, Loss: 2.6418423652648926\n",
            "p 600, Loss: 2.805454730987549\n",
            "p 900, Loss: 2.0784547328948975\n",
            "p 0, Loss: 2.079956293106079\n",
            "p 300, Loss: 2.570574998855591\n",
            "p 600, Loss: 2.6431877613067627\n",
            "p 900, Loss: 1.9344226121902466\n",
            "p 0, Loss: 2.157618761062622\n",
            "p 300, Loss: 2.5754637718200684\n",
            "p 600, Loss: 2.637392282485962\n",
            "p 900, Loss: 2.03204607963562\n",
            "p 0, Loss: 2.200335741043091\n",
            "p 300, Loss: 2.5528221130371094\n",
            "p 600, Loss: 2.737440824508667\n",
            "p 900, Loss: 2.188382863998413\n",
            "p 0, Loss: 2.357591152191162\n",
            "p 300, Loss: 2.3943960666656494\n",
            "p 600, Loss: 2.6685314178466797\n",
            "p 900, Loss: 1.8907017707824707\n",
            "p 0, Loss: 2.1118884086608887\n",
            "p 300, Loss: 2.563218116760254\n",
            "p 600, Loss: 2.5872111320495605\n",
            "p 900, Loss: 1.9621995687484741\n",
            "p 0, Loss: 2.2934107780456543\n",
            "p 300, Loss: 2.9083714485168457\n",
            "p 600, Loss: 2.7354915142059326\n",
            "p 900, Loss: 1.919894814491272\n",
            "p 0, Loss: 2.082474708557129\n",
            "p 300, Loss: 2.74833083152771\n",
            "p 600, Loss: 2.669414758682251\n",
            "p 900, Loss: 2.0780792236328125\n",
            "p 0, Loss: 2.1969645023345947\n",
            "p 300, Loss: 2.5511934757232666\n",
            "p 600, Loss: 2.644679069519043\n",
            "p 900, Loss: 2.113550901412964\n",
            "p 0, Loss: 2.3974602222442627\n",
            "p 300, Loss: 2.613274335861206\n",
            "p 600, Loss: 2.6327927112579346\n",
            "p 900, Loss: 2.0052525997161865\n",
            "p 0, Loss: 2.1919493675231934\n",
            "p 300, Loss: 2.5654585361480713\n",
            "p 600, Loss: 2.7369813919067383\n",
            "p 900, Loss: 2.1193675994873047\n",
            "p 0, Loss: 2.4091358184814453\n",
            "p 300, Loss: 2.479283094406128\n",
            "p 600, Loss: 2.6942131519317627\n",
            "p 900, Loss: 2.0513062477111816\n",
            "p 0, Loss: 2.1211142539978027\n",
            "p 300, Loss: 2.49560284614563\n",
            "p 600, Loss: 2.5912256240844727\n",
            "p 900, Loss: 2.1779589653015137\n",
            "p 0, Loss: 2.477074384689331\n",
            "p 300, Loss: 2.3892335891723633\n",
            "p 600, Loss: 2.7043297290802\n",
            "p 900, Loss: 2.0472569465637207\n",
            "p 0, Loss: 2.2106258869171143\n",
            "p 300, Loss: 2.8073890209198\n",
            "p 600, Loss: 2.577847480773926\n",
            "p 900, Loss: 1.9119514226913452\n",
            "p 0, Loss: 2.517369508743286\n",
            "p 300, Loss: 2.6914451122283936\n",
            "p 600, Loss: 2.844304323196411\n",
            "p 900, Loss: 1.997818112373352\n",
            "p 0, Loss: 2.275202751159668\n",
            "p 300, Loss: 2.4945383071899414\n",
            "p 600, Loss: 2.772918939590454\n",
            "p 900, Loss: 2.071577548980713\n",
            "p 0, Loss: 2.462533473968506\n",
            "p 300, Loss: 2.4921114444732666\n",
            "p 600, Loss: 2.8596489429473877\n",
            "p 900, Loss: 1.8850406408309937\n",
            "p 0, Loss: 2.2515451908111572\n",
            "p 300, Loss: 2.384460210800171\n",
            "p 600, Loss: 2.57631254196167\n",
            "p 900, Loss: 1.9521421194076538\n",
            "p 0, Loss: 2.1350491046905518\n",
            "p 300, Loss: 2.481062889099121\n",
            "p 600, Loss: 2.557264804840088\n",
            "p 900, Loss: 1.903647780418396\n",
            "p 0, Loss: 2.1988797187805176\n",
            "p 300, Loss: 2.5111000537872314\n",
            "p 600, Loss: 2.5761053562164307\n",
            "p 900, Loss: 1.9355876445770264\n",
            "p 0, Loss: 2.522606134414673\n",
            "p 300, Loss: 2.506932020187378\n",
            "p 600, Loss: 2.5711510181427\n",
            "p 900, Loss: 2.0306241512298584\n",
            "p 0, Loss: 2.2108209133148193\n",
            "p 300, Loss: 2.6450438499450684\n",
            "p 600, Loss: 2.607163906097412\n",
            "p 900, Loss: 1.9187880754470825\n",
            "p 0, Loss: 2.2755205631256104\n",
            "p 300, Loss: 2.513075590133667\n",
            "p 600, Loss: 2.77897572517395\n",
            "p 900, Loss: 1.933819055557251\n",
            "p 0, Loss: 2.343200206756592\n",
            "p 300, Loss: 2.656925916671753\n",
            "p 600, Loss: 2.790703296661377\n",
            "p 900, Loss: 2.1847946643829346\n",
            "p 0, Loss: 2.2520458698272705\n",
            "p 300, Loss: 2.5842208862304688\n",
            "p 600, Loss: 2.8399758338928223\n",
            "p 900, Loss: 2.1154873371124268\n",
            "p 0, Loss: 2.7699217796325684\n",
            "p 300, Loss: 2.7271106243133545\n",
            "p 600, Loss: 2.892122983932495\n",
            "p 900, Loss: 2.1501708030700684\n",
            "p 0, Loss: 2.174852132797241\n",
            "p 300, Loss: 2.554194450378418\n",
            "p 600, Loss: 2.5864267349243164\n",
            "p 900, Loss: 1.9659525156021118\n",
            "p 0, Loss: 2.2996654510498047\n",
            "p 300, Loss: 2.571244716644287\n",
            "p 600, Loss: 2.833573818206787\n",
            "p 900, Loss: 2.209672212600708\n",
            "p 0, Loss: 2.2110955715179443\n",
            "p 300, Loss: 2.5020244121551514\n",
            "p 600, Loss: 2.913830280303955\n",
            "p 900, Loss: 2.1468636989593506\n",
            "p 0, Loss: 2.1277565956115723\n",
            "p 300, Loss: 2.714339017868042\n",
            "p 600, Loss: 2.8193814754486084\n",
            "p 900, Loss: 2.0712437629699707\n",
            "p 0, Loss: 2.254091501235962\n",
            "p 300, Loss: 2.7577219009399414\n",
            "p 600, Loss: 2.6024935245513916\n",
            "p 900, Loss: 2.0926296710968018\n",
            "p 0, Loss: 2.1874821186065674\n",
            "p 300, Loss: 3.0742592811584473\n",
            "p 600, Loss: 2.7260634899139404\n",
            "p 900, Loss: 1.991169810295105\n",
            "p 0, Loss: 2.311150550842285\n",
            "p 300, Loss: 2.753553628921509\n",
            "p 600, Loss: 2.902745246887207\n",
            "p 900, Loss: 2.045384407043457\n",
            "p 0, Loss: 2.4225616455078125\n",
            "p 300, Loss: 2.8494551181793213\n",
            "p 600, Loss: 3.016559362411499\n",
            "p 900, Loss: 2.2497918605804443\n",
            "p 0, Loss: 2.304011821746826\n",
            "p 300, Loss: 2.8024685382843018\n",
            "p 600, Loss: 2.8235926628112793\n",
            "p 900, Loss: 2.2256693840026855\n",
            "p 0, Loss: 2.2550089359283447\n",
            "p 300, Loss: 3.0617101192474365\n",
            "p 600, Loss: 2.9043917655944824\n",
            "p 900, Loss: 2.2520833015441895\n",
            "p 0, Loss: 2.418478012084961\n",
            "p 300, Loss: 2.8912193775177\n",
            "p 600, Loss: 2.6481878757476807\n",
            "p 900, Loss: 2.148994207382202\n",
            "p 0, Loss: 2.2712132930755615\n",
            "p 300, Loss: 2.826040744781494\n",
            "p 600, Loss: 2.785200357437134\n",
            "p 900, Loss: 2.1072371006011963\n",
            "p 0, Loss: 2.2237796783447266\n",
            "p 300, Loss: 2.7909319400787354\n",
            "p 600, Loss: 2.7539656162261963\n",
            "p 900, Loss: 2.058454751968384\n",
            "p 0, Loss: 2.2405846118927\n",
            "p 300, Loss: 2.6361587047576904\n",
            "p 600, Loss: 2.678560733795166\n",
            "p 900, Loss: 2.048539638519287\n",
            "p 0, Loss: 2.351379632949829\n",
            "p 300, Loss: 2.7180511951446533\n",
            "p 600, Loss: 2.6452114582061768\n",
            "p 900, Loss: 1.969946265220642\n",
            "p 0, Loss: 2.3179824352264404\n",
            "p 300, Loss: 2.8540501594543457\n",
            "p 600, Loss: 2.874640464782715\n",
            "p 900, Loss: 2.089325428009033\n",
            "p 0, Loss: 2.3725407123565674\n",
            "p 300, Loss: 2.846649169921875\n",
            "p 600, Loss: 2.797075033187866\n",
            "p 900, Loss: 2.247412919998169\n",
            "p 0, Loss: 2.4918501377105713\n",
            "p 300, Loss: 2.9574410915374756\n",
            "p 600, Loss: 2.827349901199341\n",
            "p 900, Loss: 2.1189746856689453\n",
            "p 0, Loss: 2.3370604515075684\n",
            "p 300, Loss: 2.892193555831909\n",
            "p 600, Loss: 2.678462505340576\n",
            "p 900, Loss: 2.090796709060669\n",
            "p 0, Loss: 2.2855124473571777\n",
            "p 300, Loss: 2.938431978225708\n",
            "p 600, Loss: 2.78650164604187\n",
            "p 900, Loss: 2.1322433948516846\n",
            "p 0, Loss: 2.163379669189453\n",
            "p 300, Loss: 2.705519437789917\n",
            "p 600, Loss: 2.6521053314208984\n",
            "p 900, Loss: 2.121192455291748\n",
            "p 0, Loss: 2.3057901859283447\n",
            "p 300, Loss: 2.5589957237243652\n",
            "p 600, Loss: 2.587587356567383\n",
            "p 900, Loss: 1.9580092430114746\n",
            "p 0, Loss: 2.185312271118164\n",
            "p 300, Loss: 2.548119306564331\n",
            "p 600, Loss: 2.622157573699951\n",
            "p 900, Loss: 1.904502272605896\n",
            "p 0, Loss: 2.1476948261260986\n",
            "p 300, Loss: 2.584083080291748\n",
            "p 600, Loss: 2.553287982940674\n",
            "p 900, Loss: 1.9827386140823364\n",
            "p 0, Loss: 2.118042469024658\n",
            "p 300, Loss: 2.4941084384918213\n",
            "p 600, Loss: 2.764012098312378\n",
            "p 900, Loss: 1.8616268634796143\n",
            "p 0, Loss: 2.2227437496185303\n",
            "p 300, Loss: 2.5794517993927\n",
            "p 600, Loss: 2.6125171184539795\n",
            "p 900, Loss: 1.9355136156082153\n",
            "p 0, Loss: 2.218421459197998\n",
            "p 300, Loss: 2.689042568206787\n",
            "p 600, Loss: 2.623218297958374\n",
            "p 900, Loss: 1.9749828577041626\n",
            "p 0, Loss: 2.1899852752685547\n",
            "p 300, Loss: 2.5980923175811768\n",
            "p 600, Loss: 2.581564426422119\n",
            "p 900, Loss: 1.9754050970077515\n",
            "p 0, Loss: 2.2220089435577393\n",
            "p 300, Loss: 2.938392162322998\n",
            "p 600, Loss: 2.669466495513916\n",
            "p 900, Loss: 1.9831516742706299\n",
            "p 0, Loss: 2.1709537506103516\n",
            "p 300, Loss: 2.63972544670105\n",
            "p 600, Loss: 2.675201892852783\n",
            "p 900, Loss: 1.8904362916946411\n",
            "p 0, Loss: 2.456265926361084\n",
            "p 300, Loss: 2.8270668983459473\n",
            "p 600, Loss: 2.6119577884674072\n",
            "p 900, Loss: 2.0229549407958984\n",
            "p 0, Loss: 2.31172251701355\n",
            "p 300, Loss: 2.790006399154663\n",
            "p 600, Loss: 2.6480112075805664\n",
            "p 900, Loss: 2.0325303077697754\n",
            "p 0, Loss: 2.4658517837524414\n",
            "p 300, Loss: 2.969658136367798\n",
            "p 600, Loss: 2.768641948699951\n",
            "p 900, Loss: 2.0719408988952637\n",
            "p 0, Loss: 2.2794783115386963\n",
            "p 300, Loss: 2.9581902027130127\n",
            "p 600, Loss: 2.6813583374023438\n",
            "p 900, Loss: 2.190052032470703\n",
            "p 0, Loss: 2.294353485107422\n",
            "p 300, Loss: 2.978536605834961\n",
            "p 600, Loss: 2.7403876781463623\n",
            "p 900, Loss: 2.265796661376953\n",
            "p 0, Loss: 2.291586399078369\n",
            "p 300, Loss: 3.017897367477417\n",
            "p 600, Loss: 2.7951247692108154\n",
            "p 900, Loss: 2.1630589962005615\n",
            "p 0, Loss: 2.2945637702941895\n",
            "p 300, Loss: 2.8166422843933105\n",
            "p 600, Loss: 2.503774881362915\n",
            "p 900, Loss: 2.189342975616455\n",
            "p 0, Loss: 2.40633225440979\n",
            "p 300, Loss: 2.7752010822296143\n",
            "p 600, Loss: 2.788095235824585\n",
            "p 900, Loss: 2.1652469635009766\n",
            "p 0, Loss: 2.272327423095703\n",
            "p 300, Loss: 2.701078414916992\n",
            "p 600, Loss: 2.7803800106048584\n",
            "p 900, Loss: 2.0851218700408936\n",
            "p 0, Loss: 2.269747734069824\n",
            "p 300, Loss: 2.8795011043548584\n",
            "p 600, Loss: 2.745864152908325\n",
            "p 900, Loss: 2.237900972366333\n",
            "p 0, Loss: 2.625831127166748\n",
            "p 300, Loss: 2.6190266609191895\n",
            "p 600, Loss: 2.8518893718719482\n",
            "p 900, Loss: 2.1493992805480957\n",
            "p 0, Loss: 2.376124858856201\n",
            "p 300, Loss: 2.8511641025543213\n",
            "p 600, Loss: 2.7734689712524414\n",
            "p 900, Loss: 2.176138401031494\n",
            "p 0, Loss: 2.464431047439575\n",
            "p 300, Loss: 2.7793703079223633\n",
            "p 600, Loss: 3.023446559906006\n",
            "p 900, Loss: 2.0899555683135986\n",
            "p 0, Loss: 2.2483527660369873\n",
            "p 300, Loss: 3.0011868476867676\n",
            "p 600, Loss: 3.006028890609741\n",
            "p 900, Loss: 2.027787208557129\n",
            "p 0, Loss: 2.253002643585205\n",
            "p 300, Loss: 2.838959217071533\n",
            "p 600, Loss: 2.8438665866851807\n",
            "p 900, Loss: 2.072449207305908\n",
            "p 0, Loss: 2.2909488677978516\n",
            "p 300, Loss: 2.6577391624450684\n",
            "p 600, Loss: 2.831355571746826\n",
            "p 900, Loss: 2.0170319080352783\n",
            "p 0, Loss: 2.280247211456299\n",
            "p 300, Loss: 2.6792733669281006\n",
            "p 600, Loss: 2.8825790882110596\n",
            "p 900, Loss: 1.95734441280365\n",
            "p 0, Loss: 2.4290709495544434\n",
            "p 300, Loss: 2.702239513397217\n",
            "p 600, Loss: 2.761665105819702\n",
            "p 900, Loss: 2.019774913787842\n",
            "p 0, Loss: 2.317227602005005\n",
            "p 300, Loss: 2.896981954574585\n",
            "p 600, Loss: 2.7791950702667236\n",
            "p 900, Loss: 2.0543792247772217\n",
            "p 0, Loss: 2.4084253311157227\n",
            "p 300, Loss: 2.7895445823669434\n",
            "p 600, Loss: 2.739036798477173\n",
            "p 900, Loss: 2.0596261024475098\n",
            "p 0, Loss: 2.2906951904296875\n",
            "p 300, Loss: 2.5876998901367188\n",
            "p 600, Loss: 2.6915905475616455\n",
            "p 900, Loss: 1.9865175485610962\n",
            "p 0, Loss: 2.142744779586792\n",
            "p 300, Loss: 2.755605936050415\n",
            "p 600, Loss: 2.665581226348877\n",
            "p 900, Loss: 1.881806492805481\n",
            "p 0, Loss: 2.396552801132202\n",
            "p 300, Loss: 2.6491363048553467\n",
            "p 600, Loss: 2.7227823734283447\n",
            "p 900, Loss: 1.994193434715271\n",
            "p 0, Loss: 2.2657721042633057\n",
            "p 300, Loss: 2.7233898639678955\n",
            "p 600, Loss: 2.7625925540924072\n",
            "p 900, Loss: 1.9756510257720947\n",
            "p 0, Loss: 2.3194448947906494\n",
            "p 300, Loss: 2.8237297534942627\n",
            "p 600, Loss: 2.813225030899048\n",
            "p 900, Loss: 1.9542670249938965\n",
            "p 0, Loss: 2.211881160736084\n",
            "p 300, Loss: 2.78922963142395\n",
            "p 600, Loss: 2.59720778465271\n",
            "p 900, Loss: 1.980579137802124\n",
            "p 0, Loss: 2.1704437732696533\n",
            "p 300, Loss: 2.6311423778533936\n",
            "p 600, Loss: 2.8400166034698486\n",
            "p 900, Loss: 1.8799912929534912\n",
            "p 0, Loss: 2.241486072540283\n",
            "p 300, Loss: 2.7918736934661865\n",
            "p 600, Loss: 2.6300106048583984\n",
            "p 900, Loss: 1.9652029275894165\n",
            "p 0, Loss: 2.3128721714019775\n",
            "p 300, Loss: 2.9384071826934814\n",
            "p 600, Loss: 2.9112331867218018\n",
            "p 900, Loss: 1.987755298614502\n",
            "p 0, Loss: 2.318246841430664\n",
            "p 300, Loss: 2.8774683475494385\n",
            "p 600, Loss: 2.7393782138824463\n",
            "p 900, Loss: 1.993351697921753\n",
            "p 0, Loss: 2.278446912765503\n",
            "p 300, Loss: 2.9886858463287354\n",
            "p 600, Loss: 2.8605830669403076\n",
            "p 900, Loss: 2.004763603210449\n",
            "p 0, Loss: 2.154632806777954\n",
            "p 300, Loss: 2.8930375576019287\n",
            "p 600, Loss: 2.8568222522735596\n",
            "p 900, Loss: 1.98149836063385\n",
            "p 0, Loss: 2.2155086994171143\n",
            "p 300, Loss: 2.7243525981903076\n",
            "p 600, Loss: 2.9019038677215576\n",
            "p 900, Loss: 1.791826844215393\n",
            "p 0, Loss: 2.1598269939422607\n",
            "p 300, Loss: 2.768563747406006\n",
            "p 600, Loss: 2.916090726852417\n",
            "p 900, Loss: 1.9244531393051147\n",
            "p 0, Loss: 2.2752816677093506\n",
            "p 300, Loss: 2.750739812850952\n",
            "p 600, Loss: 2.8857789039611816\n",
            "p 900, Loss: 1.8112927675247192\n",
            "p 0, Loss: 2.207939863204956\n",
            "p 300, Loss: 2.8770296573638916\n",
            "p 600, Loss: 2.6849234104156494\n",
            "p 900, Loss: 2.013819932937622\n",
            "p 0, Loss: 2.3590087890625\n",
            "p 300, Loss: 3.187258243560791\n",
            "p 600, Loss: 2.938136577606201\n",
            "p 900, Loss: 1.9578688144683838\n",
            "p 0, Loss: 2.5476200580596924\n",
            "p 300, Loss: 3.232304573059082\n",
            "p 600, Loss: 2.7927634716033936\n",
            "p 900, Loss: 2.5863263607025146\n",
            "p 0, Loss: 2.571486234664917\n",
            "p 300, Loss: 3.2429378032684326\n",
            "p 600, Loss: 2.873728036880493\n",
            "p 900, Loss: 2.2065529823303223\n",
            "p 0, Loss: 2.432302474975586\n",
            "p 300, Loss: 3.1658153533935547\n",
            "p 600, Loss: 2.770751953125\n",
            "p 900, Loss: 2.105498790740967\n",
            "p 0, Loss: 2.319866418838501\n",
            "p 300, Loss: 3.1845099925994873\n",
            "p 600, Loss: 2.772092342376709\n",
            "p 900, Loss: 2.149534225463867\n",
            "p 0, Loss: 2.320634126663208\n",
            "p 300, Loss: 3.0657310485839844\n",
            "p 600, Loss: 2.6362686157226562\n",
            "p 900, Loss: 1.9138858318328857\n",
            "p 0, Loss: 2.316103935241699\n",
            "p 300, Loss: 2.9282994270324707\n",
            "p 600, Loss: 2.5477733612060547\n",
            "p 900, Loss: 1.992263674736023\n",
            "p 0, Loss: 2.4171547889709473\n",
            "p 300, Loss: 3.1856348514556885\n",
            "p 600, Loss: 2.7261903285980225\n",
            "p 900, Loss: 1.9031503200531006\n",
            "p 0, Loss: 2.286794662475586\n",
            "p 300, Loss: 3.0604279041290283\n",
            "p 600, Loss: 2.8140742778778076\n",
            "p 900, Loss: 1.914567470550537\n",
            "p 0, Loss: 2.1818253993988037\n",
            "p 300, Loss: 2.88637375831604\n",
            "p 600, Loss: 2.7122716903686523\n",
            "p 900, Loss: 1.922774076461792\n",
            "p 0, Loss: 2.2732760906219482\n",
            "p 300, Loss: 2.80788254737854\n",
            "p 600, Loss: 2.486600399017334\n",
            "p 900, Loss: 1.9131819009780884\n",
            "p 0, Loss: 2.103527069091797\n",
            "p 300, Loss: 2.9885449409484863\n",
            "p 600, Loss: 2.6032416820526123\n",
            "p 900, Loss: 1.8686164617538452\n",
            "p 0, Loss: 2.176398515701294\n",
            "p 300, Loss: 2.8193857669830322\n",
            "p 600, Loss: 2.7122273445129395\n",
            "p 900, Loss: 1.8991106748580933\n",
            "p 0, Loss: 2.105778217315674\n",
            "p 300, Loss: 2.802022933959961\n",
            "p 600, Loss: 2.427611827850342\n",
            "p 900, Loss: 1.9883482456207275\n",
            "p 0, Loss: 2.290006399154663\n",
            "p 300, Loss: 2.987380027770996\n",
            "p 600, Loss: 2.587857961654663\n",
            "p 900, Loss: 2.0167741775512695\n",
            "p 0, Loss: 2.191406011581421\n",
            "p 300, Loss: 2.890958309173584\n",
            "p 600, Loss: 2.5054314136505127\n",
            "p 900, Loss: 1.9361387491226196\n",
            "p 0, Loss: 2.2689638137817383\n",
            "p 300, Loss: 2.974226713180542\n",
            "p 600, Loss: 2.7657618522644043\n",
            "p 900, Loss: 2.0024900436401367\n",
            "p 0, Loss: 2.1474177837371826\n",
            "p 300, Loss: 2.944094657897949\n",
            "p 600, Loss: 2.7836718559265137\n",
            "p 900, Loss: 1.9102245569229126\n",
            "p 0, Loss: 2.0923452377319336\n",
            "p 300, Loss: 2.9840478897094727\n",
            "p 600, Loss: 2.6824352741241455\n",
            "p 900, Loss: 1.9903212785720825\n",
            "p 0, Loss: 2.3014087677001953\n",
            "p 300, Loss: 2.9429831504821777\n",
            "p 600, Loss: 2.6491200923919678\n",
            "p 900, Loss: 1.939471960067749\n",
            "p 0, Loss: 2.0662872791290283\n",
            "p 300, Loss: 2.9204952716827393\n",
            "p 600, Loss: 2.891415596008301\n",
            "p 900, Loss: 1.876359462738037\n",
            "p 0, Loss: 2.19311785697937\n",
            "p 300, Loss: 2.9182074069976807\n",
            "p 600, Loss: 2.822559118270874\n",
            "p 900, Loss: 2.1469709873199463\n",
            "p 0, Loss: 2.1640524864196777\n",
            "p 300, Loss: 2.8417108058929443\n",
            "p 600, Loss: 2.861560344696045\n",
            "p 900, Loss: 1.9407544136047363\n",
            "p 0, Loss: 2.442531108856201\n",
            "p 300, Loss: 3.1000382900238037\n",
            "p 600, Loss: 2.764573335647583\n",
            "p 900, Loss: 2.043858528137207\n",
            "p 0, Loss: 2.4251134395599365\n",
            "p 300, Loss: 2.972561836242676\n",
            "p 600, Loss: 2.7420268058776855\n",
            "p 900, Loss: 1.988832712173462\n",
            "p 0, Loss: 2.3424079418182373\n",
            "p 300, Loss: 3.0216727256774902\n",
            "p 600, Loss: 2.8303966522216797\n",
            "p 900, Loss: 2.0184969902038574\n",
            "p 0, Loss: 2.648928642272949\n",
            "p 300, Loss: 3.219895124435425\n",
            "p 600, Loss: 2.6108005046844482\n",
            "p 900, Loss: 1.946982979774475\n",
            "p 0, Loss: 2.4371497631073\n",
            "p 300, Loss: 2.881662607192993\n",
            "p 600, Loss: 2.6767842769622803\n",
            "p 900, Loss: 1.9552103281021118\n",
            "p 0, Loss: 2.234926700592041\n",
            "p 300, Loss: 2.729480743408203\n",
            "p 600, Loss: 2.6397037506103516\n",
            "p 900, Loss: 1.847804307937622\n",
            "p 0, Loss: 2.2799038887023926\n",
            "p 300, Loss: 3.020089864730835\n",
            "p 600, Loss: 2.6915981769561768\n",
            "p 900, Loss: 1.785982370376587\n",
            "p 0, Loss: 2.261838436126709\n",
            "p 300, Loss: 2.947234869003296\n",
            "p 600, Loss: 2.6567258834838867\n",
            "p 900, Loss: 1.9380768537521362\n",
            "p 0, Loss: 2.185908079147339\n",
            "p 300, Loss: 3.0097286701202393\n",
            "p 600, Loss: 2.57183575630188\n",
            "p 900, Loss: 1.853898525238037\n",
            "p 0, Loss: 2.17448091506958\n",
            "p 300, Loss: 2.820845365524292\n",
            "p 600, Loss: 2.6180026531219482\n",
            "p 900, Loss: 1.8977643251419067\n",
            "p 0, Loss: 2.3029308319091797\n",
            "p 300, Loss: 2.871546745300293\n",
            "p 600, Loss: 2.7205073833465576\n",
            "p 900, Loss: 2.1345608234405518\n",
            "p 0, Loss: 2.441190242767334\n",
            "p 300, Loss: 2.8281259536743164\n",
            "p 600, Loss: 2.725745439529419\n",
            "p 900, Loss: 2.1574654579162598\n",
            "p 0, Loss: 2.381659984588623\n",
            "p 300, Loss: 2.7565600872039795\n",
            "p 600, Loss: 2.7827017307281494\n",
            "p 900, Loss: 2.7063121795654297\n",
            "p 0, Loss: 2.3222360610961914\n",
            "p 300, Loss: 2.732243299484253\n",
            "p 600, Loss: 2.803417682647705\n",
            "p 900, Loss: 2.205129861831665\n",
            "p 0, Loss: 2.41607666015625\n",
            "p 300, Loss: 2.837805986404419\n",
            "p 600, Loss: 2.613988161087036\n",
            "p 900, Loss: 1.972510814666748\n",
            "p 0, Loss: 2.4424028396606445\n",
            "p 300, Loss: 2.9843642711639404\n",
            "p 600, Loss: 2.4997217655181885\n",
            "p 900, Loss: 1.9930437803268433\n",
            "p 0, Loss: 2.2868831157684326\n",
            "p 300, Loss: 2.8965933322906494\n",
            "p 600, Loss: 2.6224870681762695\n",
            "p 900, Loss: 1.9162592887878418\n",
            "p 0, Loss: 2.3158180713653564\n",
            "p 300, Loss: 3.115957021713257\n",
            "p 600, Loss: 2.7013332843780518\n",
            "p 900, Loss: 1.9527102708816528\n",
            "p 0, Loss: 2.307739496231079\n",
            "p 300, Loss: 3.1316349506378174\n",
            "p 600, Loss: 2.7391624450683594\n",
            "p 900, Loss: 1.8841131925582886\n",
            "p 0, Loss: 2.1974966526031494\n",
            "p 300, Loss: 2.9412293434143066\n",
            "p 600, Loss: 2.4908032417297363\n",
            "p 900, Loss: 1.854722261428833\n",
            "p 0, Loss: 2.3224146366119385\n",
            "p 300, Loss: 2.965744733810425\n",
            "p 600, Loss: 2.483706474304199\n",
            "p 900, Loss: 1.942458152770996\n",
            "p 0, Loss: 2.2392373085021973\n",
            "p 300, Loss: 2.889294385910034\n",
            "p 600, Loss: 2.7122304439544678\n",
            "p 900, Loss: 2.0433220863342285\n",
            "p 0, Loss: 2.283945083618164\n",
            "p 300, Loss: 2.9492430686950684\n",
            "p 600, Loss: 2.57000994682312\n",
            "p 900, Loss: 2.0588088035583496\n",
            "p 0, Loss: 2.539827823638916\n",
            "p 300, Loss: 2.9302027225494385\n",
            "p 600, Loss: 2.7163758277893066\n",
            "p 900, Loss: 1.9457509517669678\n",
            "p 0, Loss: 2.2561681270599365\n",
            "p 300, Loss: 3.013502359390259\n",
            "p 600, Loss: 2.442415475845337\n",
            "p 900, Loss: 1.917883276939392\n",
            "p 0, Loss: 2.4345736503601074\n",
            "p 300, Loss: 2.7720377445220947\n",
            "p 600, Loss: 2.6008095741271973\n",
            "p 900, Loss: 1.914863109588623\n",
            "p 0, Loss: 2.0949079990386963\n",
            "p 300, Loss: 3.078139305114746\n",
            "p 600, Loss: 2.6780529022216797\n",
            "p 900, Loss: 2.1296629905700684\n",
            "p 0, Loss: 2.446873664855957\n",
            "p 300, Loss: 2.9300460815429688\n",
            "p 600, Loss: 2.7587368488311768\n",
            "p 900, Loss: 2.2411534786224365\n",
            "p 0, Loss: 2.5289957523345947\n",
            "p 300, Loss: 2.961493730545044\n",
            "p 600, Loss: 2.702530860900879\n",
            "p 900, Loss: 2.3098678588867188\n",
            "p 0, Loss: 2.299712657928467\n",
            "p 300, Loss: 3.002748727798462\n",
            "p 600, Loss: 2.644148349761963\n",
            "p 900, Loss: 2.3630354404449463\n",
            "p 0, Loss: 2.4466419219970703\n",
            "p 300, Loss: 3.0209262371063232\n",
            "p 600, Loss: 2.443263053894043\n",
            "p 900, Loss: 1.969180941581726\n",
            "p 0, Loss: 2.527770757675171\n",
            "p 300, Loss: 3.056673049926758\n",
            "p 600, Loss: 2.5970468521118164\n",
            "p 900, Loss: 2.1373021602630615\n",
            "p 0, Loss: 2.4738543033599854\n",
            "p 300, Loss: 3.0485856533050537\n",
            "p 600, Loss: 2.6121559143066406\n",
            "p 900, Loss: 2.1173689365386963\n",
            "p 0, Loss: 2.5073082447052\n",
            "p 300, Loss: 3.249136447906494\n",
            "p 600, Loss: 2.7428267002105713\n",
            "p 900, Loss: 2.146651029586792\n",
            "p 0, Loss: 2.3780670166015625\n",
            "p 300, Loss: 3.195127010345459\n",
            "p 600, Loss: 2.652937889099121\n",
            "p 900, Loss: 2.1322412490844727\n",
            "p 0, Loss: 2.6636531352996826\n",
            "p 300, Loss: 3.361380100250244\n",
            "p 600, Loss: 2.626523017883301\n",
            "p 900, Loss: 2.0953361988067627\n",
            "p 0, Loss: 2.644505739212036\n",
            "p 300, Loss: 3.0551540851593018\n",
            "p 600, Loss: 2.715762138366699\n",
            "p 900, Loss: 2.1863620281219482\n",
            "p 0, Loss: 2.421783685684204\n",
            "p 300, Loss: 3.2201087474823\n",
            "p 600, Loss: 2.747176170349121\n",
            "p 900, Loss: 2.0314881801605225\n",
            "p 0, Loss: 2.6290881633758545\n",
            "p 300, Loss: 3.2657830715179443\n",
            "p 600, Loss: 2.627700090408325\n",
            "p 900, Loss: 2.143479108810425\n",
            "p 0, Loss: 2.4509670734405518\n",
            "p 300, Loss: 3.2522659301757812\n",
            "p 600, Loss: 2.748016595840454\n",
            "p 900, Loss: 2.1402435302734375\n",
            "p 0, Loss: 2.4805874824523926\n",
            "p 300, Loss: 3.020512342453003\n",
            "p 600, Loss: 2.7781927585601807\n",
            "p 900, Loss: 2.0781819820404053\n",
            "p 0, Loss: 2.5101535320281982\n",
            "p 300, Loss: 3.2165725231170654\n",
            "p 600, Loss: 2.771918535232544\n",
            "p 900, Loss: 2.0897107124328613\n",
            "p 0, Loss: 2.438230514526367\n",
            "p 300, Loss: 2.978701591491699\n",
            "p 600, Loss: 2.9808712005615234\n",
            "p 900, Loss: 2.203087568283081\n",
            "p 0, Loss: 2.5259580612182617\n",
            "p 300, Loss: 2.962801456451416\n",
            "p 600, Loss: 2.7049667835235596\n",
            "p 900, Loss: 2.194749355316162\n",
            "p 0, Loss: 2.474125862121582\n",
            "p 300, Loss: 3.0602493286132812\n",
            "p 600, Loss: 2.7062196731567383\n",
            "p 900, Loss: 2.043048143386841\n",
            "p 0, Loss: 2.4549026489257812\n",
            "p 300, Loss: 2.899686336517334\n",
            "p 600, Loss: 2.619954824447632\n",
            "p 900, Loss: 2.088566780090332\n",
            "p 0, Loss: 2.4351491928100586\n",
            "p 300, Loss: 2.6386260986328125\n",
            "p 600, Loss: 2.723393201828003\n",
            "p 900, Loss: 2.0355398654937744\n",
            "p 0, Loss: 2.3208999633789062\n",
            "p 300, Loss: 2.7867929935455322\n",
            "p 600, Loss: 2.8443639278411865\n",
            "p 900, Loss: 2.36531138420105\n",
            "p 0, Loss: 2.408759355545044\n",
            "p 300, Loss: 2.853266954421997\n",
            "p 600, Loss: 2.9663689136505127\n",
            "p 900, Loss: 2.2125566005706787\n",
            "p 0, Loss: 2.236096143722534\n",
            "p 300, Loss: 2.712369203567505\n",
            "p 600, Loss: 2.9211232662200928\n",
            "p 900, Loss: 1.9619126319885254\n",
            "p 0, Loss: 2.2791523933410645\n",
            "p 300, Loss: 2.8728420734405518\n",
            "p 600, Loss: 2.7555441856384277\n",
            "p 900, Loss: 2.0664775371551514\n",
            "p 0, Loss: 2.1511707305908203\n",
            "p 300, Loss: 2.925686836242676\n",
            "p 600, Loss: 2.913703680038452\n",
            "p 900, Loss: 1.9651594161987305\n",
            "p 0, Loss: 2.29699444770813\n",
            "p 300, Loss: 2.7617602348327637\n",
            "p 600, Loss: 2.9377524852752686\n",
            "p 900, Loss: 1.966157078742981\n",
            "p 0, Loss: 2.117914915084839\n",
            "p 300, Loss: 2.904505729675293\n",
            "p 600, Loss: 2.795191764831543\n",
            "p 900, Loss: 2.1152358055114746\n",
            "p 0, Loss: 2.290210247039795\n",
            "p 300, Loss: 2.8106961250305176\n",
            "p 600, Loss: 2.881988525390625\n",
            "p 900, Loss: 2.0007007122039795\n",
            "p 0, Loss: 2.1652002334594727\n",
            "p 300, Loss: 2.7402548789978027\n",
            "p 600, Loss: 2.8139021396636963\n",
            "p 900, Loss: 2.0676112174987793\n",
            "p 0, Loss: 2.298168182373047\n",
            "p 300, Loss: 2.71171498298645\n",
            "p 600, Loss: 2.6538212299346924\n",
            "p 900, Loss: 2.092491626739502\n",
            "p 0, Loss: 2.212251901626587\n",
            "p 300, Loss: 2.7969043254852295\n",
            "p 600, Loss: 2.7984817028045654\n",
            "p 900, Loss: 2.1453893184661865\n",
            "p 0, Loss: 2.198099374771118\n",
            "p 300, Loss: 2.7993316650390625\n",
            "p 600, Loss: 2.8331832885742188\n",
            "p 900, Loss: 2.198730230331421\n",
            "p 0, Loss: 2.1825039386749268\n",
            "p 300, Loss: 2.6928722858428955\n",
            "p 600, Loss: 2.8301644325256348\n",
            "p 900, Loss: 2.3690338134765625\n",
            "p 0, Loss: 2.2443645000457764\n",
            "p 300, Loss: 2.7147984504699707\n",
            "p 600, Loss: 2.794532299041748\n",
            "p 900, Loss: 2.4482946395874023\n",
            "p 0, Loss: 2.2839391231536865\n",
            "p 300, Loss: 2.666501760482788\n",
            "p 600, Loss: 2.8149607181549072\n",
            "p 900, Loss: 2.3497350215911865\n",
            "p 0, Loss: 2.120670795440674\n",
            "p 300, Loss: 2.680137872695923\n",
            "p 600, Loss: 2.8521413803100586\n",
            "p 900, Loss: 2.1108438968658447\n",
            "p 0, Loss: 2.169226884841919\n",
            "p 300, Loss: 2.678191900253296\n",
            "p 600, Loss: 2.5918498039245605\n",
            "p 900, Loss: 2.1431758403778076\n",
            "p 0, Loss: 2.2957797050476074\n",
            "p 300, Loss: 2.7019288539886475\n",
            "p 600, Loss: 2.8936948776245117\n",
            "p 900, Loss: 2.212610960006714\n",
            "p 0, Loss: 2.3116347789764404\n",
            "p 300, Loss: 2.9030845165252686\n",
            "p 600, Loss: 3.0472986698150635\n",
            "p 900, Loss: 2.2764570713043213\n",
            "p 0, Loss: 2.4171059131622314\n",
            "p 300, Loss: 2.6489627361297607\n",
            "p 600, Loss: 2.783389091491699\n",
            "p 900, Loss: 2.181335210800171\n",
            "p 0, Loss: 2.140997886657715\n",
            "p 300, Loss: 2.7004432678222656\n",
            "p 600, Loss: 2.84281325340271\n",
            "p 900, Loss: 2.270305633544922\n",
            "p 0, Loss: 2.2984955310821533\n",
            "p 300, Loss: 2.6900503635406494\n",
            "p 600, Loss: 2.79585599899292\n",
            "p 900, Loss: 2.2409515380859375\n",
            "p 0, Loss: 2.372339963912964\n",
            "p 300, Loss: 2.729541540145874\n",
            "p 600, Loss: 2.7127649784088135\n",
            "p 900, Loss: 2.219935894012451\n",
            "p 0, Loss: 2.290947437286377\n",
            "p 300, Loss: 2.7506606578826904\n",
            "p 600, Loss: 2.6680736541748047\n",
            "p 900, Loss: 2.4398093223571777\n",
            "p 0, Loss: 2.3852028846740723\n",
            "p 300, Loss: 2.9004130363464355\n",
            "p 600, Loss: 2.6763134002685547\n",
            "p 900, Loss: 2.312380313873291\n",
            "p 0, Loss: 2.3262524604797363\n",
            "p 300, Loss: 2.8521900177001953\n",
            "p 600, Loss: 2.636474847793579\n",
            "p 900, Loss: 2.308887481689453\n",
            "p 0, Loss: 2.447429656982422\n",
            "p 300, Loss: 2.943119764328003\n",
            "p 600, Loss: 2.6116719245910645\n",
            "p 900, Loss: 2.3655238151550293\n",
            "p 0, Loss: 2.7213101387023926\n",
            "p 300, Loss: 2.744969129562378\n",
            "p 600, Loss: 2.6922411918640137\n",
            "p 900, Loss: 2.165811777114868\n",
            "p 0, Loss: 2.255471706390381\n",
            "p 300, Loss: 2.8044111728668213\n",
            "p 600, Loss: 2.7610585689544678\n",
            "p 900, Loss: 2.102393865585327\n",
            "p 0, Loss: 2.2360353469848633\n",
            "p 300, Loss: 2.72023344039917\n",
            "p 600, Loss: 2.682161808013916\n",
            "p 900, Loss: 2.0771377086639404\n",
            "p 0, Loss: 2.3630712032318115\n",
            "p 300, Loss: 2.790029764175415\n",
            "p 600, Loss: 2.7501564025878906\n",
            "p 900, Loss: 1.9833661317825317\n",
            "p 0, Loss: 2.275461435317993\n",
            "p 300, Loss: 2.721473217010498\n",
            "p 600, Loss: 2.6528265476226807\n",
            "p 900, Loss: 1.9913513660430908\n",
            "p 0, Loss: 2.3905086517333984\n",
            "p 300, Loss: 2.7536251544952393\n",
            "p 600, Loss: 2.6059858798980713\n",
            "p 900, Loss: 2.0135250091552734\n",
            "p 0, Loss: 2.250365972518921\n",
            "p 300, Loss: 2.9241185188293457\n",
            "p 600, Loss: 2.6890451908111572\n",
            "p 900, Loss: 1.960848331451416\n",
            "p 0, Loss: 2.1221516132354736\n",
            "p 300, Loss: 2.811748743057251\n",
            "p 600, Loss: 2.627516508102417\n",
            "p 900, Loss: 1.9874659776687622\n",
            "p 0, Loss: 2.4048149585723877\n",
            "p 300, Loss: 2.704796552658081\n",
            "p 600, Loss: 2.757612466812134\n",
            "p 900, Loss: 2.191514730453491\n",
            "p 0, Loss: 2.1972484588623047\n",
            "p 300, Loss: 2.820329189300537\n",
            "p 600, Loss: 2.790480852127075\n",
            "p 900, Loss: 2.0086851119995117\n",
            "p 0, Loss: 2.1897106170654297\n",
            "p 300, Loss: 2.659290313720703\n",
            "p 600, Loss: 2.743255853652954\n",
            "p 900, Loss: 1.9190136194229126\n",
            "p 0, Loss: 2.152052879333496\n",
            "p 300, Loss: 2.587604522705078\n",
            "p 600, Loss: 2.6897802352905273\n",
            "p 900, Loss: 2.1503829956054688\n",
            "p 0, Loss: 2.214470148086548\n",
            "p 300, Loss: 2.7591843605041504\n",
            "p 600, Loss: 2.996218204498291\n",
            "p 900, Loss: 2.05006742477417\n",
            "p 0, Loss: 2.0373525619506836\n",
            "p 300, Loss: 2.5777876377105713\n",
            "p 600, Loss: 2.4774861335754395\n",
            "p 900, Loss: 1.9501221179962158\n",
            "p 0, Loss: 2.2165937423706055\n",
            "p 300, Loss: 2.7145700454711914\n",
            "p 600, Loss: 2.740117073059082\n",
            "p 900, Loss: 2.055112361907959\n",
            "p 0, Loss: 2.158033847808838\n",
            "p 300, Loss: 2.657601833343506\n",
            "p 600, Loss: 2.584386110305786\n",
            "p 900, Loss: 1.9451398849487305\n",
            "p 0, Loss: 2.030996561050415\n",
            "p 300, Loss: 2.8626949787139893\n",
            "p 600, Loss: 2.867473840713501\n",
            "p 900, Loss: 2.0604965686798096\n",
            "p 0, Loss: 2.035456657409668\n",
            "p 300, Loss: 2.7765953540802\n",
            "p 600, Loss: 2.8885858058929443\n",
            "p 900, Loss: 1.9579174518585205\n",
            "p 0, Loss: 2.1015100479125977\n",
            "p 300, Loss: 2.838589668273926\n",
            "p 600, Loss: 2.8007795810699463\n",
            "p 900, Loss: 1.8913284540176392\n"
          ]
        }
      ],
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 300\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(0,len(en_data) - batch_size - 1,batch_size):\n",
        "\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(source_batch, batch_size)\n",
        "\n",
        "    # remove the padding tokens when calculate the loss\n",
        "    # Create a mask to ignore padding tokens\n",
        "    # padding_mask = (target_batch != padding_token_index).float()\n",
        "    output = output.reshape(-1, target_vocab_size)\n",
        "\n",
        "    # Compute the loss with the padding mask\n",
        "    loss = criterion(output, target_batch.view(-1))\n",
        "    # loss = (loss * padding_mask.view(-1)).sum() / padding_mask.sum()\n",
        "\n",
        "    # for i in range(target_batch.shape[0]):\n",
        "    #   line = []\n",
        "    #   for j in range(len(target_batch[i])):\n",
        "    #     line.append(target_ix_to_char[target_batch[i][j].item()])\n",
        "    #   print(''.join(line))\n",
        "\n",
        "    # print(\"---\")\n",
        "    # probability_np = output.detach().numpy()\n",
        "    # Find the index of the maximum probability along the last dimension\n",
        "    # max_index_np = np.argmax(probability_np, axis=-1)\n",
        "    # Convert the resulting NumPy array back to a PyTorch tensor\n",
        "    # output = torch.tensor(max_index_np).reshape(target_batch.shape)\n",
        "    # for i in range(output.shape[0]):\n",
        "    #   line = []\n",
        "    #   for j in range(len(output[i])):\n",
        "    #     line.append(target_ix_to_char[output[i][j].item()])\n",
        "    #   print(''.join(line))\n",
        "    # print(loss.item())\n",
        "    # print(target_batch.shape)\n",
        "    # print(output.shape)\n",
        "    # break\n",
        "\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    if p%300 == 0:\n",
        "      # Print or log the training loss for each epoch\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n",
        "    # break\n",
        "  # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVVtitSdL6t",
        "outputId": "0c56bde0-a645-46f3-912b-e4e40d7259d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Je'JÊeJ\n",
            "<JeeJMes\n",
            "<JeeJMe \n"
          ]
        }
      ],
      "source": [
        "test_line = \"I like art.>\"\n",
        "\n",
        "input = line_to_tensor(test_line)\n",
        "\n",
        "\n",
        "outputs = model.translate(input)\n",
        "for tensor,p in outputs:\n",
        "  result = [target_ix_to_char[j.item()] for j in tensor]\n",
        "  print(''.join(result))\n",
        "\n",
        "# outputs = model(input,1)\n",
        "# result = []\n",
        "# for i in range(outputs.shape[0]):\n",
        "\n",
        "#   p = nn.functional.softmax(outputs[i], dim=-1).detach().numpy().ravel()\n",
        "#   ix = np.random.choice(range(target_vocab_size), p=p)\n",
        "\n",
        "#   result.append(target_ix_to_char[ix])\n",
        "\n",
        "# print(''.join(result))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJVXgOsNVeWF",
        "outputId": "11231eab-354f-4196-8563-31bec8cb79db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I guess so.>',\n",
              " 'I guess so.>',\n",
              " 'I had help.>',\n",
              " 'I hate you.>',\n",
              " 'I hate you.>',\n",
              " 'I have one.>',\n",
              " 'I have one.>',\n",
              " 'I have won.>',\n",
              " 'I have won.>',\n",
              " 'I help him.>',\n",
              " 'I hope not.>',\n",
              " 'I hope not.>',\n",
              " 'I know CPR.>',\n",
              " 'I know her.>',\n",
              " 'I know him.>',\n",
              " 'I like art.>',\n",
              " 'I like him.>',\n",
              " 'I like him.>',\n",
              " 'I like tea.>',\n",
              " 'I like you.>',\n",
              " 'I like you.>',\n",
              " 'I like you.>',\n",
              " 'I liked it.>',\n",
              " 'I liked it.>',\n",
              " 'I love Tom.>',\n",
              " 'I love tea.>',\n",
              " 'I love you.>',\n",
              " 'I love you.>',\n",
              " 'I loved it.>',\n",
              " 'I made tea.>',\n",
              " 'I made two.>',\n",
              " 'I made two.>',\n",
              " 'I met them.>',\n",
              " 'I met them.>',\n",
              " 'I met them.>',\n",
              " 'I must run.>',\n",
              " 'I must run.>',\n",
              " 'I need air.>',\n",
              " 'I need air.>',\n",
              " 'I need ice.>',\n",
              " 'I need you.>',\n",
              " 'I need you.>',\n",
              " 'I panicked.>',\n",
              " 'I promised.>',\n",
              " 'I ran away.>',\n",
              " 'I ran home.>',\n",
              " 'I remember.>',\n",
              " 'I remember.>',\n",
              " 'I remember.>',\n",
              " 'I said yes.>',\n",
              " 'I sat down.>',\n",
              " 'I saw that.>',\n",
              " 'I saw them.>',\n",
              " 'I saw them.>',\n",
              " 'I saw them.>',\n",
              " 'I screamed.>',\n",
              " 'I see them.>',\n",
              " 'I survived.>',\n",
              " 'I threw up.>',\n",
              " 'I tried it.>',\n",
              " 'I tried it.>',\n",
              " 'I use this.>',\n",
              " 'I want one!>',\n",
              " 'I want one!>',\n",
              " 'I want one.>',\n",
              " 'I want one.>',\n",
              " 'I want you.>',\n",
              " 'I want you.>',\n",
              " 'I want you.>',\n",
              " 'I was away.>',\n",
              " 'I was busy.>',\n",
              " 'I was good.>',\n",
              " 'I was good.>',\n",
              " 'I was good.>',\n",
              " 'I was good.>',\n",
              " 'I was late.>',\n",
              " 'I was late.>',\n",
              " 'I was lost.>',\n",
              " 'I was lost.>',\n",
              " 'I was sick.>',\n",
              " 'I was sick.>',\n",
              " 'I will try.>',\n",
              " 'I work out.>',\n",
              " 'I wrote it.>',\n",
              " 'I wrote it.>',\n",
              " \"I'd accept.>\",\n",
              " \"I'll check.>\",\n",
              " \"I'll do it.>\",\n",
              " \"I'll do it.>\",\n",
              " \"I'll drive.>\",\n",
              " \"I'll go in.>\",\n",
              " \"I'll hurry.>\",\n",
              " \"I'll leave.>\",\n",
              " \"I'll shoot.>\",\n",
              " \"I'll stand.>\",\n",
              " \"I'll start.>\",\n",
              " \"I'm French.>\",\n",
              " \"I'm Korean.>\",\n",
              " \"I'm a hero.>\",\n",
              " \"I'm a liar.>\",\n",
              " \"I'm baking!>\",\n",
              " \"I'm better.>\",\n",
              " \"I'm buying.>\",\n",
              " \"I'm buying.>\",\n",
              " \"I'm chubby.>\",\n",
              " \"I'm chubby.>\",\n",
              " \"I'm eating.>\",\n",
              " \"I'm famous.>\",\n",
              " \"I'm famous.>\",\n",
              " \"I'm faster.>\",\n",
              " \"I'm flabby.>\",\n",
              " \"I'm greedy.>\",\n",
              " \"I'm greedy.>\",\n",
              " \"I'm greedy.>\",\n",
              " \"I'm hiding.>\",\n",
              " \"I'm honest.>\",\n",
              " \"I'm humble.>\",\n",
              " \"I'm hungry!>\",\n",
              " \"I'm hungry.>\",\n",
              " \"I'm immune.>\",\n",
              " \"I'm immune.>\",\n",
              " \"I'm in bed.>\",\n",
              " \"I'm in bed.>\",\n",
              " \"I'm in bed.>\",\n",
              " \"I'm joking.>\",\n",
              " \"I'm loaded.>\",\n",
              " \"I'm lonely.>\",\n",
              " \"I'm lonely.>\",\n",
              " \"I'm losing.>\",\n",
              " \"I'm moving.>\",\n",
              " \"I'm normal.>\",\n",
              " \"I'm normal.>\",\n",
              " \"I'm paying.>\",\n",
              " \"I'm paying.>\",\n",
              " \"I'm pooped.>\",\n",
              " \"I'm rested.>\",\n",
              " \"I'm rested.>\",\n",
              " \"I'm ruined.>\",\n",
              " \"I'm ruined.>\",\n",
              " \"I'm shaken.>\",\n",
              " \"I'm shaken.>\",\n",
              " \"I'm single.>\",\n",
              " \"I'm skinny.>\",\n",
              " \"I'm skinny.>\",\n",
              " \"I'm sleepy!>\",\n",
              " \"I'm sleepy!>\",\n",
              " \"I'm sneaky.>\",\n",
              " \"I'm sneaky.>\",\n",
              " \"I'm strict.>\",\n",
              " \"I'm strict.>\",\n",
              " \"I'm strong.>\",\n",
              " \"I'm strong.>\",\n",
              " \"I'm thirty.>\",\n",
              " \"I'm wasted.>\",\n",
              " \"I've tried.>\",\n",
              " \"I've tried.>\",\n",
              " 'Ignore Tom.>',\n",
              " 'Ignore Tom.>',\n",
              " 'Ignore him.>',\n",
              " 'Is he busy?>',\n",
              " 'Is he dead?>',\n",
              " 'Is he dead?>',\n",
              " 'Is he tall?>',\n",
              " 'Is it done?>',\n",
              " 'Is it free?>',\n",
              " 'Is it hard?>',\n",
              " 'Is it love?>',\n",
              " 'Is it love?>',\n",
              " 'Is it nice?>',\n",
              " 'Is it safe?>',\n",
              " 'Is it safe?>',\n",
              " 'Is it safe?>',\n",
              " 'Is it true?>',\n",
              " 'Is that it?>',\n",
              " 'Is that so?>',\n",
              " 'Is that so?>',\n",
              " 'Is that so?>',\n",
              " 'It is cold.>',\n",
              " 'It matters.>',\n",
              " 'It matters.>',\n",
              " 'It matters.>',\n",
              " 'It went OK.>',\n",
              " \"It'll work.>\",\n",
              " \"It'll work.>\",\n",
              " \"It's a fad.>\",\n",
              " \"It's a fad.>\",\n",
              " \"It's a fad.>\",\n",
              " \"It's awful.>\",\n",
              " \"It's awful.>\",\n",
              " \"It's bogus.>\",\n",
              " \"It's bulky.>\",\n",
              " \"It's cheap.>\",\n",
              " \"It's clean.>\",\n",
              " \"It's early.>\",\n",
              " \"It's funny.>\",\n",
              " \"It's funny.>\",\n",
              " \"It's funny.>\",\n",
              " \"It's green.>\",\n",
              " \"It's my CD.>\",\n",
              " \"It's night.>\",\n",
              " \"It's on me.>\",\n",
              " \"It's phony.>\",\n",
              " \"It's ready.>\",\n",
              " \"It's right.>\",\n",
              " \"It's safer.>\",\n",
              " \"It's sweet.>\",\n",
              " \"It's sweet.>\",\n",
              " \"It's weird.>\",\n",
              " \"It's wrong.>\",\n",
              " \"It's yours.>\",\n",
              " \"It's yours.>\",\n",
              " \"It's yours.>\",\n",
              " \"It's yours.>\",\n",
              " 'Just relax.>',\n",
              " 'Just relax.>',\n",
              " 'Keep quiet!>',\n",
              " 'Keep quiet!>',\n",
              " 'Keep quiet.>',\n",
              " 'Keep quiet.>',\n",
              " 'Keep quiet.>',\n",
              " 'Let him go!>',\n",
              " 'Let him go!>',\n",
              " 'Let him go!>',\n",
              " 'Let him go!>',\n",
              " 'Let him go.>',\n",
              " 'Let him go.>',\n",
              " 'Let him go.>',\n",
              " 'Let him go.>',\n",
              " 'Let him in.>',\n",
              " 'Let him in.>',\n",
              " 'Let him in.>',\n",
              " 'Let him in.>',\n",
              " 'Let me die.>',\n",
              " 'Let me die.>',\n",
              " 'Let me out!>',\n",
              " 'Let me out!>',\n",
              " 'Let me pay.>',\n",
              " 'Let me pay.>',\n",
              " 'Let me see.>',\n",
              " 'Let me try.>',\n",
              " 'Let me try.>',\n",
              " 'Let us out.>',\n",
              " 'Let us out.>',\n",
              " \"Let's chat.>\",\n",
              " \"Let's kiss.>\",\n",
              " \"Let's pray.>\",\n",
              " \"Let's talk.>\",\n",
              " \"Let's work.>\",\n",
              " 'Lighten up.>',\n",
              " 'Lighten up.>',\n",
              " 'Look at it.>',\n",
              " 'Look at it.>',\n",
              " 'Look at me.>',\n",
              " 'Look it up.>',\n",
              " 'Love hurts.>',\n",
              " 'Love hurts.>',\n",
              " 'Mama cried.>',\n",
              " 'Mama cried.>',\n",
              " 'Never mind!>',\n",
              " 'Never mind!>',\n",
              " 'No comment.>',\n",
              " 'No kidding?>',\n",
              " 'No kidding?>',\n",
              " 'No problem!>',\n",
              " 'No problem!>',\n",
              " 'No problem.>',\n",
              " 'No problem.>',\n",
              " 'No problem.>',\n",
              " 'No problem.>',\n",
              " 'Oh, really?>',\n",
              " 'Once again.>',\n",
              " 'Pick it up.>',\n",
              " 'Please sit.>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Run for it!>',\n",
              " 'Say please.>',\n",
              " 'Say please.>',\n",
              " 'Shadow him.>',\n",
              " 'She is old.>',\n",
              " 'She smiled.>',\n",
              " \"She's busy.>\",\n",
              " \"She's nice.>\",\n",
              " 'Stand back!>',\n",
              " 'Stand back!>',\n",
              " 'Stand back!>',\n",
              " 'Stand back!>',\n",
              " 'Stay awake.>',\n",
              " 'Stay awake.>',\n",
              " 'Stay sharp.>',\n",
              " 'Step aside.>',\n",
              " 'Step aside.>',\n",
              " 'Stop lying.>',\n",
              " 'Stop lying.>',\n",
              " 'Study hard.>',\n",
              " 'Study hard.>',\n",
              " 'Take a bus.>',\n",
              " 'Take a nap.>',\n",
              " 'Talk to me!>',\n",
              " 'Talk to me!>',\n",
              " 'Talk to me!>',\n",
              " 'Talk to me.>',\n",
              " 'Talk to me.>',\n",
              " 'That a boy!>',\n",
              " 'That a boy!>',\n",
              " 'That hurts.>',\n",
              " 'That works.>',\n",
              " \"That's all.>\",\n",
              " \"That's fun.>\",\n",
              " \"That's fun.>\",\n",
              " \"That's her.>\",\n",
              " \"That's his.>\",\n",
              " \"That's his.>\",\n",
              " \"That's his.>\",\n",
              " \"That's odd.>\",\n",
              " 'They agree.>',\n",
              " 'They agree.>',\n",
              " 'They cheat.>',\n",
              " 'They cheat.>',\n",
              " 'They voted.>',\n",
              " 'They voted.>',\n",
              " 'This is it.>',\n",
              " 'This works.>',\n",
              " 'Time flies.>',\n",
              " 'Time flies.>',\n",
              " 'Time is up.>',\n",
              " 'Time is up.>',\n",
              " 'Tom agrees.>',\n",
              " 'Tom cheats.>',\n",
              " 'Tom drinks.>',\n",
              " 'Tom drives.>',\n",
              " 'Tom forgot.>',\n",
              " 'Tom forgot.>',\n",
              " 'Tom helped.>',\n",
              " 'Tom helped.>',\n",
              " 'Tom jumped.>',\n",
              " 'Tom looked.>',\n",
              " 'Tom looked.>',\n",
              " 'Tom nodded.>',\n",
              " 'Tom sighed.>',\n",
              " 'Tom snores.>',\n",
              " 'Tom yawned.>',\n",
              " \"Tom's dead.>\",\n",
              " \"Tom's deaf.>\",\n",
              " \"Tom's died.>\",\n",
              " \"Tom's fast.>\",\n",
              " \"Tom's free.>\",\n",
              " \"Tom's glad.>\",\n",
              " \"Tom's glad.>\",\n",
              " \"Tom's glad.>\",\n",
              " \"Tom's here.>\",\n",
              " \"Tom's here.>\",\n",
              " \"Tom's home.>\",\n",
              " \"Tom's home.>\",\n",
              " \"Tom's left.>\",\n",
              " \"Tom's left.>\",\n",
              " \"Tom's well.>\",\n",
              " 'Tough luck!>',\n",
              " 'Turn it up.>',\n",
              " 'Turn it up.>',\n",
              " 'Turn it up.>',\n",
              " 'Turn it up.>',\n",
              " 'Wait a bit.>',\n",
              " 'Wait a bit.>',\n",
              " 'Wait a sec.>',\n",
              " 'Wait a sec.>',\n",
              " 'We all die.>',\n",
              " 'We all die.>',\n",
              " 'We are men.>',\n",
              " 'We buy CDs.>',\n",
              " 'We can pay.>',\n",
              " 'We can try.>',\n",
              " 'We can try.>',\n",
              " 'We can win.>',\n",
              " 'We can win.>',\n",
              " 'We like it.>',\n",
              " 'We made it.>',\n",
              " 'We made it.>',\n",
              " 'We made it.>',\n",
              " 'We must go.>',\n",
              " 'We must go.>',\n",
              " 'We must go.>',\n",
              " 'We must go.>',\n",
              " 'We need it.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We saw you.>',\n",
              " 'We want it.>',\n",
              " \"We'll cook.>\",\n",
              " \"We'll fail.>\",\n",
              " \"We'll help.>\",\n",
              " \"We'll obey.>\",\n",
              " \"We'll pass.>\",\n",
              " \"We'll swim.>\",\n",
              " \"We'll wait.>\",\n",
              " \"We'll walk.>\",\n",
              " \"We'll work.>\",\n",
              " \"We're back.>\",\n",
              " \"We're busy.>\",\n",
              " \"We're busy.>\",\n",
              " \"We're done.>\",\n",
              " \"We're done.>\",\n",
              " \"We're done.>\",\n",
              " \"We're done.>\",\n",
              " \"We're even.>\",\n",
              " \"We're fine.>\",\n",
              " \"We're here.>\",\n",
              " \"We're here.>\",\n",
              " \"We're home.>\",\n",
              " \"We're late.>\",\n",
              " \"We're lost.>\",\n",
              " \"We're lost.>\",\n",
              " \"We're rich.>\",\n",
              " \"We're safe.>\",\n",
              " \"We're sunk.>\",\n",
              " \"We're sunk.>\",\n",
              " \"We're weak.>\",\n",
              " 'What a day!>',\n",
              " 'What is it?>',\n",
              " 'What is it?>',\n",
              " \"What's new?>\",\n",
              " 'Where am I?>',\n",
              " 'Who are we?>',\n",
              " 'Who did it?>',\n",
              " 'Who did it?>',\n",
              " 'Who saw me?>',\n",
              " 'Who talked?>',\n",
              " 'Who yelled?>',\n",
              " 'Why bother?>',\n",
              " 'You decide.>',\n",
              " 'You decide.>',\n",
              " 'You did it!>',\n",
              " 'You did it!>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " 'You may go.>',\n",
              " \"You're bad.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're big.>\",\n",
              " \"You're fun.>\",\n",
              " \"You're fun.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're old.>\",\n",
              " \"You're sad.>\",\n",
              " \"You're sad.>\",\n",
              " \"You're shy.>\",\n",
              " \"You're shy.>\",\n",
              " \"You've won!>\",\n",
              " \"You've won!>\",\n",
              " 'All is well.',\n",
              " 'Am I hungry!',\n",
              " 'Am I stupid?',\n",
              " 'Am I stupid?',\n",
              " 'Anyone home?',\n",
              " 'Anyone home?',\n",
              " 'Anyone home?',\n",
              " 'Anyone hurt?',\n",
              " 'Anyone hurt?',\n",
              " 'Are we done?',\n",
              " 'Are we done?',\n",
              " 'Are we lost?',\n",
              " 'Are we lost?',\n",
              " 'Are we safe?',\n",
              " 'Are you Tom?',\n",
              " 'Are you Tom?',\n",
              " 'Are you mad?',\n",
              " 'Are you mad?',\n",
              " 'Are you mad?',\n",
              " 'Are you new?',\n",
              " 'Are you sad?',\n",
              " 'As you like.',\n",
              " 'As you like.',\n",
              " 'Ask anybody.',\n",
              " 'Ask anybody.',\n",
              " 'Ask anybody.',\n",
              " 'Ask anybody.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be creative.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be discreet.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be friendly.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be merciful.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be prepared.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be punctual.',\n",
              " 'Be ruthless.',\n",
              " 'Be ruthless.',\n",
              " 'Be ruthless.',\n",
              " 'Be sensible.',\n",
              " 'Be sensible.',\n",
              " 'Be sensible.',\n",
              " 'Be yourself.',\n",
              " 'Be yourself.',\n",
              " 'Break it up!',\n",
              " 'Can it wait?',\n",
              " 'Can we come?',\n",
              " 'Can we help?',\n",
              " 'Can we stop?',\n",
              " 'Can we talk?',\n",
              " 'Can you see?',\n",
              " 'Can you see?',\n",
              " 'Can you ski?',\n",
              " 'Can you ski?',\n",
              " 'Can you try?',\n",
              " 'Clean it up.',\n",
              " 'Clean it up.',\n",
              " 'Come get it.',\n",
              " 'Come get me.',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it!',\n",
              " 'Come off it.',\n",
              " 'Come off it.',\n",
              " 'Cook for me.',\n",
              " 'Cook for me.',\n",
              " 'Count me in.',\n",
              " 'Cover it up.',\n",
              " 'Cover it up.',\n",
              " 'Deal me out.',\n",
              " 'Did it hurt?',\n",
              " 'Did you win?',\n",
              " 'Did you win?',\n",
              " 'Did you win?',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it again!',\n",
              " 'Do it right.',\n",
              " 'Do it right.',\n",
              " 'Do your job.',\n",
              " \"Don't do it!\",\n",
              " \"Don't do it!\",\n",
              " \"Don't do it.\",\n",
              " \"Don't do it.\",\n",
              " \"Don't gloat.\",\n",
              " \"Don't gloat.\",\n",
              " \"Don't laugh.\",\n",
              " \"Don't laugh.\",\n",
              " \"Don't leave!\",\n",
              " \"Don't leave!\",\n",
              " \"Don't leave.\",\n",
              " \"Don't leave.\",\n",
              " \"Don't shoot!\",\n",
              " \"Don't shoot!\",\n",
              " \"Don't shoot.\",\n",
              " \"Don't shoot.\",\n",
              " \"Don't shout.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " \"Don't worry.\",\n",
              " 'Flip a coin.',\n",
              " 'Get serious.',\n",
              " 'Get the box.',\n",
              " 'Get the box.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go home now.',\n",
              " 'Go on ahead.',\n",
              " 'Go to sleep.',\n",
              " 'Grab a seat.',\n",
              " 'Have a beer.',\n",
              " 'Have a look.',\n",
              " 'Have a look.',\n",
              " 'Have a look.',\n",
              " 'Have a look.',\n",
              " 'Have a seat.',\n",
              " 'Have a seat.',\n",
              " 'Have we met?',\n",
              " 'He can come.',\n",
              " 'He can read.',\n",
              " 'He can swim.',\n",
              " 'He chuckled.',\n",
              " 'He chuckled.',\n",
              " 'He found it.',\n",
              " 'He found it.',\n",
              " 'He got away.',\n",
              " 'He grew old.',\n",
              " 'He grew old.',\n",
              " 'He has come!',\n",
              " 'He has guts.',\n",
              " 'He has wine.',\n",
              " 'He helps us.',\n",
              " 'He is drunk.',\n",
              " 'He is drunk.',\n",
              " 'He is eight.',\n",
              " 'He is hated.',\n",
              " 'He is nasty.',\n",
              " 'He is smart.',\n",
              " 'He is young.',\n",
              " 'He likes me.',\n",
              " 'He likes me.',\n",
              " 'He needs it.',\n",
              " 'He resigned.',\n",
              " 'He squinted.',\n",
              " 'He squinted.',\n",
              " 'He stood up.',\n",
              " 'He stood up.',\n",
              " 'He stood up.',\n",
              " 'He was busy.',\n",
              " \"He's a hunk.\",\n",
              " \"He's a hunk.\",\n",
              " \"He's a jerk.\",\n",
              " \"He's a liar.\",\n",
              " \"He's a nerd.\",\n",
              " \"He's a slob.\",\n",
              " \"He's asleep.\",\n",
              " \"He's coming.\",\n",
              " \"He's coming.\",\n",
              " \"He's crying.\",\n",
              " \"He's faking.\",\n",
              " \"He's loaded.\",\n",
              " \"He's loaded.\",\n",
              " \"He's loaded.\",\n",
              " \"He's my age.\",\n",
              " \"He's not in.\",\n",
              " \"He's not in.\",\n",
              " \"He's not in.\",\n",
              " 'Help me out.',\n",
              " 'Help me out.',\n",
              " 'Here I come.',\n",
              " 'Here I come.',\n",
              " 'Here she is!',\n",
              " 'Here we are!',\n",
              " 'Here we are!',\n",
              " 'Here we are!',\n",
              " 'Here we are.',\n",
              " 'Here we are.',\n",
              " 'Here we are.',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How are you?',\n",
              " 'How curious!',\n",
              " 'How is life?',\n",
              " 'I also went.',\n",
              " 'I am French.',\n",
              " 'I am Korean.',\n",
              " 'I am a cook.',\n",
              " 'I am a monk.',\n",
              " 'I am better.',\n",
              " 'I am better.',\n",
              " 'I am coming.',\n",
              " 'I am hungry.',\n",
              " 'I am joking.',\n",
              " 'I am single.',\n",
              " 'I am taller.',\n",
              " 'I apologize.',\n",
              " 'I asked Tom.',\n",
              " 'I assume so.',\n",
              " 'I bought it.',\n",
              " 'I buried it.',\n",
              " 'I buried it.',\n",
              " 'I burned it.',\n",
              " 'I burned it.',\n",
              " 'I came back.',\n",
              " 'I can dance.',\n",
              " 'I can do it.',\n",
              " 'I can do it.',\n",
              " 'I can do it.',\n",
              " 'I can do it.',\n",
              " \"I can't eat.\",\n",
              " \"I can't say.\",\n",
              " \"I can't say.\",\n",
              " \"I can't see.\",\n",
              " \"I can't see.\",\n",
              " \"I can't see.\",\n",
              " \"I can't win.\",\n",
              " \"I can't win.\",\n",
              " \"I can't win.\",\n",
              " 'I caught it.',\n",
              " 'I caught it.',\n",
              " 'I confessed.',\n",
              " 'I confessed.',\n",
              " 'I could try.',\n",
              " 'I cut class.',\n",
              " \"I didn't go.\",\n",
              " \"I didn't go.\",\n",
              " \"I didn't go.\",\n",
              " \"I didn't go.\",\n",
              " 'I eat alone.',\n",
              " 'I eat alone.',\n",
              " 'I eat bread.',\n",
              " 'I eat fruit.',\n",
              " 'I exercised.',\n",
              " 'I exercised.',\n",
              " 'I feel blue.',\n",
              " 'I feel blue.',\n",
              " 'I feel cold.',\n",
              " 'I feel fine.',\n",
              " 'I feel good.',\n",
              " 'I feel lost.',\n",
              " 'I feel lost.',\n",
              " 'I feel safe.',\n",
              " 'I feel sick.',\n",
              " 'I feel weak.',\n",
              " 'I feel well.',\n",
              " 'I felt dumb.',\n",
              " 'I felt dumb.',\n",
              " 'I felt fear.',\n",
              " 'I felt good.',\n",
              " 'I felt safe.',\n",
              " 'I felt safe.',\n",
              " 'I got bored.',\n",
              " 'I got dizzy.',\n",
              " 'I got dizzy.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got fined.',\n",
              " 'I got lucky.',\n",
              " 'I got lucky.',\n",
              " 'I got upset.',\n",
              " 'I got upset.',\n",
              " 'I got upset.',\n",
              " 'I got upset.',\n",
              " 'I guess not.',\n",
              " 'I guess not.',\n",
              " 'I had to go.',\n",
              " 'I hate dogs.',\n",
              " 'I hate dogs.',\n",
              " 'I hate fish.',\n",
              " 'I hate golf.',\n",
              " 'I hate milk.',\n",
              " 'I hate sand.',\n",
              " 'I hate sand.',\n",
              " 'I hate that.',\n",
              " 'I hate them.',\n",
              " 'I hate this.',\n",
              " 'I hate work.',\n",
              " 'I have cash.',\n",
              " 'I have cash.',\n",
              " 'I have eyes.',\n",
              " 'I have food.',\n",
              " 'I have food.',\n",
              " 'I have time.',\n",
              " 'I have time.',\n",
              " 'I have wine.',\n",
              " 'I know that.',\n",
              " 'I know them.',\n",
              " 'I know this.',\n",
              " 'I know this.',\n",
              " 'I like blue.',\n",
              " 'I like blue.',\n",
              " 'I like both.',\n",
              " 'I like both.',\n",
              " 'I like cake.',\n",
              " 'I like cats.',\n",
              " 'I like cats.',\n",
              " 'I like dogs.',\n",
              " 'I like jazz.',\n",
              " 'I like math.',\n",
              " 'I like milk.',\n",
              " 'I like rice.',\n",
              " 'I like rock.',\n",
              " 'I like that.',\n",
              " 'I like that.',\n",
              " 'I like them.',\n",
              " 'I like this.',\n",
              " 'I like this.',\n",
              " 'I live here.',\n",
              " 'I love Mary.',\n",
              " 'I love kids.',\n",
              " 'I love life.',\n",
              " 'I loved you.',\n",
              " 'I loved you.',\n",
              " 'I loved you.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I messed up.',\n",
              " 'I must obey.',\n",
              " 'I must obey.',\n",
              " 'I nailed it.',\n",
              " 'I nailed it.',\n",
              " 'I nailed it.',\n",
              " 'I need food.',\n",
              " 'I need glue.',\n",
              " 'I need more.',\n",
              " 'I need more.',\n",
              " 'I need some.',\n",
              " 'I need that.',\n",
              " 'I need that.',\n",
              " 'I need that.',\n",
              " 'I need time.',\n",
              " 'I need time.',\n",
              " 'I never bet.',\n",
              " 'I never bet.',\n",
              " 'I never win.',\n",
              " 'I never win.',\n",
              " 'I often ski.',\n",
              " 'I oppose it.',\n",
              " 'I oppose it.',\n",
              " 'I pay taxes.',\n",
              " 'I pay taxes.',\n",
              " 'I recovered.',\n",
              " 'I recovered.',\n",
              " 'I recovered.',\n",
              " 'I recovered.',\n",
              " 'I said stop.',\n",
              " 'I said that.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saved you.',\n",
              " 'I saw a dog.',\n",
              " 'I sell cars.',\n",
              " 'I should go.',\n",
              " 'I should go.',\n",
              " 'I smell gas.',\n",
              " 'I surrender.',\n",
              " 'I thank you.',\n",
              " 'I trust her.',\n",
              " 'I trust him.',\n",
              " 'I trust him.',\n",
              " 'I trust you.',\n",
              " 'I trust you.',\n",
              " 'I waited up.',\n",
              " 'I want Mary.',\n",
              " 'I want cash.',\n",
              " 'I want eggs.',\n",
              " 'I want kids.',\n",
              " 'I want mine.',\n",
              " 'I want mine.',\n",
              " 'I want more.',\n",
              " 'I want more.',\n",
              " 'I want soup.',\n",
              " 'I want that.',\n",
              " 'I want that.',\n",
              " 'I want them.',\n",
              " 'I want this.',\n",
              " 'I want time.',\n",
              " 'I was alone.',\n",
              " 'I was alone.',\n",
              " 'I was alone.',\n",
              " 'I was bored.',\n",
              " 'I was broke.',\n",
              " 'I was dizzy.',\n",
              " 'I was dizzy.',\n",
              " 'I was drunk.',\n",
              " 'I was fired.',\n",
              " 'I was fired.',\n",
              " 'I was fired.',\n",
              " 'I was hired.',\n",
              " 'I was lucky.',\n",
              " 'I was moved.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was naive.',\n",
              " 'I was ready.',\n",
              " 'I was ready.',\n",
              " 'I was ready.',\n",
              " 'I was sober.',\n",
              " 'I was sorry.',\n",
              " 'I was sorry.',\n",
              " 'I was there.',\n",
              " 'I was there.',\n",
              " 'I was there.',\n",
              " 'I was there.',\n",
              " 'I was tired.',\n",
              " 'I was tired.',\n",
              " 'I was wrong.',\n",
              " 'I was wrong.',\n",
              " 'I was wrong.',\n",
              " 'I washed it.',\n",
              " 'I washed it.',\n",
              " 'I went home.',\n",
              " 'I went, too.',\n",
              " 'I went, too.',\n",
              " 'I went, too.',\n",
              " 'I will come.',\n",
              " 'I will help.',\n",
              " 'I will obey.',\n",
              " 'I will wait.',\n",
              " 'I will walk.',\n",
              " 'I will work.',\n",
              " 'I will work.',\n",
              " 'I won again.',\n",
              " \"I won't cry.\",\n",
              " \"I won't cry.\",\n",
              " 'I work here.',\n",
              " \"I'll attend.\",\n",
              " \"I'll attend.\",\n",
              " \"I'll buy it.\",\n",
              " \"I'll cancel.\",\n",
              " \"I'll change.\",\n",
              " \"I'll decide.\",\n",
              " \"I'll decide.\",\n",
              " \"I'll get in.\",\n",
              " \"I'll get it.\",\n",
              " \"I'll get it.\",\n",
              " \"I'll get up.\",\n",
              " \"I'll go now.\",\n",
              " \"I'll go see.\",\n",
              " \"I'll manage.\",\n",
              " \"I'll scream.\",\n",
              " \"I'll try it.\",\n",
              " \"I'll try it.\",\n",
              " \"I'm 17, too.\",\n",
              " \"I'm Finnish.\",\n",
              " \"I'm Finnish.\",\n",
              " \"I'm Italian.\",\n",
              " \"I'm a baker.\",\n",
              " \"I'm a baker.\",\n",
              " \"I'm all set.\",\n",
              " \"I'm all set.\",\n",
              " \"I'm ashamed.\",\n",
              " \"I'm at home.\",\n",
              " \"I'm at home.\",\n",
              " \"I'm baffled.\",\n",
              " \"I'm blessed.\",\n",
              " \"I'm blessed.\",\n",
              " \"I'm careful.\",\n",
              " \"I'm careful.\",\n",
              " \"I'm certain.\",\n",
              " \"I'm certain.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Acq_F0bUlua"
      },
      "source": [
        "I trained all day, but the performance is still poor. I'll figure out why later. Maybe the RNN is too simple? Let's see.\n",
        "\n",
        "The problem is that in order to train the model by minibatch, I have to add a lot of padding tokens into the training set. This hurts the performance of the model. As seen in the above example, when I tried to translate the English sentence 'closer look' into French, there are a lot of padding tokens (&&&), which is annoying.\n",
        "\n",
        "I think if I train the model with individual examples, then the problem could be relieved. However, the downside is that it could be time-consuming.\n",
        "\n",
        "Anyway, I will try to update another file that will use attention mechanism.\n",
        "\n",
        "See you in another Colab file\n",
        "\n",
        "see you in the next week?\n",
        "\n",
        "I have to take care the new baby in my family. so in the weekend, I didnot have the time to adjust my code, and train the model. I will try to do that in the next week. see you then.\n",
        "\n",
        "I know, this is a small project, I just want to write those projects to be more familiar with the NLP, which is helpful for me in my future phd study.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I finally found the where the problem.\n",
        "\n",
        "first I should not only use the final hidden state of the encoder. I should concate all the hidden state, and convert them into decoder.\n",
        "\n",
        "second for the decoder, for the prediction, the predicted character should be conditioned by the previous characters, the current state, and the state of the encoder.\n",
        "\n",
        "third, it is for the loss function. when I calcualte the loss, if encounter the padding, I should stop, because the error in the loss could cause the model to go at the wrong direction.\n",
        "\n",
        "okay, I will ask for more advices from chatGPT, continue this project.\n",
        "\n",
        "one more thing to mention is that, by doing the second step, I could easily add the attention machaism between the encoder and the decoder."
      ],
      "metadata": {
        "id": "xeg_uyoLvunJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now\n",
        "I changed some code in here, but the performance is still poor.\n",
        "what I change:\n",
        "1: use the final hidden state of the encoder, when predict the next token.\n",
        "2: when calculating the loss function, remove the padding tokens.\n",
        "\n",
        "\n",
        "okay, I think I could pause now,\n",
        "I will create another file to run the seq-2seq with attention, and check its performance. then use the transformers.\n",
        "\n",
        "if the performance is still poor, then I will use the word-level token not the character level.\n",
        "\n",
        "see you"
      ],
      "metadata": {
        "id": "zVpu55Scla2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have changed the approach in which the hidden_state of the encoder influlence the decoder prediction, but the performance is still poor.\n",
        "before using the word-level token, I will debug the training.\n",
        "let see."
      ],
      "metadata": {
        "id": "wpwPgUBBn_O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after debugging, I finnaly find the clue why the perforamcne is poor\n",
        "after each training step. the return result of the forward function is [T,B,C].\n",
        "when I use view(-1,C) to feed into the criterion. but then the order of the tokens is wrong. So I use permute to reshape the result.\n",
        "\n",
        "the result is still not perfect, but get better know.\n"
      ],
      "metadata": {
        "id": "oVlI9ILPRP_W"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcsn8H/II3AIeOp/MqZvYx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}