{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJL4uPYoKVGaQAW3VGMNwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZAzMVzsKlJlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e79d6cf-cf0d-4762-8b78-9c02d9fde82e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cc8e4348330>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "embedding_dim = 20\n",
        "learning_rate = 1e-1\n",
        "batch_size = 20\n",
        "line_len = 30\n",
        "shuffle = True"
      ],
      "metadata": {
        "id": "dKvwvE3qUB3q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = torch.zeros((1, 1, self.hidden_size))\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "    return h\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "    self.i2h = nn.Linear(self.embedding_dim, self.hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "    self.h2o = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, self.hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "  def forward(self, x, h):\n",
        "    output = []\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      y = self.h2o(h) + self.ob\n",
        "      output.append(y)\n",
        "    return output"
      ],
      "metadata": {
        "id": "_aiwNJdMpOvb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = Encoder(source_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "    self.decoder = Decoder(target_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "  def forward(self, source, target):\n",
        "    hidden_state = self.encoder(source)\n",
        "    output = self.decoder(target, hidden_state)\n",
        "    return torch.cat(output, dim=0)"
      ],
      "metadata": {
        "id": "0w1aF_iRqyzH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(20000,40000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append('<' + item[0] + '>')\n",
        "  fr_lines.append('<' + item[1] + '>')\n",
        "\n",
        "max_len_line_en = max([len(l) for l in en_lines])\n",
        "max_len_line_fr = max([len(l) for l in fr_lines])\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) < max_len_line_en):\n",
        "    en_lines[i] = en_lines[i].ljust(max_len_line_en, padding_character)\n",
        "  if (len(fr_lines[i]) < max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i].ljust(max_len_line_fr, padding_character)\n",
        "\n",
        "\n",
        "source_vocab = set(''.join(en_lines))\n",
        "target_vocab = set(''.join(fr_lines))\n",
        "\n",
        "source_vocab_size = len(set(''.join(en_lines)))\n",
        "target_vocab_size = len(set(''.join(fr_lines)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}"
      ],
      "metadata": {
        "id": "2lXvt3NLMryS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_data = []\n",
        "fr_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)"
      ],
      "metadata": {
        "id": "OBp6aNm3UA4v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXBDa-swjLNd",
        "outputId": "67835a15-6f5b-4aef-845c-9370801aaf5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10, 84, 21, 32, 58, 21, 85, 52, 32, 45, 85, 21, 32, 58, 41, 85, 61, 32,\n",
              "        74,  9, 46, 33, 78, 50, 21, 25,  5, 13, 54, 54, 54, 54, 54, 54, 54, 54,\n",
              "        54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54,\n",
              "        54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54,\n",
              "        54, 54])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = 10\n",
        "input = en_data[p:p+batch_size]\n",
        "input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veh_gC1jQ05j",
        "outputId": "1a8efc11-1b4a-4295-f547-15567cb6a506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "id": "Wa0KsRl8mfto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input[:,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_VyPAK9kHfC",
        "outputId": "5582535f-a872-4537-a943-65e03b93b362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "encoder = Encoder(source_vocab_size, embedding_dim, hidden_size)\n",
        "model = Seq2Seq(source_vocab_size, target_vocab_size, embedding_dim, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(len(en_data) - batch_size - 1):\n",
        "\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(source_batch, target_batch)\n",
        "\n",
        "    loss = criterion(output.view(-1, target_vocab_size), target_batch.view(-1))\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if p%100 == 0:\n",
        "      # Print or log the training loss for each epoch\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HwqI27qRs9_",
        "outputId": "1e9e6174-3000-4029-abb7-6e435249a656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p 0, Loss: 4.728506565093994\n",
            "p 100, Loss: 1.7974867820739746\n",
            "p 200, Loss: 2.522261142730713\n",
            "p 300, Loss: 4.206422328948975\n",
            "p 400, Loss: 2.211564779281616\n",
            "p 500, Loss: 1.787145972251892\n",
            "p 600, Loss: 4.528811454772949\n",
            "p 700, Loss: 5.3500752449035645\n",
            "p 800, Loss: 2.0990705490112305\n",
            "p 900, Loss: 4.296488285064697\n",
            "p 1000, Loss: 2.886258602142334\n",
            "p 1100, Loss: 1.683642864227295\n",
            "p 1200, Loss: 2.2459874153137207\n",
            "p 1300, Loss: 2.221745252609253\n",
            "p 1400, Loss: 2.273402452468872\n",
            "p 1500, Loss: 1.7251325845718384\n",
            "p 1600, Loss: 2.1542458534240723\n",
            "p 1700, Loss: 1.8772292137145996\n",
            "p 1800, Loss: 4.139653205871582\n",
            "p 1900, Loss: 4.822388172149658\n",
            "p 2000, Loss: 7.3139238357543945\n",
            "p 2100, Loss: 2.6425466537475586\n",
            "p 2200, Loss: 2.130760669708252\n",
            "p 2300, Loss: 1.7104228734970093\n",
            "p 2400, Loss: 1.7202826738357544\n",
            "p 2500, Loss: 3.5414130687713623\n",
            "p 2600, Loss: 4.9831862449646\n",
            "p 2700, Loss: 2.7417476177215576\n",
            "p 2800, Loss: 1.9318255186080933\n",
            "p 2900, Loss: 2.483248233795166\n",
            "p 3000, Loss: 3.4351084232330322\n",
            "p 3100, Loss: 3.016230821609497\n",
            "p 3200, Loss: 1.77947998046875\n",
            "p 3300, Loss: 1.937173843383789\n",
            "p 3400, Loss: 4.084178924560547\n",
            "p 3500, Loss: 6.499504566192627\n",
            "p 3600, Loss: 2.6021173000335693\n",
            "p 3700, Loss: 2.0670228004455566\n",
            "p 3800, Loss: 1.9133330583572388\n",
            "p 3900, Loss: 2.234863042831421\n",
            "p 4000, Loss: 1.843672752380371\n",
            "p 4100, Loss: 2.1164908409118652\n",
            "p 4200, Loss: 2.396517276763916\n",
            "p 4300, Loss: 1.8668479919433594\n",
            "p 4400, Loss: 2.0140998363494873\n",
            "p 4500, Loss: 1.9065665006637573\n",
            "p 4600, Loss: 10.889602661132812\n",
            "p 4700, Loss: 8.637370109558105\n",
            "p 4800, Loss: 4.014015197753906\n",
            "p 4900, Loss: 3.562840223312378\n",
            "p 5000, Loss: 2.9649288654327393\n",
            "p 5100, Loss: 2.4289960861206055\n",
            "p 5200, Loss: 2.4970006942749023\n",
            "p 5300, Loss: 2.188573122024536\n",
            "p 5400, Loss: 2.182654857635498\n",
            "p 5500, Loss: 2.6905667781829834\n",
            "p 5600, Loss: 2.0886895656585693\n",
            "p 5700, Loss: 2.046076536178589\n",
            "p 5800, Loss: 7.276686668395996\n",
            "p 5900, Loss: 4.732232093811035\n",
            "p 6000, Loss: 2.858403444290161\n",
            "p 6100, Loss: 2.2800509929656982\n",
            "p 6200, Loss: 2.6571176052093506\n",
            "p 6300, Loss: 3.1028947830200195\n",
            "p 6400, Loss: 4.263998031616211\n",
            "p 6500, Loss: 2.338768243789673\n",
            "p 6600, Loss: 2.3427677154541016\n",
            "p 6700, Loss: 1.8389098644256592\n",
            "p 6800, Loss: 4.355928421020508\n",
            "p 6900, Loss: 3.842848062515259\n",
            "p 7000, Loss: 3.386432647705078\n",
            "p 7100, Loss: 1.790371298789978\n",
            "p 7200, Loss: 3.3699724674224854\n",
            "p 7300, Loss: 4.764350891113281\n",
            "p 7400, Loss: 5.631094932556152\n",
            "p 7500, Loss: 3.0864768028259277\n",
            "p 7600, Loss: 1.8752201795578003\n",
            "p 7700, Loss: 1.7198455333709717\n",
            "p 7800, Loss: 2.762418270111084\n",
            "p 7900, Loss: 2.9275848865509033\n",
            "p 8000, Loss: 3.892357349395752\n"
          ]
        }
      ]
    }
  ]
}