{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAzMVzsKlJlV",
        "outputId": "f5f43faf-a88a-429d-b320-fa0fdd162c0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a0dc909c390>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKvwvE3qUB3q"
      },
      "outputs": [],
      "source": [
        "hidden_size = 100\n",
        "embedding_dim = 30\n",
        "learning_rate = 0.001\n",
        "batch_size = 50\n",
        "beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgaJy39hUt2C"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(0,3000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append(item[0] + '>')\n",
        "  fr_lines.append('<' + item[1] + '>')\n",
        "\n",
        "max_len_line_en = max([len(l) for l in en_lines])\n",
        "max_len_line_fr = max([len(l) for l in fr_lines])\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) < max_len_line_en):\n",
        "    en_lines[i] = en_lines[i].ljust(max_len_line_en, padding_character)\n",
        "  if (len(fr_lines[i]) < max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i].ljust(max_len_line_fr, padding_character)\n",
        "\n",
        "\n",
        "source_vocab = sorted(set(''.join(en_lines)))\n",
        "target_vocab = sorted(set(''.join(fr_lines)))\n",
        "\n",
        "source_vocab_size = len(set(''.join(source_vocab)))\n",
        "target_vocab_size = len(set(''.join(target_vocab)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}\n",
        "\n",
        "padding_token_index = target_char_to_ix[padding_character]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUfLKgxoU7j3"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([source_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "def target_line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([target_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "en_data = []\n",
        "fr_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aiwNJdMpOvb"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "    return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "    self.i2h = nn.Linear(self.embedding_dim, self.hidden_size, bias=False)\n",
        "    self.h2h = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "    self.h2o = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.e2d = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, self.hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "  def init_state(self, encode_state):\n",
        "    self.encode_state = encode_state\n",
        "\n",
        "\n",
        "  def forward(self, target):\n",
        "\n",
        "    # if x is None:\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    output = []\n",
        "    for i in range(max_len_line_fr):\n",
        "      x = target[:,i]\n",
        "      t = self.embedding(x)\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      y = self.e2d(self.encode_state) + self.h2o(h) + self.ob\n",
        "      output.append(y)\n",
        "    return torch.stack(output, dim=0)\n",
        "\n",
        "  def forward1(self, batch_size):\n",
        "\n",
        "    # if x is None:\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    x = torch.tensor([target_char_to_ix[start_character] for _ in range(batch_size)],dtype=torch.long)\n",
        "    output = []\n",
        "    for i in range(max_len_line_fr):\n",
        "      t = self.embedding(x)\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      y = self.e2d(self.encode_state) + self.h2o(h) + self.ob\n",
        "      p = nn.functional.softmax(y, dim=1)\n",
        "      ix = torch.argmax(p, dim=-1)\n",
        "      x = ix\n",
        "      output.append(y)\n",
        "    return torch.stack(output, dim=0)\n",
        "\n",
        "  def beam_search(self):\n",
        "    \"\"\"\n",
        "    Perform beam search to generate sequences.\n",
        "    \"\"\"\n",
        "    beams = [(torch.tensor([target_char_to_ix[start_character]], dtype=torch.long), 1.0)]\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "\n",
        "    for i in range(max_len_line_fr):\n",
        "      new_beams = []\n",
        "\n",
        "      for seq, score in beams:\n",
        "        x = seq[-1].view(1, -1)  # Take the last predicted token\n",
        "\n",
        "        t = self.embedding(x)\n",
        "        h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "        y = self.e2d(self.encode_state) + self.h2o(h) + self.ob\n",
        "        p = F.softmax(y, dim=-1)\n",
        "        top_probs, top_ix = torch.topk(p, beam_width, dim=-1)\n",
        "\n",
        "        for prob, token_ix in zip(top_probs[0][0], top_ix[0][0]):\n",
        "          new_seq = torch.cat((seq, torch.tensor([token_ix], dtype=torch.long)), dim=0)\n",
        "          new_beams.append((new_seq, score * prob.item()))\n",
        "\n",
        "      beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return beams\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = Encoder(source_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "    self.decoder = Decoder(target_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "  def forward(self, source, batch_size):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    output = self.decoder(batch_size)\n",
        "    return output\n",
        "\n",
        "  def translate(self, source):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    beams = self.decoder.beam_search()\n",
        "    return beams\n",
        "\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "model = Seq2Seq(source_vocab_size, target_vocab_size, embedding_dim, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(source_vocab_size, embedding_dim, hidden_size)\n",
        "\n",
        "p = 1\n",
        "\n",
        "source_batch = en_data[p:p+batch_size]\n",
        "target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "hidden = encoder(source_batch)\n",
        "hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54-2NzGed6P-",
        "outputId": "02a7d8d2-26ca-45b0-b136-ff5e49e005f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5469, -0.3315, -0.6971,  ...,  0.2071,  0.8874,  0.0376],\n",
              "        [ 0.5469, -0.3315, -0.6971,  ...,  0.2071,  0.8874,  0.0376],\n",
              "        [ 0.5468, -0.3316, -0.6972,  ...,  0.2073,  0.8874,  0.0374],\n",
              "        ...,\n",
              "        [ 0.5477, -0.3328, -0.6926,  ...,  0.2077,  0.8883,  0.0472],\n",
              "        [ 0.5477, -0.3343, -0.6931,  ...,  0.2077,  0.8885,  0.0441],\n",
              "        [ 0.5477, -0.3343, -0.6931,  ...,  0.2077,  0.8885,  0.0441]],\n",
              "       grad_fn=<TanhBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([target_char_to_ix['<'] for _ in range(batch_size)],dtype=torch.long)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTTfm5u9Jkqg",
        "outputId": "7fb30684-f54d-48a0-a0d2-3d8f2d2b6d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_batch = fr_data[p:p+batch_size]"
      ],
      "metadata": {
        "id": "kOmwLQ8wDgi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_batch[:,10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOVDg7erKzr4",
        "outputId": "062f7d24-761c-4c13-c22a-b1f20fac08a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HwqI27qRs9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998cfdb0-29da-4088-dfc5-2c8ccb8ab1e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p 0, Loss: 4.697800636291504\n",
            "p 100, Loss: 1.4145365953445435\n",
            "p 200, Loss: 1.5208317041397095\n",
            "p 300, Loss: 1.4787399768829346\n",
            "p 400, Loss: 1.4482678174972534\n",
            "p 500, Loss: 1.4810984134674072\n",
            "p 600, Loss: 1.5129822492599487\n",
            "p 700, Loss: 1.478727102279663\n",
            "p 800, Loss: 1.7247045040130615\n",
            "p 900, Loss: 1.5345492362976074\n",
            "p 1000, Loss: 1.5424134731292725\n",
            "p 1100, Loss: 1.527912974357605\n",
            "p 1200, Loss: 1.5975698232650757\n",
            "p 1300, Loss: 1.4835697412490845\n",
            "p 1400, Loss: 1.7118548154830933\n",
            "p 1500, Loss: 1.6063047647476196\n",
            "p 1600, Loss: 1.6118443012237549\n",
            "p 1700, Loss: 1.6741050481796265\n",
            "p 1800, Loss: 1.5356875658035278\n",
            "p 1900, Loss: 1.5401149988174438\n",
            "p 2000, Loss: 1.5905791521072388\n",
            "p 2100, Loss: 1.7025511264801025\n",
            "p 2200, Loss: 1.7367477416992188\n",
            "p 2300, Loss: 1.468629002571106\n",
            "p 2400, Loss: 1.72535240650177\n",
            "p 2500, Loss: 1.5578110218048096\n",
            "p 2600, Loss: 1.6707231998443604\n",
            "p 2700, Loss: 1.7332000732421875\n",
            "p 2800, Loss: 1.724569320678711\n",
            "p 2900, Loss: 1.8029965162277222\n",
            "p 0, Loss: 1.4752938747406006\n",
            "p 100, Loss: 1.4127888679504395\n",
            "p 200, Loss: 1.5166789293289185\n",
            "p 300, Loss: 1.4761455059051514\n",
            "p 400, Loss: 1.4453679323196411\n",
            "p 500, Loss: 1.4799201488494873\n",
            "p 600, Loss: 1.511650562286377\n",
            "p 700, Loss: 1.4789353609085083\n",
            "p 800, Loss: 1.7204197645187378\n",
            "p 900, Loss: 1.5333360433578491\n",
            "p 1000, Loss: 1.5407830476760864\n",
            "p 1100, Loss: 1.5272780656814575\n",
            "p 1200, Loss: 1.5985300540924072\n",
            "p 1300, Loss: 1.4826500415802002\n",
            "p 1400, Loss: 1.7077922821044922\n",
            "p 1500, Loss: 1.6034598350524902\n",
            "p 1600, Loss: 1.6047569513320923\n",
            "p 1700, Loss: 1.674668312072754\n",
            "p 1800, Loss: 1.5339025259017944\n",
            "p 1900, Loss: 1.5402405261993408\n",
            "p 2000, Loss: 1.5907716751098633\n",
            "p 2100, Loss: 1.702675700187683\n",
            "p 2200, Loss: 1.7385658025741577\n",
            "p 2300, Loss: 1.4666634798049927\n",
            "p 2400, Loss: 1.7235718965530396\n",
            "p 2500, Loss: 1.5568666458129883\n",
            "p 2600, Loss: 1.6716409921646118\n",
            "p 2700, Loss: 1.7296682596206665\n",
            "p 2800, Loss: 1.7241650819778442\n",
            "p 2900, Loss: 1.8018206357955933\n",
            "p 0, Loss: 1.4695261716842651\n",
            "p 100, Loss: 1.4102718830108643\n",
            "p 200, Loss: 1.5162556171417236\n",
            "p 300, Loss: 1.4742743968963623\n",
            "p 400, Loss: 1.4449518918991089\n",
            "p 500, Loss: 1.480047345161438\n",
            "p 600, Loss: 1.5120147466659546\n",
            "p 700, Loss: 1.4779157638549805\n",
            "p 800, Loss: 1.7176268100738525\n",
            "p 900, Loss: 1.532517671585083\n",
            "p 1000, Loss: 1.5402005910873413\n",
            "p 1100, Loss: 1.5271189212799072\n",
            "p 1200, Loss: 1.595106601715088\n",
            "p 1300, Loss: 1.4817769527435303\n",
            "p 1400, Loss: 1.706484079360962\n",
            "p 1500, Loss: 1.6030117273330688\n",
            "p 1600, Loss: 1.6033480167388916\n",
            "p 1700, Loss: 1.6745721101760864\n",
            "p 1800, Loss: 1.5343950986862183\n",
            "p 1900, Loss: 1.540189504623413\n",
            "p 2000, Loss: 1.5906352996826172\n",
            "p 2100, Loss: 1.7025045156478882\n",
            "p 2200, Loss: 1.7374258041381836\n",
            "p 2300, Loss: 1.4675331115722656\n",
            "p 2400, Loss: 1.724413275718689\n",
            "p 2500, Loss: 1.5571049451828003\n",
            "p 2600, Loss: 1.6713308095932007\n",
            "p 2700, Loss: 1.7302932739257812\n",
            "p 2800, Loss: 1.7234911918640137\n",
            "p 2900, Loss: 1.802032232284546\n"
          ]
        }
      ],
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(len(en_data) - batch_size - 1):\n",
        "\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # encoder = Encoder(source_vocab_size, embedding_dim, hidden_size)\n",
        "    # output = encoder(source_batch)\n",
        "    output = model(source_batch, target_batch)\n",
        "\n",
        "    # remove the padding tokens when calculate the loss\n",
        "    # Create a mask to ignore padding tokens\n",
        "    padding_mask = (target_batch != padding_token_index).float()\n",
        "\n",
        "    # Compute the loss with the padding mask\n",
        "    loss = criterion(output.view(-1, target_vocab_size), target_batch.view(-1))\n",
        "    loss = (loss * padding_mask.view(-1)).sum() / padding_mask.sum()\n",
        "\n",
        "\n",
        "    # loss = criterion(output.view(-1, target_vocab_size), target_batch.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    if p%100 == 0:\n",
        "      # Print or log the training loss for each epoch\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n",
        "    p += batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVVtitSdL6t",
        "outputId": "273bfb61-f7ca-4f3a-b91e-439d0a8a2054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "< &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "<e&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n"
          ]
        }
      ],
      "source": [
        "test_line = \"I fell>\"\n",
        "\n",
        "input = line_to_tensor(test_line)\n",
        "start = target_line_to_tensor(\"<\")\n",
        "\n",
        "\n",
        "outputs = model.translate(input)\n",
        "for tensor,p in outputs:\n",
        "  result = [target_ix_to_char[j.item()] for j in tensor]\n",
        "  print(''.join(result))\n",
        "\n",
        "# outputs = model(input,1)\n",
        "# result = []\n",
        "# for i in range(outputs.shape[0]):\n",
        "\n",
        "#   p = nn.functional.softmax(outputs[i], dim=-1).detach().numpy().ravel()\n",
        "#   ix = np.random.choice(range(target_vocab_size), p=p)\n",
        "\n",
        "#   result.append(target_ix_to_char[ix])\n",
        "\n",
        "# print(''.join(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Acq_F0bUlua"
      },
      "source": [
        "I trained all day, but the performance is still poor. I'll figure out why later. Maybe the RNN is too simple? Let's see.\n",
        "\n",
        "The problem is that in order to train the model by minibatch, I have to add a lot of padding tokens into the training set. This hurts the performance of the model. As seen in the above example, when I tried to translate the English sentence 'closer look' into French, there are a lot of padding tokens (&&&), which is annoying.\n",
        "\n",
        "I think if I train the model with individual examples, then the problem could be relieved. However, the downside is that it could be time-consuming.\n",
        "\n",
        "Anyway, I will try to update another file that will use attention mechanism.\n",
        "\n",
        "See you in another Colab file\n",
        "\n",
        "see you in the next week?\n",
        "\n",
        "I have to take care the new baby in my family. so in the weekend, I didnot have the time to adjust my code, and train the model. I will try to do that in the next week. see you then.\n",
        "\n",
        "I know, this is a small project, I just want to write those projects to be more familiar with the NLP, which is helpful for me in my future phd study.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I finally found the where the problem.\n",
        "\n",
        "first I should not only use the final hidden state of the encoder. I should concate all the hidden state, and convert them into decoder.\n",
        "\n",
        "second for the decoder, for the prediction, the predicted character should be conditioned by the previous characters, the current state, and the state of the encoder.\n",
        "\n",
        "third, it is for the loss function. when I calcualte the loss, if encounter the padding, I should stop, because the error in the loss could cause the model to go at the wrong direction.\n",
        "\n",
        "okay, I will ask for more advices from chatGPT, continue this project.\n",
        "\n",
        "one more thing to mention is that, by doing the second step, I could easily add the attention machaism between the encoder and the decoder."
      ],
      "metadata": {
        "id": "xeg_uyoLvunJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now\n",
        "I changed some code in here, but the performance is still poor.\n",
        "what I change:\n",
        "1: use the final hidden state of the encoder, when predict the next token.\n",
        "2: when calculating the loss function, remove the padding tokens.\n",
        "\n",
        "\n",
        "okay, I think I could pause now,\n",
        "I will create another file to run the seq-2seq with attention, and check its performance. then use the transformers.\n",
        "\n",
        "if the performance is still poor, then I will use the word-level token not the character level.\n",
        "\n",
        "see you"
      ],
      "metadata": {
        "id": "zVpu55Scla2d"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSwFGenhOMOFDR8GGcCjcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}