I have been studying machine translation for a long time and have used OpenNMT to complete some projects under low resource conditions. Despite being familiar with RNN and transformer, I have rarely had the opportunity to implement a machine translation entirely on my own.

In this repository, I am attempting to do just that. The first project will be at a basic level, using a seq-to-seq(RNN) architecture. The second will also use the same RNN architecture but will employ an attention system to connect the encoder and the decoder. The third, and potentially the final project, will utilize transformers.

If time allows, I would like to try statistical machine translation as well. Let's see how it goes.

I hope this repository will be beneficial to anyone interested in machine translation.

I will make an new seq-to-seq architecture, but will make the rnn more deep, let's say 3 layers, to check the performance.
