{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_GRU_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAzMVzsKlJlV",
        "outputId": "074a49e7-fa99-4564-af71-ea627a7d2025"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7948ec7a8210>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dKvwvE3qUB3q"
      },
      "outputs": [],
      "source": [
        "hidden_size = 100\n",
        "embedding_dim = 30\n",
        "learning_rate = 0.001\n",
        "batch_size = 50\n",
        "beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pgaJy39hUt2C"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(20000,30000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append(item[0] + '>')\n",
        "  fr_lines.append(item[1] + '>')\n",
        "\n",
        "max_len_line_en = max([len(l) for l in en_lines])\n",
        "max_len_line_fr = max([len(l) for l in fr_lines])\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) < max_len_line_en):\n",
        "    en_lines[i] = en_lines[i].ljust(max_len_line_en, padding_character)\n",
        "  if (len(fr_lines[i]) < max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i].ljust(max_len_line_fr, padding_character)\n",
        "\n",
        "\n",
        "source_vocab = sorted(set(''.join(en_lines)))\n",
        "target_vocab = sorted(set(''.join(fr_lines)))\n",
        "\n",
        "target_vocab.append('<')\n",
        "\n",
        "source_vocab_size = len(set(''.join(source_vocab)))\n",
        "target_vocab_size = len(set(''.join(target_vocab)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}\n",
        "\n",
        "padding_token_index = target_char_to_ix[padding_character]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GUfLKgxoU7j3"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([source_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "def target_line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([target_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "en_data = []\n",
        "fr_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_aiwNJdMpOvb"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    seq_len = encoder_outputs.size(1)\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=-1)))\n",
        "    attention_scores = torch.matmul(energy, self.v)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "    context_vector = torch.sum(attention_weights.unsqueeze(2) * encoder_outputs, dim=1)\n",
        "    return context_vector\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.Wr = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.Hr = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "    self.Wz = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.Hz = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "    self.Wh = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.Hh = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "    self.rb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.zb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.hb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_prev = torch.zeros(1, self.hidden_size)\n",
        "    h_list = []\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      rt = torch.sigmoid(self.Wr(t) + self.Hr(h_prev) + self.rb)\n",
        "      zt = torch.sigmoid(self.Wz(t) + self.Hz(h_prev) + self.zb)\n",
        "\n",
        "      tht = torch.tanh(self.Wh(t) + rt * self.Hh(h_prev) + self.hb)\n",
        "      h_prev = zt * tht + (1 - zt) * h_prev\n",
        "      h_list.append(h_prev)\n",
        "    return torch.stack(h_list, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "\n",
        "    self.Wr = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.Hr = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "    self.Wz = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.Hz = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "    self.Wh = nn.Linear(embedding_dim, hidden_size, bias=False)\n",
        "    self.Hh = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "    self.rb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.zb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.hb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "    self.Ho = nn.Linear(hidden_size, vocab_size)\n",
        "    self.e2d = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
        "    self.att = Attention(hidden_size)\n",
        "\n",
        "  def init_state(self, encode_state):\n",
        "    T,B,C = encode_state.shape\n",
        "    self.encode_state = encode_state.reshape(B,T,C)\n",
        "\n",
        "\n",
        "  def forward(self, batch_size):\n",
        "\n",
        "    # if x is None:\n",
        "    h_prev = torch.zeros(batch_size, self.hidden_size)\n",
        "    x = torch.tensor([target_char_to_ix[start_character] for _ in range(batch_size)],dtype=torch.long)\n",
        "    output = []\n",
        "    for i in range(max_len_line_fr):\n",
        "      t = self.embedding(x)\n",
        "\n",
        "      rt = torch.sigmoid(self.Wr(t) + self.Hr(h_prev) + self.rb)\n",
        "      zt = torch.sigmoid(self.Wz(t) + self.Hz(h_prev) + self.zb)\n",
        "\n",
        "      tht = torch.tanh(self.Wh(t) + rt * self.Hh(h_prev) + self.hb)\n",
        "      h_prev = zt * tht + (1 - zt) * h_prev\n",
        "\n",
        "      context_state = self.att(h_prev, self.encode_state)\n",
        "      y = self.e2d(context_state) + self.Ho(h_prev) + self.ob\n",
        "      p = nn.functional.softmax(y, dim=1)\n",
        "      ix = torch.argmax(p, dim=-1)\n",
        "      x = ix\n",
        "      output.append(y)\n",
        "    return torch.stack(output, dim=0).permute(1, 0, 2)\n",
        "\n",
        "  def beam_search(self):\n",
        "    \"\"\"\n",
        "    Perform beam search to generate sequences.\n",
        "    \"\"\"\n",
        "    beams = [(torch.tensor([target_char_to_ix[start_character]], dtype=torch.long), 1.0)]\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "\n",
        "    for i in range(max_len_line_fr):\n",
        "      new_beams = []\n",
        "\n",
        "      for seq, score in beams:\n",
        "        x = seq[-1].view(1, -1)  # Take the last predicted token\n",
        "\n",
        "        t = self.embedding(x)\n",
        "        rt = torch.sigmoid(self.Wr(t) + self.Hr(h) + self.rb)\n",
        "        zt = torch.sigmoid(self.Wz(t) + self.Hz(h) + self.zb)\n",
        "\n",
        "        tht = torch.tanh(self.Wh(t) + rt * self.Hh(h) + self.hb)\n",
        "        h = zt * tht + (1 - zt) * h\n",
        "\n",
        "        context_state = self.att(h.reshape(-1, self.hidden_size), self.encode_state)\n",
        "        y = self.e2d(context_state) + self.Ho(h) + self.ob\n",
        "        p = F.softmax(y, dim=-1)\n",
        "        top_probs, top_ix = torch.topk(p, beam_width, dim=-1)\n",
        "\n",
        "        for prob, token_ix in zip(top_probs[0][0], top_ix[0][0]):\n",
        "          new_seq = torch.cat((seq, torch.tensor([token_ix], dtype=torch.long)), dim=0)\n",
        "          new_beams.append((new_seq, score * prob.item()))\n",
        "\n",
        "      beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return beams\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = Encoder(source_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "    self.decoder = Decoder(target_vocab_size, self.embedding_dim, self.hidden_size)\n",
        "  def forward(self, source, batch_size):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    output = self.decoder(batch_size)\n",
        "    return output\n",
        "\n",
        "  def translate(self, source):\n",
        "    hidden_state = self.encoder(source)\n",
        "    self.decoder.init_state(hidden_state)\n",
        "    beams = self.decoder.beam_search()\n",
        "    return beams\n",
        "\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "model = Seq2Seq(source_vocab_size, target_vocab_size, embedding_dim, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HwqI27qRs9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5546a482-facb-4835-cbcf-73161980ca10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p 0, Loss: 4.638576984405518\n",
            "p 500, Loss: 3.0082247257232666\n",
            "p 1000, Loss: 2.1656718254089355\n",
            "p 1500, Loss: 1.8153971433639526\n",
            "p 2000, Loss: 2.1199541091918945\n",
            "p 2500, Loss: 1.8754996061325073\n",
            "p 3000, Loss: 1.7304545640945435\n",
            "p 3500, Loss: 1.7717962265014648\n",
            "p 4000, Loss: 1.4608933925628662\n",
            "p 4500, Loss: 1.6876193284988403\n",
            "p 5000, Loss: 1.6519733667373657\n",
            "p 5500, Loss: 1.2705917358398438\n"
          ]
        }
      ],
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(0,len(en_data) - batch_size - 1,batch_size):\n",
        "\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # encoder = Encoder(source_vocab_size, embedding_dim, hidden_size)\n",
        "    # output = encoder(source_batch)\n",
        "    output = model(source_batch, batch_size)\n",
        "\n",
        "    output = output.reshape(-1, target_vocab_size)\n",
        "    # remove the padding tokens when calculate the loss\n",
        "    # Create a mask to ignore padding tokens\n",
        "    padding_mask = (target_batch != padding_token_index).float()\n",
        "\n",
        "    # Compute the loss with the padding mask\n",
        "    loss = criterion(output, target_batch.view(-1))\n",
        "    loss = (loss * padding_mask.view(-1)).sum() / padding_mask.sum()\n",
        "\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    if p%500 == 0:\n",
        "      # Print or log the training loss for each epoch\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVVtitSdL6t",
        "outputId": "06842a01-e563-43c0-af2a-69a20bb72189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Je  e&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "<Je    &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "<Je  s&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n"
          ]
        }
      ],
      "source": [
        "test_line = \"I feel gidd>&\"\n",
        "\n",
        "input = line_to_tensor(test_line)\n",
        "\n",
        "outputs = model.translate(input)\n",
        "for tensor,p in outputs:\n",
        "  result = [target_ix_to_char[j.item()] for j in tensor]\n",
        "  print(''.join(result))\n",
        "\n",
        "# outputs = model(input,1)\n",
        "# result = []\n",
        "# for i in range(outputs.shape[0]):\n",
        "\n",
        "#   p = nn.functional.softmax(outputs[i], dim=-1).detach().numpy().ravel()\n",
        "#   ix = np.random.choice(range(target_vocab_size), p=p)\n",
        "\n",
        "#   result.append(target_ix_to_char[ix])\n",
        "\n",
        "# print(''.join(result))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_lines"
      ],
      "metadata": {
        "id": "6hhu-SBOUE-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I changed the RNN to GRU\n",
        "and train it again to check the result"
      ],
      "metadata": {
        "id": "T_qh6_UVCIcS"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7umQp+gMFhttmyBwbHCC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}