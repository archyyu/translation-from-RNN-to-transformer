{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/translation-from-RNN-to-transformer/blob/main/machine_translation_by_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ma80RtZJnLKK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oMBvKgnEpZmH"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/eng-fra.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.split('\\n')\n",
        "en_lines = []\n",
        "fr_lines = []\n",
        "fr_targets = []\n",
        "\n",
        "start_character = '<'\n",
        "end_character = '>'\n",
        "padding_character = '&'\n",
        "\n",
        "for i in range(0,10000):\n",
        "  item = lines[i].split('\\t')\n",
        "  en_lines.append('<' + item[0] + '>')\n",
        "  fr_lines.append('<' + item[1])\n",
        "  fr_targets.append(item[1] + '>')\n",
        "\n",
        "max_len_line_en = max([len(l) for l in en_lines])\n",
        "max_len_line_fr = max([len(l) for l in fr_lines])\n",
        "max_len_line_en = max_len_line_fr\n",
        "\n",
        "for i in range(len(en_lines)):\n",
        "  if (len(en_lines[i]) < max_len_line_en):\n",
        "    en_lines[i] = en_lines[i].ljust(max_len_line_en, padding_character)\n",
        "  if (len(fr_lines[i]) < max_len_line_fr):\n",
        "    fr_lines[i] = fr_lines[i].ljust(max_len_line_fr, padding_character)\n",
        "    fr_targets[i] = fr_targets[i].ljust(max_len_line_fr, padding_character)\n",
        "\n",
        "\n",
        "source_vocab = sorted(set(''.join(en_lines)))\n",
        "target_vocab = sorted(set(''.join(fr_lines)))\n",
        "target_vocab.append('>')\n",
        "\n",
        "source_vocab_size = len(set(''.join(source_vocab)))\n",
        "target_vocab_size = len(set(''.join(target_vocab)))\n",
        "\n",
        "source_char_to_ix = {ch: i for i, ch in enumerate(source_vocab)}\n",
        "source_ix_to_char = {i: ch for i, ch in enumerate(source_vocab)}\n",
        "\n",
        "target_char_to_ix = {ch: i for i, ch in enumerate(target_vocab)}\n",
        "target_ix_to_char = {i: ch for i, ch in enumerate(target_vocab)}\n",
        "\n",
        "padding_token_index = target_char_to_ix[padding_character]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gOwCNJcStMI2"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 200\n",
        "seq_length = 80\n",
        "learning_rate = 0.0001\n",
        "batch_size = 20\n",
        "num_heads = 10\n",
        "head_size = 20\n",
        "layer_num = 4\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d3bzq7JxOzAh"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([source_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "def target_line_to_tensor(line):\n",
        "  result = []\n",
        "  line_ten = torch.tensor([target_char_to_ix[ch] for ch in test_line], dtype=torch.long).view(1, -1)\n",
        "  result.append(line_ten)\n",
        "  return torch.cat(result, dim=0)\n",
        "\n",
        "en_data = []\n",
        "fr_data = []\n",
        "fr_targets_data = []\n",
        "for i in range(len(en_lines)):\n",
        "  e = torch.tensor([source_char_to_ix[ch] for ch in en_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  en_data.append(e)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_lines[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_data.append(f)\n",
        "  f = torch.tensor([target_char_to_ix[ch] for ch in fr_targets[i]], dtype=torch.long).view(1, -1)\n",
        "  fr_targets_data.append(f)\n",
        "\n",
        "en_data = torch.cat(en_data, dim=0)\n",
        "fr_data = torch.cat(fr_data, dim=0)\n",
        "fr_targets_data = torch.cat(fr_targets_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "wbaEJaXpaHC4"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_size, head_size):\n",
        "    super(AttentionHead, self).__init__()\n",
        "    self.C = embed_size\n",
        "    self.head_size = head_size\n",
        "    self.q = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.k = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.v = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    B,T,C = q.shape\n",
        "    q = self.q(q)\n",
        "    k = self.k(k)\n",
        "    v = self.v(v)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n",
        "    wei.masked_fill_(mask == 0, -1e9)\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "    out = self.dropout(wei @ v)\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, embedding_size, num_heads):\n",
        "    super(MultiHeadAttentionBlock, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    head_size = embedding_size // num_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(embedding_size, head_size) for _ in range(num_heads)\n",
        "    ])\n",
        "\n",
        "    self.final_linear = nn.Linear(num_heads * head_size, embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "\n",
        "    head_outputs = [head(q, k, v, mask) for head in self.heads]\n",
        "    concatenated_output = torch.cat(head_outputs, dim=-1)\n",
        "    final_output = self.final_linear(concatenated_output)\n",
        "    final_output = self.dropout(final_output)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, features):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.norm(x)\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, d_ff: int) -> None:\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "    # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, seq_len):\n",
        "    super().__init__()\n",
        "    self.C = d_model\n",
        "    T = seq_len\n",
        "    n = 10000\n",
        "    pe = torch.zeros((T, self.C))\n",
        "    for k in range(T):\n",
        "      for i in torch.arange(int(self.C/2)):\n",
        "        denominator = torch.pow(n, 2*i/self.C)\n",
        "        pe[k, 2*i] += torch.sin(k/denominator)\n",
        "        pe[k, 2*i+1] += torch.cos(k/denominator)\n",
        "\n",
        "    pe.requires_grad_(False)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "  def __init__(self, features: int) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, h, d_model) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = MultiHeadAttentionBlock(d_model, h)\n",
        "    self.feed_forward_block = FeedForwardBlock(d_model, d_model * 4)\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(d_model) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, h, d_model) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([EncoderBlock(h, d_model) for _ in range(layer_num)])\n",
        "    self.norm = LayerNormalization(d_model)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class DecoderSelfAttentionBlock(nn.Module):\n",
        "  def __init__(self, h, d_model):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = MultiHeadAttentionBlock(d_model, h)\n",
        "    self.residual_conn = ResidualConnection(d_model)\n",
        "\n",
        "  def forward(self, x, k, v, mask):\n",
        "    x = self.residual_conn(x, lambda x: self.self_attention_block(x, k, v, mask))\n",
        "    return x\n",
        "\n",
        "class DecoderCrossAttentionBlock(nn.Module):\n",
        "  def __init__(self, h, d_model):\n",
        "    super().__init__()\n",
        "    self.cross_attention_block = MultiHeadAttentionBlock(d_model, h)\n",
        "    self.residual_conn = ResidualConnection(d_model)\n",
        "\n",
        "  def forward(self, x, encoder_output, mask):\n",
        "    x = self.residual_conn(x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, mask))\n",
        "    return x\n",
        "\n",
        "class DecoderFeedforwardBlock(nn.Module):\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.feed_forward_block = FeedForwardBlock(d_model, 4*d_model)\n",
        "    self.residual_conn = ResidualConnection(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.residual_conn(x, self.feed_forward_block)\n",
        "    return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, h, d_model) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = DecoderSelfAttentionBlock(h, d_model)\n",
        "    self.cross_attention_block = DecoderCrossAttentionBlock(h, d_model)\n",
        "    self.feed_forward_block = DecoderFeedforwardBlock(d_model)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    x = self.self_attention_block(x, x, x, tgt_mask)\n",
        "    x = self.cross_attention_block(x, encoder_output, src_mask)\n",
        "    x = self.feed_forward_block(x)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, h, d_model) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([DecoderBlock(h, d_model) for i in range(layer_num)])\n",
        "    self.norm = LayerNormalization(d_model)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return self.proj(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, h, d_model, src_vocab_size, tgt_vocab_size, seq_len) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(h, d_model)\n",
        "    self.decoder = Decoder(h, d_model)\n",
        "    self.src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    self.tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "    self.src_pos = PositionalEncoding(d_model, seq_len)\n",
        "    self.tgt_pos = PositionalEncoding(d_model, seq_len)\n",
        "    self.projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    encoder_output = self.encode(src, src_mask)\n",
        "    decoder_output = self.decode(encoder_output, src_mask, tgt, tgt_mask)\n",
        "    return self.project(decoder_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "z5ONskhX-hQK"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=target_char_to_ix['&'],label_smoothing=0.1)\n",
        "# model = Transformer(num_heads, embedding_dim, source_vocab_size, target_vocab_size, max_len_line_en, max_len_line_fr, head_size)\n",
        "model = Transformer(num_heads, embedding_dim, source_vocab_size, target_vocab_size, max_len_line_en)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, eps=1e-9)\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "  return mask == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "NRjINVa0-ZbW",
        "outputId": "6cfaff6c-cde7-48e9-9e83-315f4bee99b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p 0, Loss: 4.702218532562256\n",
            "p 1000, Loss: 2.973327159881592\n",
            "p 2000, Loss: 2.353698968887329\n",
            "p 3000, Loss: 2.5687243938446045\n",
            "p 4000, Loss: 2.864623785018921\n",
            "p 5000, Loss: 2.68341326713562\n",
            "p 6000, Loss: 2.676335096359253\n",
            "p 7000, Loss: 2.36032772064209\n",
            "p 8000, Loss: 2.2943074703216553\n",
            "p 9000, Loss: 2.40937876701355\n",
            "p 0, Loss: 3.0346388816833496\n",
            "p 1000, Loss: 2.3620543479919434\n",
            "p 2000, Loss: 2.03411865234375\n",
            "p 3000, Loss: 2.3095972537994385\n",
            "p 4000, Loss: 2.6267452239990234\n",
            "p 5000, Loss: 2.382294178009033\n",
            "p 6000, Loss: 2.190484046936035\n",
            "p 7000, Loss: 2.1710896492004395\n",
            "p 8000, Loss: 2.1295042037963867\n",
            "p 9000, Loss: 2.287872076034546\n",
            "p 0, Loss: 2.778531074523926\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-8a96f0be3977>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# loss = (loss * padding_mask).sum() / padding_mask.sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#training\n",
        "import torch.optim as optim\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  for p in range(0,len(en_data) - batch_size - 1,batch_size):\n",
        "    source_batch = en_data[p:p+batch_size]\n",
        "    target_batch = fr_data[p:p+batch_size]\n",
        "    actual_target_batch = fr_targets_data[p:p+batch_size]\n",
        "\n",
        "    src_mask = (source_batch != source_char_to_ix['&']).unsqueeze(1).int()\n",
        "    tgt_mask = (target_batch != target_char_to_ix['&']).unsqueeze(1).int() & causal_mask(target_batch.shape[1])\n",
        "\n",
        "    cross_mask = (target_batch != target_char_to_ix['&']).unsqueeze(1).int()\n",
        "\n",
        "    # print(src_mask.shape)\n",
        "    # print(tgt_mask.shape)\n",
        "    # print(mm.shape)\n",
        "\n",
        "    # break\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    output = model(source_batch, target_batch, src_mask, tgt_mask)\n",
        "\n",
        "    # print(torch.argmax(output, dim=-1))\n",
        "\n",
        "    # padding_mask = (actual_target_batch != padding_token_index).float()\n",
        "    loss = criterion(output.view(-1, target_vocab_size), actual_target_batch.reshape(-1))\n",
        "    # loss = (loss * padding_mask).sum() / padding_mask.sum()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if p % 500 == 0:\n",
        "      print(f'p {p}, Loss: {loss.item()}')\n",
        "\n",
        "    # break\n",
        "  # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "FpgZG9toDSdb",
        "outputId": "c0530604-ef29-495e-91b8-5b578ebd00e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[28, 66, 63, 63, 70,  0, 53, 60, 58, 50,  7, 19,  3]])\n",
            "tensor([[23, 60, 57, 57, 64,  0, 47, 54, 52, 44,  7, 90,  3]])\n",
            "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]], dtype=torch.int32)\n",
            "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]], dtype=torch.int32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (13) must match the size of tensor b (58) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-ae25d2f4b85e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0msoftmax_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-dc3381fe02cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-dc3381fe02cb>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-dc3381fe02cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mResidualConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (13) must match the size of tensor b (58) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  output = '<'\n",
        "  test_line = \"Hurry home.>&\"\n",
        "  top_three_values = []\n",
        "  top_three_values.append(['<', 1])\n",
        "\n",
        "  for i in range(20):\n",
        "\n",
        "    top_three_values.sort(key=lambda x: x[1],reverse=True)\n",
        "    top_three_values = top_three_values[:3]\n",
        "\n",
        "    new_top_three_values = []\n",
        "\n",
        "    for prefix in top_three_values:\n",
        "\n",
        "      input = line_to_tensor(test_line)\n",
        "      target = target_line_to_tensor(prefix[0])\n",
        "      src_mask = (input != source_char_to_ix['&']).unsqueeze(1).int()\n",
        "      tgt_mask = (target != target_char_to_ix['&']).unsqueeze(1).int() & causal_mask(target.shape[1])\n",
        "\n",
        "      print(input)\n",
        "      print(target)\n",
        "      print(src_mask)\n",
        "      print(tgt_mask)\n",
        "\n",
        "      exit(False)\n",
        "\n",
        "      outputs = model(input, target, src_mask, tgt_mask)\n",
        "      outputs = outputs.squeeze(0)\n",
        "      softmax_probs = F.softmax(outputs, dim=-1)\n",
        "\n",
        "      topk_item, topk_indices = torch.topk(softmax_probs[len(output) - 1], k=3)\n",
        "\n",
        "      for i in range(3):\n",
        "        sss = prefix[0] + target_ix_to_char[topk_indices[i].item()]\n",
        "        item = [sss, prefix[1] * topk_item[i]]\n",
        "        new_top_three_values.append(item)\n",
        "\n",
        "    top_three_values = new_top_three_values\n",
        "\n",
        "top_three_values.sort(key=lambda x: x[1],reverse=True)\n",
        "top_three_values = top_three_values[:3]\n",
        "\n",
        "for o in top_three_values:\n",
        "  print(o[0],o[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q74b4XmRx-jx"
      },
      "source": [
        "It is about the Chinese New Year\n",
        "\n",
        "So I will halt the project contemparaly\n",
        "\n",
        "and continue after the festival"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compared with translation of RNN\n",
        "the transformer actually need more parameters to train.\n",
        "if those two model have the same amount of parameters, then rnn will perform better\n",
        "\n",
        "the main advantage of transformer, I think, is it could scale well. which means if we add more layers, then the performance will increase linearly(i think), but the rnn is not.\n",
        "\n",
        "in the GPT project, I have tested rnn, if I made the rnn more deep, the overall performance didnot go well."
      ],
      "metadata": {
        "id": "IjmaDmv4zvZ5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+jsb/Wbr34pHq417+1MX/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}